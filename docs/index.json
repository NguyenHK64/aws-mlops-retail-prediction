[
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.1-blog1/",
	"title": "CÃ´ng bá»‘ cÃ¡c Äá»‘i tÃ¡c AWS cá»§a NÄƒm 2025 khu vá»±c chÃ¢u Ã‚u, Trung ÄÃ´ng vÃ  chÃ¢u Phi",
	"tags": ["Announcements", "AWS Partner Network", "AWS re:Invent", "Customer Solutions", "Foundational (100)"],
	"description": "",
	"content": "BÃ i viáº¿t gá»‘c Ä‘Æ°á»£c Ä‘Äƒng vÃ o ngÃ y 02 thÃ¡ng 12 nÄƒm 2025\nChÃºng tÃ´i hÃ o há»©ng cÃ´ng bá»‘ cÃ¡c Äá»‘i tÃ¡c AWS cá»§a NÄƒm 2025 khu vá»±c chÃ¢u Ã‚u, Trung ÄÃ´ng vÃ  chÃ¢u Phi (EMEA). Nhá»¯ng giáº£i thÆ°á»Ÿng hÃ ng nÄƒm nÃ y ghi nháº­n cÃ¡c thÃ nh viÃªn cá»§a Máº¡ng lÆ°á»›i Äá»‘i tÃ¡c AWS (APN) luÃ´n mang láº¡i káº¿t quáº£ xuáº¥t sáº¯c vÃ  thÃºc Ä‘áº©y Ä‘á»•i má»›i cho khÃ¡ch hÃ ng cá»§a chÃºng tÃ´i.\nTrong suá»‘t nÄƒm 2025, cÃ¡c Ä‘á»‘i tÃ¡c chiáº¿n tháº¯ng Ä‘Ã£ chá»©ng minh chuyÃªn mÃ´n cá»§a há» báº±ng cÃ¡ch cung cáº¥p cÃ¡c giáº£i phÃ¡p cloud chuyá»ƒn Ä‘á»•i vÃ  táº¡o ra giÃ¡ trá»‹ kinh doanh Ä‘o lÆ°á»ng Ä‘Æ°á»£c thÃ´ng qua cÃ¡c cÃ´ng nghá»‡ AWS. Há» Ä‘Ã£ hÆ°á»›ng dáº«n thÃ nh cÃ´ng khÃ¡ch hÃ ng qua nhá»¯ng quÃ¡ trÃ¬nh di chuyá»ƒn phá»©c táº¡p vÃ  giÃºp cÃ¡c tá»• chá»©c tá»‘i Æ°u hÃ³a mÃ´i trÆ°á»ng AWS cá»§a há» Ä‘á»ƒ tá»‘i Ä‘a hÃ³a tÃ¡c Ä‘á»™ng.\nSá»± há»£p tÃ¡c giá»¯a AWS vÃ  cÃ¡c Ä‘á»‘i tÃ¡c cá»§a chÃºng tÃ´i tiáº¿p tá»¥c lÃ  ná»n táº£ng cho sá»± thÃ nh cÃ´ng cá»§a khÃ¡ch hÃ ng. Nhá»¯ng quan há»‡ Ä‘á»‘i tÃ¡c nÃ y thÃºc Ä‘áº©y Ä‘á»•i má»›i vÃ  giÃºp khÃ¡ch hÃ ng táº­n dá»¥ng tá»‘i Ä‘a cÃ¡c dá»‹ch vá»¥ AWS Ä‘á»ƒ thÃºc Ä‘áº©y tÄƒng trÆ°á»Ÿng cá»§a há».\nHÃ£y cÃ¹ng chÃºng tÃ´i chÃºc má»«ng nhá»¯ng ngÆ°á»i chiáº¿n tháº¯ng giáº£i thÆ°á»Ÿng Äá»‘i tÃ¡c AWS cá»§a NÄƒm 2025!\nNgÆ°á»i chiáº¿n tháº¯ng VÆ°Æ¡ng quá»‘c Anh vÃ  Ireland Accenture Äá»‘i tÃ¡c TÆ° váº¥n cá»§a NÄƒm â€“ UKI\nAccenture lÃ  má»™t cÃ´ng ty dá»‹ch vá»¥ chuyÃªn nghiá»‡p giÃºp cÃ¡c tá»• chá»©c xÃ¢y dá»±ng lÃµi sá»‘ cá»§a há», tá»‘i Æ°u hÃ³a hoáº¡t Ä‘á»™ng vÃ  thÃºc Ä‘áº©y tÄƒng trÆ°á»Ÿng doanh thu. Vá»›i hÆ¡n 60.000 chuyÃªn gia Ä‘Æ°á»£c Ä‘Ã o táº¡o AWS vÃ  30.000 chuyÃªn gia Ä‘Æ°á»£c chá»©ng nháº­n AWS, Accenture giÃºp cÃ¡c doanh nghiá»‡p khai thÃ¡c sá»©c máº¡nh cá»§a cÃ´ng nghá»‡ AWS Ä‘á»ƒ thÃºc Ä‘áº©y giÃ¡ trá»‹ vÃ  Ä‘á»•i má»›i.\nBytes Technology Group Äá»‘i tÃ¡c TÆ° váº¥n Triá»ƒn vá»ng cá»§a NÄƒm â€“ UKI\nBytes Technology Group lÃ  Äá»‘i tÃ¡c TÆ° váº¥n HÃ ng Ä‘áº§u AWS táº­p trung vÃ o Di chuyá»ƒn, Hiá»‡n Ä‘áº¡i hÃ³a, Dá»‹ch vá»¥ Quáº£n lÃ½ vÃ  Marketplace trÃªn AWS phá»¥c vá»¥ khu vá»±c UKI.\nSnowflake Äá»‘i tÃ¡c CÃ´ng nghá»‡ cá»§a NÄƒm â€“ UKI\nSnowflake lÃ  ná»n táº£ng cho ká»· nguyÃªn AI, giÃºp cÃ¡c doanh nghiá»‡p dá»… dÃ ng Ä‘á»•i má»›i nhanh hÆ¡n vÃ  thu Ä‘Æ°á»£c nhiá»u giÃ¡ trá»‹ hÆ¡n tá»« dá»¯ liá»‡u. CÃ¡c cÃ´ng ty lá»›n nháº¥t tháº¿ giá»›i sá»­ dá»¥ng ÄÃ¡m mÃ¢y Dá»¯ liá»‡u AI cá»§a Snowflake Ä‘á»ƒ xÃ¢y dá»±ng, sá»­ dá»¥ng vÃ  chia sáº» dá»¯ liá»‡u, á»©ng dá»¥ng vÃ  AI.\nSnowplow Äá»‘i tÃ¡c CÃ´ng nghá»‡ Triá»ƒn vá»ng cá»§a NÄƒm â€“ UKI\nSnowplow lÃ  nhÃ  lÃ£nh Ä‘áº¡o toÃ n cáº§u vá» dá»¯ liá»‡u hÃ nh vi cho AI, cho phÃ©p cÃ¡c cÃ´ng ty Æ°u tiÃªn sá»‘ hÃ³a thu tháº­p, xá»­ lÃ½ vÃ  cung cáº¥p dá»¯ liá»‡u sá»± kiá»‡n cáº¥p Ä‘á»™ cao má»™t cÃ¡ch an toÃ n theo thá»i gian thá»±c. CÃ¡c nhÃ³m dá»¯ liá»‡u dá»±a vÃ o Snowplow Ä‘á»ƒ kiá»ƒm soÃ¡t Ä‘Æ°á»ng á»‘ng dá»¯ liá»‡u hoÃ n chá»‰nh, trong khi cÃ¡c nhÃ³m sáº£n pháº©m vÃ  ká»¹ thuáº­t tÃ­ch há»£p bá»‘i cáº£nh khÃ¡ch hÃ ng sÃ¢u sáº¯c vÃ o ML vÃ  cÃ¡c á»©ng dá»¥ng agentic. ÄÆ°á»£c tin tÆ°á»Ÿng há»— trá»£ phÃ¢n tÃ­ch nÃ¢ng cao, mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n, siÃªu cÃ¡ nhÃ¢n hÃ³a vÃ  bá»‘i cáº£nh tÃ¡c nhÃ¢n Ä‘á»‘i thoáº¡i AI.\nNgÆ°á»i chiáº¿n tháº¯ng Iberia IBM Äá»‘i tÃ¡c TÆ° váº¥n cá»§a NÄƒm â€“ Iberia\nIBM Consulting chuyÃªn vá» chuyá»ƒn Ä‘á»•i doanh nghiá»‡p thÃ´ng qua cÃ¡c giáº£i phÃ¡p cloud lai vÃ  trÃ­ tuá»‡ nhÃ¢n táº¡o. CÃ´ng ty giÃºp cÃ¡c tá»• chá»©c hiá»‡n Ä‘áº¡i hÃ³a vÃ  tÃ¡i táº¡o láº¡i doanh nghiá»‡p cá»§a há» báº±ng cÃ¡ch táº­n dá»¥ng cÃ´ng nghá»‡ tiÃªn tiáº¿n vÃ  chuyÃªn mÃ´n ngÃ nh sÃ¢u. LÃ  Ä‘á»‘i tÃ¡c chuyá»ƒn Ä‘á»•i toÃ n cáº§u, IBM Consulting thiáº¿t káº¿, triá»ƒn khai vÃ  quáº£n lÃ½ cÃ¡c mÃ´i trÆ°á»ng cloud lai phá»©c táº¡p trong khi khai thÃ¡c sá»©c máº¡nh cá»§a AI Ä‘á»ƒ thÃºc Ä‘áº©y Ä‘á»•i má»›i vÃ  giÃ¡ trá»‹ kinh doanh.\nLogicalis Spain Äá»‘i tÃ¡c TÆ° váº¥n Triá»ƒn vá»ng cá»§a NÄƒm â€“ Iberia\nLogicalis lÃ  nhÃ  cung cáº¥p giáº£i phÃ¡p IT toÃ n cáº§u vÃ  dá»‹ch vá»¥ quáº£n lÃ½ káº¿t há»£p sá»± xuáº¥t sáº¯c ká»¹ thuáº­t vá»›i hiá»ƒu biáº¿t chiáº¿n lÆ°á»£c Ä‘á»ƒ cung cáº¥p cÃ¡c giáº£i phÃ¡p cloud thÃºc Ä‘áº©y káº¿t quáº£ kinh doanh. Vá»›i chuyÃªn mÃ´n vá» AI, báº£o máº­t cloud, cá»™ng tÃ¡c, báº£o máº­t vÃ  chuyá»ƒn Ä‘á»•i sá»‘, Logicalis giÃºp cÃ¡c doanh nghiá»‡p Ä‘á»•i má»›i, kiáº¿n trÃºc, di chuyá»ƒn vÃ  tá»‘i Æ°u hÃ³a cÃ¡c khá»‘i lÆ°á»£ng cÃ´ng viá»‡c phá»©c táº¡p quan trá»ng cá»§a há» trÃªn AWS. Há»“ sÆ¡ thÃ nh tÃ­ch Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh cá»§a cÃ´ng ty bao gá»“m xÃ¢y dá»±ng vÃ  quáº£n lÃ½ cÃ¡c mÃ´i trÆ°á»ng AWS phá»©c táº¡p cho cÃ¡c doanh nghiá»‡p trÃªn toÃ n tháº¿ giá»›i, cho phÃ©p há» Ä‘áº¡t Ä‘Æ°á»£c sá»± nhanh nháº¹n, kháº£ nÄƒng má»Ÿ rá»™ng vÃ  hiá»‡u quáº£ chi phÃ­ lá»›n hÆ¡n.\nCrowdStrike Äá»‘i tÃ¡c CÃ´ng nghá»‡ cá»§a NÄƒm â€“ Iberia\nCrowdStrike lÃ  nhÃ  lÃ£nh Ä‘áº¡o toÃ n cáº§u vá» an ninh máº¡ng cung cáº¥p ná»n táº£ng cloud-native Ä‘á»ƒ báº£o vá»‡ Ä‘iá»ƒm cuá»‘i, khá»‘i lÆ°á»£ng cÃ´ng viá»‡c cloud, danh tÃ­nh vÃ  dá»¯ liá»‡u. ÄÆ°á»£c há»— trá»£ bá»Ÿi CrowdStrike Security Cloud vÃ  AI, ná»n táº£ng FalconÂ® sá»­ dá»¥ng cÃ¡c chá»‰ sá»‘ táº¥n cÃ´ng thá»i gian thá»±c, thÃ´ng tin tÃ¬nh bÃ¡o vá» má»‘i Ä‘e dá»a vÃ  phÃ©p Ä‘o tá»« xa phong phÃº Ä‘á»ƒ cung cáº¥p phÃ¡t hiá»‡n chÃ­nh xÃ¡c, báº£o vá»‡ tá»± Ä‘á»™ng, sÄƒn lÃ¹ng má»‘i Ä‘e dá»a Æ°u tÃº vÃ  kháº£ nÄƒng quan sÃ¡t lá»— há»•ng Ä‘Æ°á»£c Æ°u tiÃªn.\nRed Points Äá»‘i tÃ¡c CÃ´ng nghá»‡ Triá»ƒn vá»ng cá»§a NÄƒm â€“ Iberia\nRed Points lÃ  giáº£i phÃ¡p Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i nháº¥t trÃªn tháº¿ giá»›i chá»‘ng láº¡i hÃ ng giáº£, máº¡o danh thÆ°Æ¡ng hiá»‡u vÃ  vi pháº¡m báº£n quyá»n. HÆ¡n 1.300 tá»• chá»©c tin tÆ°á»Ÿng ná»n táº£ng cá»§a Red Points Ä‘á»ƒ báº£o vá»‡ ngÆ°á»i tiÃªu dÃ¹ng cá»§a há» trÃªn táº¥t cáº£ cÃ¡c kÃªnh sá»‘ - marketplace, trang web, phÆ°Æ¡ng tiá»‡n truyá»n thÃ´ng xÃ£ há»™i, quáº£ng cÃ¡o, á»©ng dá»¥ng, tÃªn miá»n vÃ  hÆ¡n tháº¿ ná»¯a. CÃ¡c mÃ´ hÃ¬nh AI cá»§a Red Points Ä‘Æ°á»£c Ä‘Ã o táº¡o trÃªn hÆ¡n 2,7 tá»· Ä‘iá»ƒm dá»¯ liá»‡u hÃ ng thÃ¡ng, Ä‘áº¡i diá»‡n cho cÆ¡ sá»Ÿ dá»¯ liá»‡u báº£o vá»‡ thÆ°Æ¡ng hiá»‡u lá»›n nháº¥t tá»«ng Ä‘Æ°á»£c thu tháº­p. Trong thá»i Ä‘áº¡i mÃ  dá»¯ liá»‡u xÃ¡c Ä‘á»‹nh Ä‘á»™ tin cáº­y AI, Red Points káº¿t há»£p lá»£i tháº¿ nÃ y vá»›i má»™t tháº­p ká»· chuyÃªn mÃ´n Ä‘á»ƒ mang láº¡i sá»± báº£o vá»‡ chÃ­nh xÃ¡c, cÃ³ thá»ƒ má»Ÿ rá»™ng.\nNgÆ°á»i chiáº¿n tháº¯ng Italy Reply Äá»‘i tÃ¡c TÆ° váº¥n cá»§a NÄƒm â€“ Italy\nReply lÃ  cÃ´ng ty tÆ° váº¥n hiá»‡n Ä‘áº¡i hÃ ng Ä‘áº§u cung cáº¥p cÃ¡c giáº£i phÃ¡p end-to-end trÃªn AWS. AirWalk Reply, Comystos Reply, Data Reply, Cortex, Sense Reply vÃ  Storm Reply lÃ  cÃ¡c cÃ´ng ty cá»§a Reply Group chuyÃªn vá» AWS nhÆ° má»™t quy trÃ¬nh toÃ n bá»™ tá»« Cloud Advisory Ä‘áº¿n Migration vÃ  Managed Services vá»›i trá»ng tÃ¢m máº¡nh vÃ o Cloud Native Development, IoT, Big Data vÃ  ML.\nBeNext Äá»‘i tÃ¡c TÆ° váº¥n Triá»ƒn vá»ng cá»§a NÄƒm â€“ Italy\nBeNext lÃ  Äá»‘i tÃ¡c Dá»‹ch vá»¥ Cáº¥p cao AWS káº¿t há»£p ká»¹ thuáº­t cloud vá»›i tÆ° duy hÆ°á»›ng sáº£n pháº©m Ä‘á»ƒ giÃºp cÃ¡c cÃ´ng ty Ä‘á»•i má»›i nhanh hÆ¡n. ThÃ´ng qua cÃ¡c khung Ä‘Ã¡nh giÃ¡ Ä‘á»™c quyá»n, giao hÃ ng automation-first vÃ  AWS Cloud Academy phÃ¡t triá»ƒn nhanh chÃ³ng, BeNext cho phÃ©p cÃ¡c tá»• chá»©c xÃ¢y dá»±ng cÃ¡c ná»n táº£ng cloud an toÃ n, cÃ³ thá»ƒ má»Ÿ rá»™ng vÃ  tá»‘i Æ°u chi phÃ­.\nSalesforce Äá»‘i tÃ¡c CÃ´ng nghá»‡ cá»§a NÄƒm â€“ Italy\nNhÃ  cung cáº¥p cloud hÃ ng Ä‘áº§u tháº¿ giá»›i vÃ  ná»n táº£ng CRM sá»‘ 1 Ä‘ang giÃºp khÃ¡ch hÃ ng quáº£n lÃ½ dá»¯ liá»‡u cá»§a há» má»™t cÃ¡ch liá»n máº¡ch vÃ  an toÃ n trÃªn Salesforce vÃ  AWS vÃ  tÃ­ch há»£p an toÃ n vÃ  cÃ³ trÃ¡ch nhiá»‡m cÃ¡c cÃ´ng nghá»‡ trÃ­ tuá»‡ nhÃ¢n táº¡o táº¡o sinh má»›i nháº¥t (AI) vÃ o á»©ng dá»¥ng vÃ  quy trÃ¬nh lÃ m viá»‡c cá»§a há». AWS vÃ  Salesforce há»— trá»£ khÃ¡ch hÃ ng thÃ´ng qua cÃ¡c tÃ­ch há»£p má»›i vÃ  nÃ¢ng cao giá»¯a cÃ´ng nghá»‡ AWS vÃ  sáº£n pháº©m Salesforce, bao gá»“m quáº£n lÃ½ dá»¯ liá»‡u thá»‘ng nháº¥t, triá»ƒn khai liá»n máº¡ch giá»ng nÃ³i, video vÃ  dá»‹ch vá»¥ AI cá»§a AWS, tÃ¹y chá»n mua sáº¯m Ä‘Æ°á»£c sáº¯p xáº¿p há»£p lÃ½ thÃ´ng qua AWS Marketplace vÃ  nhiá»u hÆ¡n ná»¯a.\nErmes Äá»‘i tÃ¡c CÃ´ng nghá»‡ Triá»ƒn vá»ng cá»§a NÄƒm â€“ Italy\nKhi trÃ¬nh duyá»‡t vÃ  thiáº¿t bá»‹ di Ä‘á»™ng Ä‘Ã£ trá»Ÿ thÃ nh biÃªn giá»›i Ä‘áº§u tiÃªn Ä‘á»‘i vá»›i nÄƒng suáº¥t dá»±a trÃªn cloud, vÃ  cÃ¡c cÃ´ng cá»¥ tÄƒng cÆ°á»ng AI Ä‘ang thu hÃºt sá»± chÃº Ã½ cá»§a má»i chá»©c nÄƒng kinh doanh, báº£o máº­t pháº£i phÃ¡t triá»ƒn cÃ¹ng vá»›i chu vi máº¡ng Ä‘á»ƒ cÃ³ tÃ­nh nÄƒng ngá»¯ cáº£nh vÃ  láº¥y ngÆ°á»i dÃ¹ng lÃ m trung tÃ¢m. Ermes Browser Security hÆ¡n ná»¯a xem xÃ©t má»™t trÃ¬nh duyá»‡t an toÃ n vÃ  thÃ´ng tin di Ä‘á»™ng nhÆ° bÆ°á»›c Ä‘áº§u tiÃªn hÆ°á»›ng tá»›i má»™t ná»n táº£ng thÃ´ng minh rá»™ng hÆ¡n sáº½ báº£o máº­t cÃ¡ch má»i ngÆ°á»i chia sáº», cá»™ng tÃ¡c vÃ  táº¡o ra trÃªn cÃ¡c cÃ´ng cá»¥ SaaS, á»©ng dá»¥ng nháº¯n tin vÃ  há»‡ thá»‘ng GenAI. Ermes ná»•i báº­t vÃ¬ sá»© má»‡nh cá»§a mÃ¬nh lÃ  báº£o vá»‡ trÃ¬nh duyá»‡t khá»i cÃ¡c loáº¡i táº¥n cÃ´ng máº¡ng khÃ¡c nhau nhÆ° lá»«a Ä‘áº£o, quáº£ng cÃ¡o Ä‘á»™c háº¡i vÃ  ngÄƒn cháº·n dá»¯ liá»‡u, cáº£i thiá»‡n tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng.\nAtyos Äá»‘i tÃ¡c TÆ° váº¥n Triá»ƒn vá»ng cá»§a NÄƒm â€“ France\nAtyos táº­p trung vÃ o tá»± Ä‘á»™ng hÃ³a, phÃ¡t triá»ƒn cloud-native, dá»¯ liá»‡u vÃ  cÃ¡c giáº£i phÃ¡p AI. CÃ´ng ty thiáº¿t káº¿, di chuyá»ƒn vÃ  quáº£n lÃ½ cÃ¡c mÃ´i trÆ°á»ng cloud cÃ³ tÃ­nh má»Ÿ rá»™ng cao, linh hoáº¡t vÃ  hiá»‡u quáº£ chi phÃ­. Cho dÃ¹ má»™t tá»• chá»©c Ä‘ang khÃ¡m phÃ¡, khá»Ÿi táº¡o hoáº·c thÃºc Ä‘áº©y hÃ nh trÃ¬nh cloud cá»§a há», Atyos cung cáº¥p hÆ°á»›ng dáº«n chuyÃªn gia cÃ¢n báº±ng hoÃ n háº£o sá»± nhanh nháº¹n vÃ  Ä‘á»™ tin cáº­y. Atyos cÅ©ng cung cáº¥p cÃ¡c chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o AWS chÃ­nh thá»©c, giÃºp khÃ¡ch hÃ ng má»Ÿ khÃ³a toÃ n bá»™ tiá»m nÄƒng cá»§a AWS.\nSplunk Äá»‘i tÃ¡c CÃ´ng nghá»‡ cá»§a NÄƒm â€“ France\nCÃ¡c tá»• chá»©c hÃ ng Ä‘áº§u tháº¿ giá»›i tin tÆ°á»Ÿng Splunk, má»™t cÃ´ng ty cá»§a Cisco, Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c kháº£ nÄƒng hiá»ƒn thá»‹ vÃ´ song trong toÃ n bá»™ dáº¥u chÃ¢n sá»‘ cá»§a há». Ná»n táº£ng Báº£o máº­t vÃ  Kháº£ nÄƒng quan sÃ¡t thá»‘ng nháº¥t cá»§a há», Ä‘Æ°á»£c há»— trá»£ bá»Ÿi AI hÃ ng Ä‘áº§u trong ngÃ nh, trao quyá»n cho cÃ¡c nhÃ³m SecOps, ITOps vÃ  ká»¹ thuáº­t cá»§a khÃ¡ch hÃ ng Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu vÃ  cá»™ng tÃ¡c cá»§a há» - á»Ÿ báº¥t ká»³ quy mÃ´ nÃ o.\nAxway Äá»‘i tÃ¡c CÃ´ng nghá»‡ Triá»ƒn vá»ng cá»§a NÄƒm â€“ France\nAxway lÃ  cÃ´ng ty pháº§n má»m toÃ n cáº§u chuyÃªn vá» quáº£n lÃ½ API, MFT vÃ  cÃ¡c giáº£i phÃ¡p tÃ­ch há»£p B2B Ä‘Æ°á»£c tin tÆ°á»Ÿng bá»Ÿi hÃ ng nghÃ¬n doanh nghiá»‡p trÃªn toÃ n tháº¿ giá»›i. Báº±ng cÃ¡ch há»£p tÃ¡c vá»›i AWS cho SaaS vÃ  dá»‹ch vá»¥ quáº£n lÃ½ trong hÆ¡n má»™t tháº­p ká»·, Axway cung cáº¥p cÃ¡c lá»›p bá»• sung vá» nÄƒng lá»±c vÃ  báº£o máº­t liÃªn ngÃ nh, bao gá»“m há»— trá»£ cho AWS PrivateLink vÃ  chuyÃªn mÃ´n Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh vá» tuÃ¢n thá»§ vÃ  mÃ´i trÆ°á»ng Ä‘Æ°á»£c quy Ä‘á»‹nh trÃªn toÃ n cáº§u.\nNgÆ°á»i chiáº¿n tháº¯ng Benelux Deloitte Äá»‘i tÃ¡c TÆ° váº¥n cá»§a NÄƒm â€“ Benelux\nDeloitte lÃ  tá»• chá»©c dá»‹ch vá»¥ chuyÃªn nghiá»‡p hÃ ng Ä‘áº§u tháº¿ giá»›i, cÃ³ cÃ¡c chuyÃªn gia trong lÄ©nh vá»±c cloud, cyber, GenAI, phÃ¢n tÃ­ch vÃ  cÃ¡c ngÃ nh khÃ¡c hoáº¡t Ä‘á»™ng cáº¡nh nhau vá»›i cÃ¡c nhÃ  lÃ£nh Ä‘áº¡o kinh nghiá»‡m trong cÃ¡c ngÃ nh nhÆ° chÄƒm sÃ³c sá»©c khá»e cÃ´ng cá»™ng, giao thÃ´ng váº­n táº£i, cÆ¡ sá»Ÿ háº¡ táº§ng, quáº£n lÃ½ cÆ¡ sá»Ÿ háº¡ táº§ng, an ninh quá»‘c gia vÃ  phÃ²ng thá»§, cÅ©ng nhÆ° báº£o máº­t vÃ  tuÃ¢n thá»§.\nIlionx Äá»‘i tÃ¡c TÆ° váº¥n Triá»ƒn vá»ng cá»§a NÄƒm â€“ Benelux\nIlionx, má»™t cÃ´ng ty dá»‹ch vá»¥ IT HÃ  Lan, Ä‘Æ°á»£c thÃ nh láº­p nÄƒm 2002 vÃ  cÃ³ trá»¥ sá»Ÿ táº¡i Utrecht, chuyÃªn táº¡o ra cÃ¡c giáº£i phÃ¡p Ä‘Æ¡n giáº£n, bá»n vá»¯ng cho chiáº¿n lÆ°á»£c sá»‘, á»©ng dá»¥ng cloud, dá»¯ liá»‡u vÃ  AI, siÃªu tá»± Ä‘á»™ng hÃ³a, tÃ­ch há»£p vÃ  dá»‹ch vá»¥ quáº£n lÃ½ Ä‘á»ƒ giÃºp cÃ¡c tá»• chá»©c há»£p lÃ½ hÃ³a cÃ¡c quy trÃ¬nh phá»©c táº¡p.\nTrend Micro Äá»‘i tÃ¡c CÃ´ng nghá»‡ cá»§a NÄƒm â€“ Benelux\nTrend Micro, nhÃ  lÃ£nh Ä‘áº¡o toÃ n cáº§u vá» an ninh máº¡ng, giÃºp lÃ m cho tháº¿ giá»›i an toÃ n Ä‘á»ƒ trao Ä‘á»•i thÃ´ng tin sá»‘ giá»¯a má»i ngÆ°á»i, chÃ­nh phá»§ vÃ  doanh nghiá»‡p. Trend táº­n dá»¥ng chuyÃªn mÃ´n báº£o máº­t vÃ  AI Ä‘á»ƒ báº£o vá»‡ hÆ¡n 500.000 doanh nghiá»‡p vÃ  hÃ ng triá»‡u cÃ¡ nhÃ¢n trÃªn cÃ¡c Ä‘Ã¡m mÃ¢y, máº¡ng, Ä‘iá»ƒm cuá»‘i vÃ  thiáº¿t bá»‹ trÃªn toÃ n tháº¿ giá»›i. Báº£o máº­t chá»§ Ä‘á»™ng báº¯t Ä‘áº§u tá»« Ä‘Ã¢y.\nWeaviate Äá»‘i tÃ¡c CÃ´ng nghá»‡ Triá»ƒn vá»ng cá»§a NÄƒm â€“ Benelux\nWeaviate lÃ  cÆ¡ sá»Ÿ dá»¯ liá»‡u vector AI-Native Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘Æ¡n giáº£n hÃ³a viá»‡c phÃ¡t triá»ƒn AI táº¡o sinh vÃ  á»©ng dá»¥ng tÃ¬m kiáº¿m thÃ´ng minh. NÃ³ tÃ­ch há»£p liá»n máº¡ch vá»›i kháº£ nÄƒng cá»§a Amazon Bedrock vÃ  Sagemaker, Weaviate trao quyá»n cho cÃ¡c tá»• chá»©c diá»…n giáº£i hiá»‡u quáº£ cÃ¡c táº­p dá»¯ liá»‡u lá»›n cá»§a thÃ´ng tin khÃ´ng cÃ³ cáº¥u trÃºc. NÃ³ cung cáº¥p chá»©c nÄƒng tÃ¬m kiáº¿m vector vÃ  tá»« khÃ³a máº¡nh máº½, cÃ³ thá»ƒ má»Ÿ rá»™ng trÃªn táº¥t cáº£ cÃ¡c loáº¡i dá»¯ liá»‡u.\nSnapSoft Äá»‘i tÃ¡c TÆ° váº¥n Triá»ƒn vá»ng cá»§a NÄƒm â€“ CEE\nSnapSoft lÃ  Äá»‘i tÃ¡c AWS Cáº¥p cao chuyÃªn vá» di chuyá»ƒn cloud, AI/ML, phÃ¢n tÃ­ch vÃ  DevOps. NÄƒm 2025, SnapSoft Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c cÃ¡c nÄƒng lá»±c AWS GenAI vÃ  SMB, Ä‘Æ°á»£c thÄƒng cáº¥p lÃªn Cáº¥p cao vÃ  cung cáº¥p hÆ¡n 200 giáº£i phÃ¡p end-to-end cloud trÃªn toÃ n cáº§u.\nSnowflake Äá»‘i tÃ¡c CÃ´ng nghá»‡ cá»§a NÄƒm â€“ CEE\nSnowflake lÃ  ná»n táº£ng cho ká»· nguyÃªn AI, giÃºp cÃ¡c doanh nghiá»‡p dá»… dÃ ng Ä‘á»•i má»›i nhanh hÆ¡n vÃ  thu Ä‘Æ°á»£c nhiá»u giÃ¡ trá»‹ hÆ¡n tá»« dá»¯ liá»‡u. CÃ¡c cÃ´ng ty lá»›n nháº¥t tháº¿ giá»›i sá»­ dá»¥ng ÄÃ¡m mÃ¢y Dá»¯ liá»‡u AI cá»§a Snowflake Ä‘á»ƒ xÃ¢y dá»±ng, sá»­ dá»¥ng vÃ  chia sáº» dá»¯ liá»‡u, á»©ng dá»¥ng vÃ  AI.\nTiger Technology Äá»‘i tÃ¡c CÃ´ng nghá»‡ Triá»ƒn vá»ng cá»§a NÄƒm â€“ CEE\nTiger Technology cho phÃ©p cÃ¡c tá»• chá»©c má»Ÿ rá»™ng liá»n máº¡ch lÆ°u trá»¯ táº¡i chá»— lÃªn cloud vá»›i pháº§n má»m quáº£n lÃ½ dá»¯ liá»‡u cloud lai cá»§a nÃ³. Giáº£i phÃ¡p hÃ ng Ä‘áº§u cá»§a nÃ³, Tiger Bridge, giÃºp khÃ¡ch hÃ ng tá»‘i Æ°u hÃ³a chi phÃ­ lÆ°u trá»¯, báº£o vá»‡ dá»¯ liá»‡u vÃ  trÃ­ch xuáº¥t giÃ¡ trá»‹ tá»« tá»‡p thÃ´ng qua quy trÃ¬nh lÃ m viá»‡c tá»± Ä‘á»™ng. Báº±ng cÃ¡ch káº¿t ná»‘i mÃ´i trÆ°á»ng táº¡i chá»— vÃ  cloud, Tiger Technology trao quyá»n cho khÃ¡ch hÃ ng thÃºc Ä‘áº©y chuyá»ƒn Ä‘á»•i sá»‘ vÃ  Ã¡p dá»¥ng AWS vá»›i sá»± tá»± tin.\nNgÆ°á»i chiáº¿n tháº¯ng Trung ÄÃ´ng vÃ  Báº¯c Phi (MENA) VNGRS Äá»‘i tÃ¡c TÆ° váº¥n Triá»ƒn vá»ng cá»§a NÄƒm â€“ Turkey\nVNGRS, Äá»‘i tÃ¡c TÆ° váº¥n NÃ¢ng cao AWS, xuáº¥t sáº¯c trong viá»‡c giÃºp khÃ¡ch hÃ ng thÃºc Ä‘áº©y Ä‘á»•i má»›i trÃªn AWS. CÃ´ng ty cung cáº¥p cÃ¡c chÆ°Æ¡ng trÃ¬nh dá»¯ liá»‡u doanh nghiá»‡p quy mÃ´ lá»›n, tá»« cÃ¡c ná»n táº£ng lakehouse Ä‘áº¿n cÃ¡c sÃ¡ng kiáº¿n streaming vÃ  phÃ¢n tÃ­ch thá»i gian thá»±c, cÅ©ng nhÆ° cÃ¡c giáº£i phÃ¡p AI táº¡o sinh nhÆ° Kumru.ai cho khÃ¡ch hÃ ng trÃªn khu vá»±c MENAT.\nMongoDB Äá»‘i tÃ¡c CÃ´ng nghá»‡ cá»§a NÄƒm â€“ Turkey\nMongoDB lÃ  cÆ¡ sá»Ÿ dá»¯ liá»‡u NoSQL phá»• biáº¿n lÆ°u trá»¯ dá»¯ liá»‡u trong cÃ¡c tÃ i liá»‡u linh hoáº¡t, giá»‘ng JSON. KhÃ´ng giá»‘ng nhÆ° cÃ¡c cÆ¡ sá»Ÿ dá»¯ liá»‡u quan há»‡ truyá»n thá»‘ng, nÃ³ khÃ´ng yÃªu cáº§u lÆ°á»£c Ä‘á»“ cá»‘ Ä‘á»‹nh, khiáº¿n nÃ³ lÃ½ tÆ°á»Ÿng Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u khÃ´ng cÃ³ cáº¥u trÃºc vÃ  phÃ¡t triá»ƒn á»©ng dá»¥ng nhanh chÃ³ng. NÃ³ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho cÃ¡c á»©ng dá»¥ng web vÃ  di Ä‘á»™ng hiá»‡n Ä‘áº¡i.\nNgÆ°á»i chiáº¿n tháº¯ng Nordics Knowit Äá»‘i tÃ¡c TÆ° váº¥n cá»§a NÄƒm â€“ Nordics\nKnowit lÃ  Äá»‘i tÃ¡c AWS hÃ ng Ä‘áº§u trong khu vá»±c Nordics, chuyÃªn vá» di chuyá»ƒn cloud vÃ  giáº£i phÃ¡p AI. Vá»›i chuyÃªn mÃ´n sÃ¢u vá» di chuyá»ƒn cÆ¡ sá»Ÿ dá»¯ liá»‡u, hiá»‡n Ä‘áº¡i hÃ³a á»©ng dá»¥ng vÃ  triá»ƒn khai machine learning, Knowit giÃºp cÃ¡c tá»• chá»©c chuyá»ƒn Ä‘á»•i liá»n máº¡ch sang AWS trong khi táº­n dá»¥ng cÃ¡c cÃ´ng nghá»‡ AI tiÃªn tiáº¿n nhÆ° Amazon SageMaker vÃ  Amazon Bedrock Ä‘á»ƒ thÃºc Ä‘áº©y Ä‘á»•i má»›i vÃ  chuyá»ƒn Ä‘á»•i kinh doanh.\nNordHero Äá»‘i tÃ¡c TÆ° váº¥n Triá»ƒn vá»ng cá»§a NÄƒm â€“ Nordics\nVá»›i vÄƒn phÃ²ng táº¡i JyvÃ¤skylÃ¤ vÃ  Helsinki, Pháº§n Lan, NordHero lÃ  cÃ´ng ty tÆ° váº¥n cloud chuyÃªn vá» kiáº¿n trÃºc cloud-native, DevOps, dá»¯ liá»‡u vÃ  phÃ¢n tÃ­ch, vÃ  AI trÃªn AWS. NÃ³ giÃºp cÃ¡c tá»• chá»©c thiáº¿t káº¿, xÃ¢y dá»±ng vÃ  váº­n hÃ nh cÃ¡c mÃ´i trÆ°á»ng cloud an toÃ n, cÃ³ thá»ƒ má»Ÿ rá»™ng thÃºc Ä‘áº©y tÃ¡c Ä‘á»™ng kinh doanh thá»±c sá»±. Ná»•i tiáº¿ng vá»›i chuyÃªn mÃ´n ká»¹ thuáº­t sÃ¢u sáº¯c, nhÃ³m NordHero lÃ  cÃ¡c thÃ nh viÃªn cÃ³ uy tÃ­n cao trong cá»™ng Ä‘á»“ng AWS vá»›i nhiá»u chá»©ng nháº­n ká»¹ thuáº­t vÃ  kinh doanh khÃ¡c, NordHero lÃ  AWS-accredited Well-Architected Reviewer vÃ  Immersion Day Facilitator.\nNgÆ°á»i chiáº¿n tháº¯ng Israel AllCloud Äá»‘i tÃ¡c TÆ° váº¥n cá»§a NÄƒm â€“ Israel\nAllCloud lÃ  Ä‘á»‘i tÃ¡c AWS Premier vÃ  cung cáº¥p cÃ¡c dá»‹ch vá»¥ quáº£n lÃ½ Ä‘Æ°á»£c kiá»ƒm toÃ¡n. CÃ´ng ty lÃ  nhÃ  lÃ£nh Ä‘áº¡o trong viá»‡c khuáº¿ch Ä‘áº¡i tiá»m nÄƒng cloud cá»§a tá»• chá»©c thÃ´ng qua AI. Vá»›i há»“ sÆ¡ thÃ nh tÃ­ch vá» hÃ ng trÄƒm triá»ƒn khai thÃ nh cÃ´ng trÃªn AWS vÃ  Salesforce, AllCloud Ä‘Ã£ phÃ¡t triá»ƒn cÃ¡c chiáº¿n lÆ°á»£c vÃ  giáº£i phÃ¡p cho phÃ©p cÃ¡c doanh nghiá»‡p á»Ÿ má»i quy mÃ´ váº«n Ä‘i Ä‘áº§u trong Ä‘á»•i má»›i.\nProfiSea Äá»‘i tÃ¡c Rising Star cá»§a NÄƒm â€“ Israel\nProfiSea, Äá»‘i tÃ¡c Dá»‹ch vá»¥ Cáº¥p cao AWS, chuyÃªn vá» DevOps, CloudOps vÃ  FinOps. NÃ³ cung cáº¥p di chuyá»ƒn cloud, hiá»‡n Ä‘áº¡i hÃ³a vÃ  cÃ¡c giáº£i phÃ¡p tá»± Ä‘á»™ng hÃ³a Ä‘á»ƒ tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t, báº£o máº­t vÃ  hiá»‡u quáº£ chi phÃ­. Ná»n táº£ng FinOps cá»§a nÃ³, Uniskai by ProfiSea Labs, cung cáº¥p kháº£ nÄƒng hiá»ƒn thá»‹ cloud thá»‘ng nháº¥t vÃ  quáº£n lÃ½ chi phÃ­ thÃ´ng minh cho cÃ¡c doanh nghiá»‡p vÃ  startup.\nSnowflake Äá»‘i tÃ¡c CÃ´ng nghá»‡ cá»§a NÄƒm â€“ Israel\nSnowflake lÃ  ná»n táº£ng cho ká»· nguyÃªn AI, giÃºp cÃ¡c doanh nghiá»‡p dá»… dÃ ng Ä‘á»•i má»›i nhanh hÆ¡n vÃ  thu Ä‘Æ°á»£c nhiá»u giÃ¡ trá»‹ hÆ¡n tá»« dá»¯ liá»‡u. CÃ¡c cÃ´ng ty lá»›n nháº¥t tháº¿ giá»›i sá»­ dá»¥ng ÄÃ¡m mÃ¢y Dá»¯ liá»‡u AI cá»§a Snowflake Ä‘á»ƒ xÃ¢y dá»±ng, sá»­ dá»¥ng vÃ  chia sáº» dá»¯ liá»‡u, á»©ng dá»¥ng vÃ  AI.\nScytale Äá»‘i tÃ¡c CÃ´ng nghá»‡ Triá»ƒn vá»ng cá»§a NÄƒm â€“ Israel\nScytale lÃ  giáº£i phÃ¡p GRC tá»± Ä‘á»™ng giÃ¡m sÃ¡t liÃªn tá»¥c cÃ¡c Ä‘iá»u khiá»ƒn báº£o máº­t vÃ  tuÃ¢n thá»§, tá»± Ä‘á»™ng hÃ³a thu tháº­p báº±ng chá»©ng vÃ  há»£p lÃ½ hÃ³a cÃ¡c khung tuÃ¢n thá»§ nhÆ° SOC 2, ISO 27001 vÃ  GDPR. ÄÆ°á»£c tin tÆ°á»Ÿng bá»Ÿi 899+ khÃ¡ch hÃ ng trÃªn toÃ n tháº¿ giá»›i, Scytale cho phÃ©p cÃ¡c tá»• chá»©c chia sáº» tÆ° tháº¿ tuÃ¢n thá»§ thá»i gian thá»±c cá»§a há» vá»›i khÃ¡ch hÃ ng tiá»m nÄƒng vÃ  khÃ¡ch hÃ ng Ä‘á»ƒ xÃ¢y dá»±ng niá»m tin vÃ  thÃºc Ä‘áº©y tÄƒng trÆ°á»Ÿng.\nLink bÃ i viáº¿t gá»‘c : https://aws.amazon.com/blogs/apn/announcing-the-regional-2025-aws-partners-of-the-year-for-europe-middle-east-and-africa/\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/1-introduction/",
	"title": "MLOps Architecture Design",
	"tags": [],
	"description": "",
	"content": "AWS MLOps Retail Prediction Platform AWS MLOps Retail Prediction is a complete end-to-end MLOps system built on AWS Cloud, automating the entire lifecycle from infrastructure provisioning, model training, inference API deployment, to monitoring and cost optimization. The project is designed to ensure scalability, reliability, and strong security for real-world Machine Learning applications.\n1. MLOps Architecture on AWS Cloud 1.1 Project Objectives Fully automate the MLOps workflow:\nğŸ—ï¸ Infrastructure as Code: Automate infrastructure provisioning using Terraform (VPC, EKS, IAM, EC2, ECR, S3) ğŸ¤– ML Training: Distributed model training on SageMaker with Model Registry ğŸš€ Container Deployment: Package \u0026amp; deploy the inference API on EKS with autoscaling ğŸ“Š Monitoring \u0026amp; Security: Monitoring with CloudWatch, security with KMS \u0026amp; CloudTrail ğŸ”„ CI/CD Pipeline: Automated pipeline from code/data changes â†’ build â†’ train â†’ deploy ğŸ’° Cost Optimization: Integrate DataOps and teardown strategies to optimize costs 1.2 Overall Flow Infrastructure â†’ Training â†’ Deployment â†’ Monitoring â†’ CI/CD â†’ Cost Optimization\n2. Customer Price Sensitivity Prediction Problem 2.1 Dataset Description - dunnhumby Source Files Data source: dunnhumby Source Files\nDataset used: transaction.csv (â‰ˆ 2.67 million rows, 22 columns)\nDescription: Each row represents a product in a customer\u0026rsquo;s single shopping transaction.\n2.1.1 Detailed Data Schema Column Name Data Type Description / Meaning Example Value SHOP_WEEK int64 Shopping week (format YYYYWW) 200807 SHOP_DATE int64 Shopping date (format YYYYMMDD) 20080407 SHOP_WEEKDAY int64 Day of week (1=Sunday, 2=Monday, â€¦, 7=Saturday) 2 SHOP_HOUR int64 Transaction hour (0â€“23) 14 QUANTITY int64 Quantity purchased in the transaction line 1 SPEND float64 Amount spent (currency: British Pound Â£) for the line (product Ã— quantity) 1.01 PROD_CODE object Detailed product code (lowest level) PRD0900005 PROD_CODE_10 object Product group code level 1 (main category) CL00155 PROD_CODE_20 object Product group code level 2 DEP00053 PROD_CODE_30 object Product group code level 3 G00016 PROD_CODE_40 object Product group code level 4 (may represent a sub-category) NaN / G00420 CUST_CODE object Customer identifier (anonymized) CUST0000123 seg_1 object Customer segment level 1 (high-level behavior classification) BG / AZ / NaN seg_2 object Customer segment level 2 (more detailed than seg_1) DI / CZ / BU BASKET_ID int64 Basket ID (each purchase event of a customer) 994110500233340 BASKET_SIZE object Basket size (Small/Medium/Large) S / M / L BASKET_PRICE_SENSITIVITY object Customer price sensitivity within the transaction (Low/Medium/High or short codes like LA, MM, UM) MM / LA / UM BASKET_TYPE object Basket type (Full Shop / Small Shop / Top Up / Fresh / Nonfood\u0026hellip;) Full Shop BASKET_DOMINANT_MISSION object Primary basket mission (Mixed, Grocery, Fresh, Nonfood, \u0026hellip;), indicating dominant product intent Mixed / Fresh STORE_CODE object Store code where the transaction occurred STORE00001 STORE_FORMAT object Store format (LS = Large Store, SS = Small Store, Express, etc.) LS STORE_REGION object Geographic region of the store (E01â€“E05, corresponding to regions in the UK) E02 2.1.2 Feature Groups by Business Context Group Columns Meaning ğŸ›’ Basket BASKET_SIZE, BASKET_TYPE, BASKET_DOMINANT_MISSION Basket size, type, and shopping mission ğŸ’¸ Spending SPEND, QUANTITY Amount spent and quantity purchased ğŸ¬ Store STORE_REGION, STORE_FORMAT Store region and store type ğŸ“¦ Product PROD_CODE_20, PROD_CODE_30 Main product group ğŸ¯ Label BASKET_PRICE_SENSITIVITY Price sensitivity â€“ Low / Medium / High 2.2 Problem Objective Build a multi-class supervised machine learning classifier to predict customer price sensitivity (Low / Medium / High) for each transaction, based on basket features, store context, and shopping behavior.\nğŸ’¡ Use case: Segment customers by price sensitivity â†’ enable dynamic pricing, personalized promotions, and revenue optimization.\n2.3 Problem Type and Models Type: Multi-class classification (Supervised Learning)\nPlanned models:\nModel Reason Decision Tree Easy to interpret feature impact Random Forest High accuracy, reduces overfitting Logistic Regression (multi-class) Baseline for comparison XGBoost Strong performance on tabular data Evaluation: Accuracy, Precision, Recall, F1-score, Confusion Matrix\n2.4 Overall Architecture (AWS MLOps) Core components:\nAmazon S3: Store raw/silver/gold datasets, partitioned by STORE_REGION, BASKET_TYPE Glue/Athena: ETL and data exploration SageMaker Feature Store: Manage feature parity between training â†” inference SageMaker Training \u0026amp; Model Registry: Model training and versioning EKS (FastAPI): Deploy a real-time price sensitivity prediction API CloudWatch: Track latency, real-world accuracy, and costs 2.5 KPIs and Expected Results Group Metric Target ML Accuracy â‰¥ 0.75 ML Macro F1 â‰¥ 0.70 ML Precision (per class) â‰¥ 0.65 Ops P95 latency (API) \u0026lt; 200 ms Ops Throughput (requests/s) â‰¥ 100 Business Reduce pricing mistakes â‰¥ 10% Cost Infrastructure cost/month \u0026lt; $500 3. Project Structure The project is organized in a modular structure with clear separation of concerns:\nretail-forecast/ â”œâ”€â”€ README.md # Project overview \u0026amp; setup guide â”œâ”€â”€ .gitignore # Git ignore patterns â”œâ”€â”€ aws/ # AWS-specific configurations â”‚ â”œâ”€â”€ .travis.yml # Travis CI configuration â”‚ â”œâ”€â”€ Jenkinsfile # Jenkins pipeline configuration â”‚ â”œâ”€â”€ infra/ # Terraform infrastructure â”‚ â”‚ â”œâ”€â”€ main.tf # Main infrastructure config â”‚ â”‚ â”œâ”€â”€ variables.tf # Input variables â”‚ â”‚ â””â”€â”€ output.tf # Output values â”‚ â”œâ”€â”€ k8s/ # Kubernetes manifests â”‚ â”‚ â”œâ”€â”€ deployment.yaml # Application deployment â”‚ â”‚ â”œâ”€â”€ service.yaml # Service configuration â”‚ â”‚ â”œâ”€â”€ hpa.yaml # Horizontal Pod Autoscaler â”‚ â”‚ â””â”€â”€ namespace.yaml # Namespace definition â”‚ â””â”€â”€ script/ # Automation scripts â”‚ â”œâ”€â”€ create_training_job.py # SageMaker training job â”‚ â”œâ”€â”€ register_model.py # Model registry script â”‚ â”œâ”€â”€ deploy_endpoint.py # Model deployment â”‚ â””â”€â”€ autoscaling_endpoint.py # Auto-scaling setup â”œâ”€â”€ azure/ # Azure-specific configurations â”‚ â”œâ”€â”€ azure-pipelines.yml # Azure DevOps pipeline â”‚ â”œâ”€â”€ aml/ # Azure ML configurations â”‚ â”‚ â”œâ”€â”€ train-job.yml # Training job definition â”‚ â”‚ â”œâ”€â”€ train.Dockerfile # Training container â”‚ â”‚ â””â”€â”€ infer.Dockerfile # Inference container â”‚ â”œâ”€â”€ infra/ # Bicep infrastructure â”‚ â”‚ â””â”€â”€ main.bicep # Azure infrastructure â”‚ â””â”€â”€ k8s/ # AKS manifests â”‚ â”œâ”€â”€ deployment.yaml # Application deployment â”‚ â”œâ”€â”€ service.yaml # Service configuration â”‚ â””â”€â”€ hpa.yaml # Horizontal Pod Autoscaler â”œâ”€â”€ core/ # Shared ML core modules â”‚ â””â”€â”€ requirements.txt # Core Python dependencies â”œâ”€â”€ server/ # Inference API server â”‚ â”œâ”€â”€ DockerFile # Container definition â”‚ â”œâ”€â”€ requirements.txt # Server dependencies â”‚ â””â”€â”€ Readme.md # Server documentation â””â”€â”€ tests/ # Test suites â””â”€â”€ (test files) # Unit \u0026amp; integration tests 3.1 Detailed Folder Structure ğŸ“‚ aws/ - AWS Implementation\ninfra/: Terraform Infrastructure as Code k8s/: Kubernetes manifests for EKS deployment script/: Python scripts for SageMaker automation CI/CD configurations (Jenkins, Travis) ğŸ“‚ azure/ - Azure Implementation\ninfra/: Bicep templates for Azure resources aml/: Azure ML configurations k8s/: AKS manifests Azure DevOps pipeline ğŸ“‚ core/ - Shared Components\nCommon ML utilities and libraries Shared dependencies and configurations ğŸ“‚ server/ - Inference API\nFastAPI application Docker containerization API documentation ğŸ“‚ tests/ - Testing Framework\nUnit tests for the ML pipeline Integration tests for infrastructure End-to-end testing scenarios 4. Technologies Used 4.1 Infrastructure \u0026amp; Platform Stack Infrastructure as Code: Terraform for automated provisioning Container Platform: Amazon EKS (Kubernetes) with managed node groups Container Registry: Amazon ECR with vulnerability scanning Networking: Multi-AZ VPC, NAT gateways, security groups Load Balancing: Application Load Balancer with health checks 4.2 ML \u0026amp; Data Platform Stack ML Training: Amazon SageMaker with distributed training Data Storage: Amazon S3 data lake with versioning Model Registry: SageMaker Model Registry for version control Data Processing: Automated preprocessing and feature engineering ML Framework: TensorFlow/PyTorch on SageMaker training jobs 4.3 DevOps \u0026amp; Security Stack CI/CD Platform: Jenkins or Travis CI for automated pipelines Monitoring: CloudWatch (logs, metrics, dashboards, alarms) Security: KMS encryption, CloudTrail auditing, IAM with IRSA DataOps: S3-based data versioning and lifecycle management 5. Detailed MLOps Architecture 5.1 Phase 1: Infrastructure Foundation Terraform Infrastructure as Code\nVPC with multi-AZ public/private subnets EKS cluster with managed node groups (auto-scaling) IAM roles with IRSA (IAM Roles for Service Accounts) Security groups using least privilege access ECR repositories for container images Network Architecture\nPublic subnets: NAT Gateway, Load Balancer Private subnets: EKS worker nodes, SageMaker VPC endpoints: S3, ECR, CloudWatch (reduce data transfer costs) 5.2 Phase 2: ML Training \u0026amp; Model Management SageMaker Training Pipeline\nData Ingestion: S3 data lake with automated validation Distributed Training: SageMaker training jobs with Spot Instances Model Registry: Versioned model artifacts with metadata tracking Experiment Tracking: Performance metrics and hyperparameter tuning Data Management Strategy\nRaw data â†’ Processed data â†’ Feature store â†’ Model artifacts S3 Intelligent-Tiering for cost optimization Data lineage tracking and version control 5.3 Phase 3: Containerized Inference Platform EKS Deployment Architecture\nDocker Containers: FastAPI inference service Kubernetes Deployment: Rolling updates with zero downtime Horizontal Pod Autoscaler: Dynamic scaling based on CPU/memory Service Discovery: Internal service communication Application Load Balancer: External access with SSL termination Monitoring \u0026amp; Observability\nCloudWatch Logs: Centralized logging from all components Custom Metrics: Model performance, latency, throughput Alarms \u0026amp; Notifications: Automated alerting when issues occur Dashboards: Real-time visualization of system health 5.4 Phase 4: CI/CD \u0026amp; Automation Automated Pipeline Flow\n1. Code/Data Change â†’ Git Webhook 2. Jenkins/Travis Build â†’ Run Tests 3. SageMaker Training â†’ Model Validation 4. Docker Build â†’ Push to ECR 5. Kubernetes Deploy â†’ Rolling Update 6. Health Check â†’ Monitor Performance DataOps Workflow\nData Versioning: S3 with metadata tracking Data Quality: Automated validation and testing Feature Engineering: Reproducible pipelines Model Deployment: A/B testing capabilities 6. Scope \u0026amp; Expected Outcomes 6.1 In Scope âœ… Complete Infrastructure: Terraform IaC for all AWS resources\nâœ… ML Training: SageMaker distributed training with hyperparameter tuning\nâœ… Container Deployment: EKS with autoscaling and load balancing\nâœ… Security Best Practices: KMS encryption, CloudTrail auditing, IAM least privilege\nâœ… Monitoring \u0026amp; Alerting: Comprehensive CloudWatch monitoring\nâœ… CI/CD Automation: End-to-end pipeline from code to production\nâœ… Cost Optimization: Auto-scaling, Spot Instances, lifecycle policies\n6.2 Out of Scope âŒ Multi-region deployment (focus on ap-southeast-1)\nâŒ Advanced ML features (A/B testing, canary deployments)\nâŒ Real-time streaming inference (batch-focused)\nâŒ Custom monitoring solutions (CloudWatch-only)\n6.3 Expected Outcomes ğŸ¯ Production-Ready MLOps Platform: Scalable, reliable, cost-effective\nğŸ¯ Automated ML Lifecycle: From data ingestion to model deployment\nğŸ¯ Infrastructure Reproducibility: Terraform state management\nğŸ¯ Operational Excellence: Comprehensive monitoring and alerting\nğŸ¯ Cost Efficiency: Optimized resource usage with auto-scaling\nThis architecture is designed to support enterprise-grade ML workloads, scaling from proof-of-concept to production with millions of requests/day.\nPrerequisites: AWS account with permissions for EKS, SageMaker, ECR, S3, CloudWatch, IAM. Terraform \u0026gt;= 1.0, kubectl, AWS CLI, Docker, Python 3.8+.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Ha Kha Nguyen\nPhone Number: 0827979337\nEmail: hakhanguyen09052004@gmail.com\nUniversity: University of Information Technology\nMajor: Information System\nClass: HTTT2022.2\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, I will introduce my internship worklog.\nDuration: 12 Weeks. Overview: The internship program is structured to provide a comprehensive understanding of AWS services, starting from foundational concepts and progressing to advanced topics, culminating in a Capstone Project. Weekly Contents:\nWeek 1: Introduction to AWS Week 2: Networking on AWS Week 3: Compute VM Service on AWS Week 4: Storage Service on AWS Week 5: Security Services on AWS Week 6: Database Service on AWS Week 7: DataLake Service on AWS Week 8: Serverless Compute Week 9: Container Compute Week 10: DevOps \u0026amp; IaC Week 11: Observability \u0026amp; FinOps Week 12: Capstone: Personal Project "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.2-blog2/",
	"title": "CÃ¡ch tÃ¹y chá»‰nh pháº£n há»“i Ä‘á»‘i vá»›i cÃ¡c cuá»™c táº¥n cÃ´ng DDoS táº§ng 7 báº±ng AWS WAF Anti-DDoS AMR",
	"tags": [],
	"description": "",
	"content": "Bá»Ÿi Achraf Souk | 09 thÃ¡ng 12 nÄƒm 2025 | trong Advanced (300), AWS WAF, Security, Identity, \u0026amp; Compliance\nTrong ná»­a Ä‘áº§u nÄƒm nay, AWS WAF Ä‘Ã£ giá»›i thiá»‡u cÃ¡c biá»‡n phÃ¡p báº£o vá»‡ táº§ng á»©ng dá»¥ng má»›i Ä‘á»ƒ giáº£i quyáº¿t xu hÆ°á»›ng gia tÄƒng cá»§a cÃ¡c cuá»™c táº¥n cÃ´ng tá»« chá»‘i dá»‹ch vá»¥ phÃ¢n tÃ¡n (DDoS) táº§ng 7 (L7) cÃ³ thá»i gian tá»“n táº¡i ngáº¯n vÃ  thÃ´ng lÆ°á»£ng cao. CÃ¡c biá»‡n phÃ¡p báº£o vá»‡ nÃ y Ä‘Æ°á»£c cung cáº¥p thÃ´ng qua nhÃ³m quy táº¯c AWS WAF Anti-DDoS AWS Managed Rules (Anti-DDoS AMR). Máº·c dÃ¹ cáº¥u hÃ¬nh máº·c Ä‘á»‹nh cÃ³ hiá»‡u quáº£ Ä‘á»‘i vá»›i háº§u háº¿t cÃ¡c khá»‘i lÆ°á»£ng cÃ´ng viá»‡c, báº¡n cÃ³ thá»ƒ muá»‘n Ä‘iá»u chá»‰nh pháº£n há»“i Ä‘á»ƒ phÃ¹ há»£p vá»›i má»©c Ä‘á»™ cháº¥p nháº­n rá»§i ro cá»§a á»©ng dá»¥ng.\nTrong bÃ i viáº¿t nÃ y, báº¡n sáº½ há»c cÃ¡ch Anti-DDoS AMR hoáº¡t Ä‘á»™ng vÃ  cÃ¡ch báº¡n cÃ³ thá»ƒ tÃ¹y chá»‰nh hÃ nh vi cá»§a nÃ³ báº±ng cÃ¡ch sá»­ dá»¥ng nhÃ£n vÃ  cÃ¡c quy táº¯c AWS WAF bá»• sung. Báº¡n sáº½ Ä‘Æ°á»£c hÆ°á»›ng dáº«n qua ba tÃ¬nh huá»‘ng thá»±c táº¿, má»—i tÃ¬nh huá»‘ng minh há»a má»™t ká»¹ thuáº­t tÃ¹y chá»‰nh khÃ¡c nhau.\nCÃ¡ch thá»©c hoáº¡t Ä‘á»™ng cá»§a Anti-DDoS AMR HÃ¬nh 1, khi Anti-DDoS AMR phÃ¡t hiá»‡n má»™t cuá»™c táº¥n cÃ´ng DDoS, nÃ³ thÃªm nhÃ£n event-detected vÃ o táº¥t cáº£ cÃ¡c yÃªu cáº§u Ä‘áº¿n vÃ  nhÃ£n ddos-request vÃ o cÃ¡c yÃªu cáº§u Ä‘áº¿n Ä‘Æ°á»£c nghi ngá» gÃ³p pháº§n vÃ o cuá»™c táº¥n cÃ´ng. NÃ³ cÅ©ng thÃªm má»™t nhÃ£n dá»±a trÃªn má»©c Ä‘á»™ tin cáº­y bá»• sung, cháº³ng háº¡n nhÆ° high-suspicion-ddos-request, khi yÃªu cáº§u Ä‘Æ°á»£c nghi ngá» gÃ³p pháº§n vÃ o cuá»™c táº¥n cÃ´ng. Trong AWS WAF, nhÃ£n lÃ  siÃªu dá»¯ liá»‡u Ä‘Æ°á»£c thÃªm vÃ o má»™t yÃªu cáº§u bá»Ÿi má»™t quy táº¯c khi quy táº¯c phÃ¹ há»£p vá»›i yÃªu cáº§u. Sau khi Ä‘Æ°á»£c thÃªm, nhÃ£n cÃ³ sáºµn cho cÃ¡c quy táº¯c tiáº¿p theo, cÃ³ thá»ƒ sá»­ dá»¥ng nÃ³ Ä‘á»ƒ lÃ m phong phÃº logic Ä‘Ã¡nh giÃ¡ cá»§a chÃºng. Anti-DDoS AMR sá»­ dá»¥ng cÃ¡c nhÃ£n Ä‘Æ°á»£c thÃªm Ä‘á»ƒ giáº£m thiá»ƒu cuá»™c táº¥n cÃ´ng DDoS.\nHÃ¬nh 1 â€“ Quy trÃ¬nh cá»§a Anti-DDoS AMR\nCÃ¡c biá»‡n phÃ¡p giáº£m thiá»ƒu máº·c Ä‘á»‹nh dá»±a trÃªn sá»± káº¿t há»£p cá»§a cÃ¡c hÃ nh Ä‘á»™ng Block vÃ  JavaScript Challenge. HÃ nh Ä‘á»™ng Challenge chá»‰ cÃ³ thá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ Ä‘Ãºng cÃ¡ch bá»Ÿi má»™t mÃ¡y khÃ¡ch Ä‘ang mong Ä‘á»£i ná»™i dung HTML. VÃ¬ lÃ½ do nÃ y, báº¡n cáº§n loáº¡i trá»« cÃ¡c Ä‘Æ°á»ng dáº«n cá»§a cÃ¡c yÃªu cáº§u khÃ´ng thá»ƒ thÃ¡ch thá»©c (cháº³ng háº¡n nhÆ° láº¥y API) trong cáº¥u hÃ¬nh Anti-DDoS AMR. Anti-DDoS AMR Ã¡p dá»¥ng nhÃ£n challengeable-request cho cÃ¡c yÃªu cáº§u khÃ´ng khá»›p vá»›i cÃ¡c loáº¡i trá»« thÃ¡ch thá»©c Ä‘Æ°á»£c cáº¥u hÃ¬nh. Theo máº·c Ä‘á»‹nh, cÃ¡c quy táº¯c giáº£m thiá»ƒu sau Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ theo thá»© tá»±:\nâ€¢ ChallengeAllDuringEvent, tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i logic sau: IF event-detected AND challengeable-request THEN challenge.\nâ€¢ ChallengeDDoSRequests, tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i logic sau: If (high-suspicion-ddos-request OR medium-suspicion-ddos-request OR low-suspicion-ddos-request) AND challengeable-request THEN challenge. Äá»™ nháº¡y cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c thay Ä‘á»•i Ä‘á»ƒ phÃ¹ há»£p vá»›i nhu cáº§u cá»§a báº¡n, cháº³ng háº¡n nhÆ° chá»‰ thÃ¡ch thá»©c cÃ¡c yÃªu cáº§u DDoS cÃ³ má»©c Ä‘á»™ nghi ngá» trung bÃ¬nh vÃ  cao.\nâ€¢ DDoSRequests, tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i logic sau: IF high-suspicion-ddos-request THEN block. Äá»™ nháº¡y cá»§a nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c thay Ä‘á»•i Ä‘á»ƒ phÃ¹ há»£p vá»›i nhu cáº§u cá»§a báº¡n, cháº³ng háº¡n nhÆ° cháº·n cÃ¡c yÃªu cáº§u DDoS cÃ³ má»©c Ä‘á»™ nghi ngá» trung bÃ¬nh ngoÃ i viá»‡c cÃ³ má»©c Ä‘á»™ nghi ngá» cao.\nTÃ¹y chá»‰nh pháº£n há»“i Ä‘á»‘i vá»›i cÃ¡c cuá»™c táº¥n cÃ´ng DDoS táº§ng 7 Viá»‡c tÃ¹y chá»‰nh nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng hai cÃ¡ch tiáº¿p cáº­n khÃ¡c nhau. Trong cÃ¡ch tiáº¿p cáº­n Ä‘áº§u tiÃªn, báº¡n cáº¥u hÃ¬nh Anti-DDoS AMR Ä‘á»ƒ thá»±c hiá»‡n hÃ nh Ä‘á»™ng mÃ  báº¡n muá»‘n, sau Ä‘Ã³ báº¡n thÃªm cÃ¡c quy táº¯c tiáº¿p theo Ä‘á»ƒ cá»§ng cá»‘ thÃªm pháº£n há»“i cá»§a báº¡n trong cÃ¡c Ä‘iá»u kiá»‡n nháº¥t Ä‘á»‹nh. Trong cÃ¡ch tiáº¿p cáº­n thá»© hai, báº¡n thay Ä‘á»•i má»™t sá»‘ hoáº·c táº¥t cáº£ cÃ¡c quy táº¯c cá»§a Anti-DDoS AMR thÃ nh cháº¿ Ä‘á»™ Ä‘áº¿m, sau Ä‘Ã³ táº¡o cÃ¡c quy táº¯c bá»• sung xÃ¡c Ä‘á»‹nh pháº£n há»“i cá»§a báº¡n Ä‘á»‘i vá»›i cÃ¡c cuá»™c táº¥n cÃ´ng DDoS.\nTrong cáº£ hai cÃ¡ch tiáº¿p cáº­n, cÃ¡c quy táº¯c tiáº¿p theo Ä‘Æ°á»£c cáº¥u hÃ¬nh báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c Ä‘iá»u kiá»‡n mÃ  báº¡n xÃ¡c Ä‘á»‹nh, káº¿t há»£p vá»›i cÃ¡c Ä‘iá»u kiá»‡n dá»±a trÃªn nhÃ£n Ã¡p dá»¥ng cho cÃ¡c yÃªu cáº§u bá»Ÿi Anti-DDoS AMR. Pháº§n sau bao gá»“m ba vÃ­ dá»¥ vá» viá»‡c tÃ¹y chá»‰nh pháº£n há»“i cá»§a báº¡n Ä‘á»‘i vá»›i cÃ¡c cuá»™c táº¥n cÃ´ng DDoS. Hai vÃ­ dá»¥ Ä‘áº§u tiÃªn dá»±a trÃªn cÃ¡ch tiáº¿p cáº­n Ä‘áº§u tiÃªn, trong khi vÃ­ dá»¥ cuá»‘i cÃ¹ng dá»±a trÃªn cÃ¡ch tiáº¿p cáº­n thá»© hai.\nVÃ­ dá»¥ 1: Giáº£m thiá»ƒu nháº¡y cáº£m hÆ¡n bÃªn ngoÃ i cÃ¡c quá»‘c gia cá»‘t lÃµi Giáº£ sá»­ ráº±ng hoáº¡t Ä‘á»™ng kinh doanh chÃ­nh cá»§a báº¡n Ä‘Æ°á»£c thá»±c hiá»‡n á»Ÿ hai quá»‘c gia chÃ­nh, UAE vÃ  KSA. Báº¡n hÃ i lÃ²ng vá»›i hÃ nh vi máº·c Ä‘á»‹nh cá»§a Anti-DDoS AMR á»Ÿ cÃ¡c quá»‘c gia nÃ y, nhÆ°ng báº¡n muá»‘n cháº·n tÃ­ch cá»±c hÆ¡n bÃªn ngoÃ i cÃ¡c quá»‘c gia nÃ y. Báº¡n cÃ³ thá»ƒ triá»ƒn khai Ä‘iá»u nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c quy táº¯c sau:\nâ€¢ Anti-DDoS AMR vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh â€¢ Má»™t quy táº¯c tÃ¹y chá»‰nh cháº·n náº¿u cÃ¡c Ä‘iá»u kiá»‡n sau Ä‘Æ°á»£c Ä‘Ã¡p á»©ng: YÃªu cáº§u Ä‘Æ°á»£c khá»Ÿi táº¡o tá»« bÃªn ngoÃ i UAE hoáº·c KSA VÃ€ yÃªu cáº§u cÃ³ nhÃ£n high-suspicion-ddos-request hoáº·c medium-suspicion-ddos-request\nCáº¥u hÃ¬nh Sau khi Anti-DDoS AMR cá»§a báº¡n vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh, táº¡o má»™t quy táº¯c tÃ¹y chá»‰nh tiáº¿p theo vá»›i Ä‘á»‹nh nghÄ©a JSON sau:\nLÆ°u Ã½: Báº¡n cáº§n sá»­ dá»¥ng trÃ¬nh chá»‰nh sá»­a quy táº¯c JSON cá»§a AWS WAF hoáº·c cÃ¡c cÃ´ng cá»¥ infrastructure-as-code (IaC) Ä‘á»ƒ Ä‘á»‹nh nghÄ©a quy táº¯c nÃ y. Báº£ng Ä‘iá»u khiá»ƒn AWS WAF hiá»‡n táº¡i khÃ´ng cho phÃ©p táº¡o quy táº¯c vá»›i logic AND/OR lá»“ng nhau.\n{ \u0026#34;Action\u0026#34;: { \u0026#34;Block\u0026#34;: {} }, \u0026#34;Name\u0026#34;: \u0026#34;more-sensitive-ddos-mitigation-outside-of-core-countries\u0026#34;, \u0026#34;Priority\u0026#34;: 1, \u0026#34;Statement\u0026#34;: { \u0026#34;AndStatement\u0026#34;: { \u0026#34;Statements\u0026#34;: [ { \u0026#34;NotStatement\u0026#34;: { \u0026#34;Statement\u0026#34;: { \u0026#34;GeoMatchStatement\u0026#34;: { \u0026#34;CountryCodes\u0026#34;: [ \u0026#34;AE\u0026#34;, \u0026#34;SA\u0026#34; ] } } } }, { \u0026#34;OrStatement\u0026#34;: { \u0026#34;Statements\u0026#34;: [ { \u0026#34;LabelMatchStatement\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;awswaf:managed:aws:anti-ddos:high-suspicion-ddos-request\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;LABEL\u0026#34; } }, { \u0026#34;LabelMatchStatement\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;awswaf:managed:aws:anti-ddos:medium-suspicion-ddos-request\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;LABEL\u0026#34; } } ] } } ] } } } VÃ­ dá»¥ 2: NgÆ°á»¡ng giá»›i háº¡n tá»‘c Ä‘á»™ tháº¥p hÆ¡n trong cÃ¡c cuá»™c táº¥n cÃ´ng DDoS Giáº£ sá»­ ráº±ng á»©ng dá»¥ng cá»§a báº¡n cÃ³ cÃ¡c URL nháº¡y cáº£m tá»‘n nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n. Äá»ƒ báº£o vá»‡ tÃ­nh kháº£ dá»¥ng cá»§a á»©ng dá»¥ng, báº¡n Ä‘Ã£ Ã¡p dá»¥ng má»™t quy táº¯c giá»›i háº¡n tá»‘c Ä‘á»™ cho cÃ¡c URL nÃ y Ä‘Æ°á»£c cáº¥u hÃ¬nh vá»›i ngÆ°á»¡ng 100 yÃªu cáº§u trong cá»­a sá»• 2 phÃºt. Báº¡n cÃ³ thá»ƒ cá»§ng cá»‘ pháº£n há»“i nÃ y trong má»™t cuá»™c táº¥n cÃ´ng DDoS báº±ng cÃ¡ch Ã¡p dá»¥ng ngÆ°á»¡ng tÃ­ch cá»±c hÆ¡n. Báº¡n cÃ³ thá»ƒ triá»ƒn khai Ä‘iá»u nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c quy táº¯c sau:\nMá»™t Anti-DDoS AMR vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh Má»™t quy táº¯c giá»›i háº¡n tá»‘c Ä‘á»™, cÃ³ pháº¡m vi cho cÃ¡c URL nháº¡y cáº£m, Ä‘Æ°á»£c cáº¥u hÃ¬nh vá»›i ngÆ°á»¡ng 100 yÃªu cáº§u trong cá»­a sá»• 2 phÃºt Má»™t quy táº¯c giá»›i háº¡n tá»‘c Ä‘á»™, cÃ³ pháº¡m vi cho cÃ¡c URL nháº¡y cáº£m vÃ  nhÃ£n event-detected, Ä‘Æ°á»£c cáº¥u hÃ¬nh vá»›i ngÆ°á»¡ng 10 yÃªu cáº§u trong cá»­a sá»• 10 phÃºt Cáº¥u hÃ¬nh Sau khi thÃªm Anti-DDoS AMR cá»§a báº¡n vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh vÃ  quy táº¯c giá»›i háº¡n tá»‘c Ä‘á»™ cá»§a báº¡n cho cÃ¡c URL nháº¡y cáº£m, táº¡o má»™t quy táº¯c giá»›i háº¡n tá»‘c Ä‘á»™ má»›i tiáº¿p theo vá»›i Ä‘á»‹nh nghÄ©a JSON sau:\n{ \u0026#34;Action\u0026#34;: { \u0026#34;Block\u0026#34;: {} }, \u0026#34;Name\u0026#34;: \u0026#34;ip-rate-limit-10-10mins-under-ddos\u0026#34;, \u0026#34;Priority\u0026#34;: 2, \u0026#34;Statement\u0026#34;: { \u0026#34;RateBasedStatement\u0026#34;: { \u0026#34;AggregateKeyType\u0026#34;: \u0026#34;IP\u0026#34;, \u0026#34;EvaluationWindowSec\u0026#34;: 600, \u0026#34;Limit\u0026#34;: 10, \u0026#34;ScopeDownStatement\u0026#34;: { \u0026#34;AndStatement\u0026#34;: { \u0026#34;Statements\u0026#34;: [ { \u0026#34;ByteMatchStatement\u0026#34;: { \u0026#34;FieldToMatch\u0026#34;: { \u0026#34;UriPath\u0026#34;: {} }, \u0026#34;PositionalConstraint\u0026#34;: \u0026#34;STARTS_WITH\u0026#34;, \u0026#34;SearchString\u0026#34;: \u0026#34;/sensitive-path\u0026#34;, \u0026#34;TextTransformations\u0026#34;: [ { \u0026#34;Priority\u0026#34;: 0, \u0026#34;Type\u0026#34;: \u0026#34;LOWERCASE\u0026#34; } ] } }, { \u0026#34;LabelMatchStatement\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;awswaf:managed:aws:anti-ddos:event-detected\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;LABEL\u0026#34; } } ] } } } } } VÃ­ dá»¥ 3: Pháº£n há»“i thÃ­ch á»©ng theo kháº£ nÄƒng má»Ÿ rá»™ng á»©ng dá»¥ng cá»§a báº¡n Giáº£ sá»­ ráº±ng báº¡n Ä‘ang váº­n hÃ nh má»™t á»©ng dá»¥ng káº¿ thá»«a cÃ³ thá»ƒ má»Ÿ rá»™ng an toÃ n Ä‘áº¿n má»™t ngÆ°á»¡ng nháº¥t Ä‘á»‹nh vá» khá»‘i lÆ°á»£ng lÆ°u lÆ°á»£ng, sau Ä‘Ã³ nÃ³ bá»‹ suy giáº£m. Náº¿u tá»•ng khá»‘i lÆ°á»£ng lÆ°u lÆ°á»£ng, bao gá»“m cáº£ lÆ°u lÆ°á»£ng DDoS, náº±m dÆ°á»›i ngÆ°á»¡ng nÃ y, báº¡n quyáº¿t Ä‘á»‹nh khÃ´ng thÃ¡ch thá»©c yÃªu cáº§u trong cuá»™c táº¥n cÃ´ng DDoS Ä‘á»ƒ trÃ¡nh lÃ m suy giáº£m tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng. Trong tÃ¬nh huá»‘ng nÃ y, báº¡n chá»‰ dá»±a vÃ o hÃ nh Ä‘á»™ng cháº·n máº·c Ä‘á»‹nh cá»§a cÃ¡c yÃªu cáº§u DDoS cÃ³ má»©c Ä‘á»™ nghi ngá» cao. Náº¿u tá»•ng khá»‘i lÆ°á»£ng lÆ°u lÆ°á»£ng vÆ°á»£t quÃ¡ ngÆ°á»¡ng an toÃ n cá»§a á»©ng dá»¥ng káº¿ thá»«a Ä‘á»ƒ xá»­ lÃ½ lÆ°u lÆ°á»£ng, thÃ¬ báº¡n quyáº¿t Ä‘á»‹nh sá»­ dá»¥ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i biá»‡n phÃ¡p giáº£m thiá»ƒu ChallengeDDoSRequests máº·c Ä‘á»‹nh cá»§a Anti-DDoS AMR. Báº¡n cÃ³ thá»ƒ triá»ƒn khai Ä‘iá»u nÃ y báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c quy táº¯c sau:\nMá»™t Anti-DDoS AMR vá»›i quy táº¯c ChallengeAllDuringEvent vÃ  ChallengeDDoSRequests Ä‘Æ°á»£c cáº¥u hÃ¬nh á»Ÿ cháº¿ Ä‘á»™ Ä‘áº¿m. Má»™t quy táº¯c giá»›i háº¡n tá»‘c Ä‘á»™ Ä‘áº¿m lÆ°u lÆ°á»£ng cá»§a báº¡n vÃ  Ä‘Æ°á»£c cáº¥u hÃ¬nh vá»›i ngÆ°á»¡ng tÆ°Æ¡ng á»©ng vá»›i kháº£ nÄƒng cá»§a á»©ng dá»¥ng Ä‘á»ƒ xá»­ lÃ½ lÆ°u lÆ°á»£ng bÃ¬nh thÆ°á»ng. NhÆ° má»™t hÃ nh Ä‘á»™ng, nÃ³ chá»‰ Ä‘áº¿m yÃªu cáº§u vÃ  Ã¡p dá»¥ng nhÃ£n tÃ¹y chá»‰nhâ€”vÃ­ dá»¥, CapacityExceededâ€”khi Ä‘áº¡t ngÆ°á»¡ng. Má»™t quy táº¯c mÃ´ phá»ng ChallengeDDoSRequests nhÆ°ng chá»‰ khi nhÃ£n CapacityExceeded cÃ³ máº·t: ThÃ¡ch thá»©c náº¿u cÃ¡c nhÃ£n ddos-request, CapacityExceeded, vÃ  challengeable-request cÃ³ máº·t Cáº¥u hÃ¬nh Äáº§u tiÃªn, cáº­p nháº­t Anti-DDoS AMR cá»§a báº¡n báº±ng cÃ¡ch thay Ä‘á»•i cÃ¡c hÃ nh Ä‘á»™ng Challenge thÃ nh cÃ¡c hÃ nh Ä‘á»™ng Count.\nHÃ¬nh 2 â€“ Quy táº¯c Anti-DDoS AMR Ä‘Ã£ cáº­p nháº­t trong vÃ­ dá»¥ 3\nSau Ä‘Ã³ táº¡o quy táº¯c giá»›i háº¡n tá»‘c Ä‘á»™ capacity-exceeded-detection á»Ÿ cháº¿ Ä‘á»™ Ä‘áº¿m, sá»­ dá»¥ng Ä‘á»‹nh nghÄ©a JSON sau:\n{ \u0026#34;Priority\u0026#34;: 7, \u0026#34;RuleLabels\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;mycompany:capacityexceeded\u0026#34; } ], \u0026#34;Statement\u0026#34;: { \u0026#34;RateBasedStatement\u0026#34;: { \u0026#34;AggregateKeyType\u0026#34;: \u0026#34;IP\u0026#34;, \u0026#34;EvaluationWindowSec\u0026#34;: 120, \u0026#34;Limit\u0026#34;: 10000 } }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;capacity-exceeded-detection\u0026#34;, \u0026#34;SampledRequestsEnabled\u0026#34;: true } } Cuá»‘i cÃ¹ng, táº¡o quy táº¯c challenge-if-ddos-and-capacity-exceeded challenge báº±ng Ä‘á»‹nh nghÄ©a JSON sau:\n{ \u0026#34;Action\u0026#34;: { \u0026#34;Challenge\u0026#34;: {} }, \u0026#34;Name\u0026#34;: \u0026#34;challenge-if-ddos-and-capacity-exceeded\u0026#34;, \u0026#34;Priority\u0026#34;: 2, \u0026#34;Statement\u0026#34;: { \u0026#34;AndStatement\u0026#34;: { \u0026#34;Statements\u0026#34;: [ { \u0026#34;LabelMatchStatement\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;mycompany:capacityexceeded\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;LABEL\u0026#34; } }, { \u0026#34;LabelMatchStatement\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;awswaf:managed:aws:anti-ddos:ddos-request\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;LABEL\u0026#34; } }, { \u0026#34;LabelMatchStatement\u0026#34;: { \u0026#34;Key\u0026#34;: \u0026#34;awswaf:managed:aws:anti-ddos:challengeable-request\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;LABEL\u0026#34; } } ] } } } Káº¿t luáº­n Báº±ng cÃ¡ch káº¿t há»£p cÃ¡c biá»‡n phÃ¡p báº£o vá»‡ tÃ­ch há»£p cá»§a Anti-DDoS AMR vá»›i logic tÃ¹y chá»‰nh, báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh cÃ¡c biá»‡n phÃ¡p phÃ²ng thá»§ Ä‘á»ƒ phÃ¹ há»£p vá»›i há»“ sÆ¡ rá»§i ro, mÃ´ hÃ¬nh lÆ°u lÆ°á»£ng vÃ  kháº£ nÄƒng má»Ÿ rá»™ng á»©ng dá»¥ng Ä‘á»™c Ä‘Ã¡o cá»§a mÃ¬nh. CÃ¡c vÃ­ dá»¥ trong bÃ i viáº¿t nÃ y minh há»a cÃ¡ch báº¡n cÃ³ thá»ƒ tinh chá»‰nh Ä‘á»™ nháº¡y, thá»±c thi cÃ¡c biá»‡n phÃ¡p giáº£m thiá»ƒu máº¡nh máº½ hÆ¡n trong cÃ¡c Ä‘iá»u kiá»‡n cá»¥ thá»ƒ vÃ  tháº­m chÃ­ xÃ¢y dá»±ng cÃ¡c biá»‡n phÃ¡p phÃ²ng thá»§ thÃ­ch á»©ng pháº£n á»©ng Ä‘á»™ng vá»›i kháº£ nÄƒng cá»§a há»‡ thá»‘ng.\nBáº¡n cÃ³ thá»ƒ sá»­ dá»¥ng há»‡ thá»‘ng gáº¯n nhÃ£n Ä‘á»™ng trong AWS WAF Ä‘á»ƒ triá»ƒn khai tÃ¹y chá»‰nh má»™t cÃ¡ch chi tiáº¿t. Báº¡n cÅ©ng cÃ³ thá»ƒ sá»­ dá»¥ng nhÃ£n AWS WAF Ä‘á»ƒ loáº¡i trá»« viá»‡c ghi log tá»‘n kÃ©m cá»§a lÆ°u lÆ°á»£ng táº¥n cÃ´ng DDoS.\nLink bÃ i viáº¿t gá»‘c : https://aws.amazon.com/blogs/security/how-to-customize-your-response-to-layer-7-ddos-attacks-using-aws-waf-anti-ddos-amr/\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/2-iam-roles-audit/",
	"title": "IAM Roles &amp; Audit For MLops",
	"tags": [],
	"description": "",
	"content": "ğŸ¯ Task 2 Objectives Set up access permissions (IAM) for all AWS services in the pipeline and enable CloudTrail to monitor and record all activities in the AWS account.\nâ†’ Ensure security, access control, and evidence of team activities.\nğŸ“¥ Input\nAWS Account with admin rights\nProject naming convention: mlops-retail-prediction-dev\nTarget region: ap-southeast-1\nMulti-region CloudTrail enabled\nInput from Task 1: Task 1 (Introduction) â€” project conventions, naming, and high-level objectives\nInput from Task 1: Task 1 (Introduction) â€” project conventions, naming, and high-level objectives\nâœ… Output\nAWS services have appropriate Least Privilege permissions for their roles All operations are recorded by CloudTrail Meets rubric criteria: security, access control, cloud project management ğŸ’° Estimated Cost â‰ˆ 0.05 USD/month (CloudTrail + S3 storage for logs) ğŸ’° Estimated Cost â‰ˆ 0.05 USD/month (CloudTrail + S3 storage for logs)\nğŸ“Œ Main Steps\nCloudTrail Setup - Set up multi-region audit logging S3 CloudTrail Bucket - Centralized log storage EKS Cluster Service Role - Role for control plane EKS Node Group Role - Role for worker nodes SageMaker Execution Role - Role for training \u0026amp; deployment IRSA Foundation - Prepare pod-level permissions âœ… Deliverables\nCloudTrail multi-region trail with logging to S3 EKS Cluster Service Role (Console) EKS Node Group Role with ECR/S3/CloudWatch permissions SageMaker Execution Role with necessary S3 permissions Security foundation for IRSA setup ğŸ“Š Acceptance Criteria\nCloudTrail records all API calls and user activities EKS cluster can be created with appropriate service roles SageMaker training jobs have permissions to read/write S3 Node groups can pull images from ECR All IAM roles comply with principle of least privilege Audit trail ready for compliance and monitoring 1. CloudTrail Setup - Audit Foundation 1.1. Create S3 Bucket for CloudTrail Go to S3 Console: AWS Console â†’ S3 â†’ \u0026ldquo;Create bucket\u0026rdquo;\nBucket Configuration:\nBucket name: mlops-cloudtrail-logs-us-east-1-842676018087\rRegion: us-east-1 (must be same region as CloudTrail trail)\rBlock all public access: âœ… Enabled\rVersioning: âœ… Enabled Default encryption: âœ… AWS KMS\rKMS key: alias/mlops-retail-prediction-dev-cloudtrail-key Lifecycle Policy Configuration:\nStep 1. S3 Console â†’ select bucket mlops-cloudtrail-logs-ap-southeast-1 â†’ Management â†’ Create lifecycle rule.\nStep 2. Name the rule (e.g. CloudTrailLogLifecycle), Apply to all objects or use Prefix mlops-logs/.\nConfiguration field details:\nObject tags:\nâŒ No need to add tags since we\u0026rsquo;re using prefix filtering Object size:\nâŒ No need to specify minimum object size âŒ No need to specify maximum object size CloudTrail logs are typically small and uniform in size Lifecycle rule actions:\nâœ… Transition current versions of objects between storage classes Select to automatically move logs to cheaper storage classes Select to automatically move logs to cheaper storage classes âŒ Transition noncurrent versions of objects between storage classes Not needed as CloudTrail logs don\u0026rsquo;t have many versions âŒ Expire current versions of objects Don\u0026rsquo;t expire as we need to keep logs for audit âŒ Permanently delete noncurrent versions of objects Don\u0026rsquo;t delete as we need to keep log history âœ… Delete expired object delete markers or incomplete multipart uploads Select to clean up failed markers and uploads Step 3. Choose actions (Current versions):\nAfter 30 days â†’ STANDARD_IA After 90 days â†’ GLACIER / GLACIER_IR (optional) After 365 days â†’ DEEP_ARCHIVE Step 4. Verify the rule is Active in the Management tab.\n1.2 Configure Trail âš ï¸ Region Note:\nCloudTrail is a multi-region service but the trail must be created in a specific region (home region) S3 bucket and KMS key must be created in the same region as the CloudTrail trail In this case, we will create all resources in the us-east-1 region 1.3. Recommendation: Synchronize region between S3 and SageMaker Project In short: If the main pipeline data (prefix gold/ and artifacts/) is in us-east-1, create SageMaker Domain / Project in us-east-1 to avoid cross-region errors (S3 301), KMS key complexity, and other endpoint issues.\nIf your organization requires SageMaker to be in ap-southeast-1, you need to copy or replicate data to a bucket in ap-southeast-1 before creating the Project. Example sync commands (PowerShell / CloudShell):\naws s3 mb s3://mlops-retail-prediction-dev-842676018087-apse1 --region ap-southeast-1 aws s3 sync s3://mlops-retail-prediction-dev-842676018087/gold/ s3://mlops-retail-prediction-dev-842676018087-apse1/gold/ --acl bucket-owner-full-control aws s3 sync s3://mlops-retail-prediction-dev-842676018087/artifacts/ s3://mlops-retail-prediction-dev-842676018087-apse1/artifacts/ --acl bucket-owner-full-control Along with:\nCreate KMS key in the destination region if using SSE-KMS. Update IAM policies to allow SageMaker role access to the new bucket. Suggestion: For labs and quick debugging, the least risky approach is to create Project/Domain where the bucket currently exists (in this lab: us-east-1).\n1.2.1 Create KMS Key for CloudTrail Create KMS Key (in region us-east-1): AWS Console â†’ KMS â†’ us-east-1 â†’ Customer managed keys â†’ Create key Key Configuration:\nKey type: âœ… Symmetric (encrypt and decrypt data)\rKey type: âœ… Symmetric (encrypt and decrypt data)\rKey usage: âœ… Encrypt and decrypt Add labels:\nAlias: alias/mlops-retail-prediction-dev-cloudtrail-key\rDescription (optional): KMS key for CloudTrail logs encryption\rTags (optional):\r- Key: Project\r- Value: MLOps-Retail-Prediction Configure administrative permissions:\nKey administrators: Select IAM users/roles allowed to manage key\rKey deletion: Whether to allow key deletion Configure usage permissions:\nKey users: - Add service principal: cloudtrail.amazonaws.com Edit key policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;Key policy created by CloudTrail\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Enable IAM User Permissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [\u0026#34;arn:aws:iam::842676018087:root\u0026#34;] }, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow CloudTrail to encrypt logs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:GenerateDataKey*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;kms:EncryptionContext:aws:cloudtrail:arn\u0026#34;: [ \u0026#34;arn:aws:cloudtrail:*:842676018087:trail/*\u0026#34; ] }, \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:us-east-1:842676018087:trail/mlops-retail-prediction-audit-trail\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow CloudTrail to describe key\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Important points in KMS policy: Enable IAM User Permissions: Allow root account to manage key Allow CloudTrail to encrypt logs:\nAllow generateDataKey with EncryptionContext and SourceArn conditions EncryptionContext limits to CloudTrail trails in account SourceArn specifies exactly which trail is allowed to use Allow CloudTrail to describe key: Allow CloudTrail to view key information Default Key Policy for CloudTrail: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;Key policy created by CloudTrail\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Enable IAM User Permissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::842676018087:root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow CloudTrail to encrypt logs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:GenerateDataKey*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:us-east-1:842676018087:trail/mlops-retail-prediction-audit-trail\u0026#34; }, \u0026#34;StringLike\u0026#34;: { \u0026#34;kms:EncryptionContext:aws:cloudtrail:arn\u0026#34;: \u0026#34;arn:aws:cloudtrail:*:842676018087:trail/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow CloudTrail to describe key\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow principals in the account to decrypt log files\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;*\u0026#34; }, \u0026#34;Action\u0026#34;: [\u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:ReEncryptFrom\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;kms:CallerAccount\u0026#34;: \u0026#34;842676018087\u0026#34; }, \u0026#34;StringLike\u0026#34;: { \u0026#34;kms:EncryptionContext:aws:cloudtrail:arn\u0026#34;: \u0026#34;arn:aws:cloudtrail:*:842676018087:trail/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;Enable cross account log decryption\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;*\u0026#34; }, \u0026#34;Action\u0026#34;: [\u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:ReEncryptFrom\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;kms:CallerAccount\u0026#34;: \u0026#34;842676018087\u0026#34; }, \u0026#34;StringLike\u0026#34;: { \u0026#34;kms:EncryptionContext:aws:cloudtrail:arn\u0026#34;: \u0026#34;arn:aws:cloudtrail:*:842676018087:trail/*\u0026#34; } } } ] } Key Policy includes:\nAllow root account to manage key Allow CloudTrail to encrypt logs with condition that trail ARN matches Allow CloudTrail to view key information Allow principals in account to decrypt logs Support cross-account log decryption if needed KMS key must be created in the same region as the S3 bucket and include a policy that allows CloudTrail usage.\n1.2.2 Configure S3 Bucket Policy Go to S3 bucket permissions:\nS3 Console â†’ mlops-cloudtrail-logs-ap-southeast-1 â†’ Permissions â†’ Bucket policy Default policy for S3 bucket:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AWSCloudTrailAclCheck\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::mlops-cloudtrail-logs-ap-southeast-1-842676018087\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:us-east-1:842676018087:trail/mlops-retail-prediction-audit-trail\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSCloudTrailWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::mlops-cloudtrail-logs-ap-southeast-1-842676018087/mlops-logs/AWSLogs/842676018087/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:us-east-1:842676018087:trail/mlops-retail-prediction-audit-trail\u0026#34;, \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34; } } } ] } S3 Bucket Policy includes:\nAWSCloudTrailAclCheck: Allow CloudTrail to check bucket ACL AWSCloudTrailWrite: Allow CloudTrail to write logs to bucket Conditions: aws:SourceArn: Ensure only specific trail can access s3:x-amz-acl: Ensure bucket owner has full control of objects This policy allows CloudTrail to check bucket ACL and write logs to bucket.\n1.2.3 Create the CloudTrail Step 1: Create a new trail\nAWS Console â†’ us-east-1 â†’ CloudTrail â†’ Create trail Step 2: Basic trail configuration (in us-east-1)\nItem Value Trail name mlops-retail-prediction-audit-trail Apply trail to all regions âœ… Yes Management events âœ… Read/Write Data events âœ… S3 bucket data events Insights events âœ… Enabled (detect anomalous behavior) Step 3: Storage configuration\nItem Value S3 bucket mlops-cloudtrail-logs-ap-southeast-1 Log file prefix mlops-logs/ Log file SSE-KMS encryption âœ… Enabled AWS KMS alias alias/mlops-retail-prediction-dev-cloudtrail-key (select the created key) Step 4: CloudWatch Logs integration (optional)\nItem Value CloudWatch Logs âœ… Enabled Log group mlops-cloudtrail-log-group IAM Role CloudTrail_CloudWatchLogs_Role (auto-created) The CloudTrail_CloudWatchLogs_Role will be auto-created with necessary permissions: logs:PutLogEvents, logs:CreateLogStream, logs:DescribeLogStreams.\nStep 5: Review and create the trail\nCorrect order to avoid errors:\nâœ… Create KMS key with proper policy âœ… Configure S3 bucket policy âœ… Create CloudTrail with the configured KMS and S3 âœ… Verify logs are being delivered 2. IAM Roles Setup - Service permissions 2.1. EKS Cluster Service Role AWS Console â†’ IAM â†’ Roles â†’ \u0026ldquo;Create role\u0026rdquo; Trusted entity type: /* Lines 445-447 omitted */ Attach policies: /* Lines 450-451 omitted */ Role details: /* Lines 454-456 omitted */ 2.2. EKS Node Group Role Similar steps Trusted entity type: /* Lines 463-465 omitted */ Attach policies: /* Lines 468-472 omitted */ Role details: /* Lines 475-478 omitted */ 2.3. SageMaker Execution Role Trusted entity type:\n/* Lines 484-486 omitted */ Attach policies:\n/* Lines 489-492 omitted */ Add required inline EC2 policy (REQUIRED for Projects):\n/* Lines 496-521 omitted */ Policy name: SageMakerEC2Access Role details:\n/* Lines 525-527 omitted */ 2.4. REQUIRED: Add EC2 Permissions Because SageMaker Projects are mandatory, EC2 permissions must be added:\nIAM Console â†’ Roles â†’ mlops-retail-prediction-dev-sagemaker-execution Permissions tab â†’ Add permissions â†’ Create inline policy JSON tab â†’ paste policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, /* Lines 540-553 omitted */ ] } Review â†’ Policy name: SageMakerEC2Access Create policy âœ… Verify: the role should have these 4 policies attached:\nAmazonSageMakerFullAccess (AWS managed) AmazonS3FullAccess (AWS managed) AmazonS3FullAccess (AWS managed) CloudWatchLogsFullAccess (AWS managed) SageMakerEC2Access (the inline policy created above) SageMaker Unified Studio (2024+) requirements:\nâœ… Projects are mandatory âœ… EC2 permissions are REQUIRED (add inline policy) âœ… Project profile must be configured first If EC2 permissions are missing:\nâŒ Project creation will fail âŒ \u0026ldquo;Insufficient permissions to describe VPCs\u0026rdquo; errors âŒ Studio notebooks inaccessible âœ… SOLUTION:\nAdd the inline EC2 policy above Create the Project with \u0026ldquo;ML and generative AI model development\u0026rdquo; Studio should work normally 3. Validation \u0026amp; Security checks 3.1. Verify CloudTrail Check CloudTrail status: AWS Console â†’ CloudTrail â†’ Trails\nâœ… mlops-retail-prediction-audit-trail: Active\râœ… Multi-region trail: Enabled\râœ… Management events: Read/Write\râœ… Data events: S3 configured\râœ… CloudWatch Logs: Integrated Verify S3 logging: AWS Console â†’ S3 â†’ mlops-cloudtrail-logs-ap-southeast-1\nâœ… Log files present: /mlops-logs/AWSLogs/[account-id]/CloudTrail/\râœ… Encryption: SSE-S3 enabled\râœ… Lifecycle policy: Applied\râœ… Access logging: Configured 3.2. IAM Roles summary Go to IAM â†’ Roles and verify:\nâœ… mlops-retail-prediction-dev-eks-cluster-role\râœ… mlops-retail-prediction-dev-eks-nodegroup-role\râœ… mlops-retail-prediction-dev-sagemaker-execution\râœ… CloudTrail_CloudWatchLogs_Role (auto-created) Check Trust Relationships:\nOpen each role â†’ Trust relationships tab Verify trusted entities such as eks.amazonaws.com (EKS cluster role) /* Lines 622-624 omitted */ cloudtrail.amazonaws.com (CloudTrail logging role) 3.3. Security checks Verify CloudTrail logging:\nMake a test API call (e.g., list S3 buckets) Check CloudTrail logs within 5â€“10 minutes Confirm the event appears in CloudWatch Logs Verify IAM permissions:\n# Test SageMaker role can assume and access S3 aws sts assume-role --role-arn arn:aws:iam::ACCOUNT:role/mlops-retail-prediction-dev-sagemaker-execution --role-session-name test Test SageMaker role full access:\n# Test S3 access aws s3 ls s3://mlops-retail-prediction-dev-ACCOUNT/ --profile sagemaker-test # Test SageMaker training job permissions aws sagemaker list-training-jobs --region us-east-1 --profile sagemaker-test # Test EC2 permissions (if added) # Test EC2 permissions (if added) aws ec2 describe-vpcs --region us-east-1 --profile sagemaker-test # Test EKS roles readiness for cluster creation aws eks describe-cluster --name test-cluster --region ap-southeast-1 4. Cost optimization \u0026amp; compliance 4.1 CloudTrail cost management â€” comparison table Item Price Notes / Assumptions Example estimate S3 â€” Standard $0.023 / GBâ€‘month Hot logs (day 0â€“30) 1 GB â†’ $0.023 S3 â€” Standardâ€‘IA $0.0125 / GBâ€‘month After 30 days (infrequent access) 1 GB â†’ $0.0125 S3 â€” Glacier $0.004 / GBâ€‘month Long-term retention (90â€“365 days) 1 GB â†’ $0.004 S3 â€” Deep Archive $0.00099 / GBâ€‘month Retention \u0026gt;365 days 1 GB â†’ $0.00099 CloudTrail â€” Management events Free (first copy) Management API calls â€” CloudTrail â€” Data events $0.10 / 100,000 events S3 object-level, Lambda, etc. 100k events â†’ $0.10 CloudTrail â€” Insights $0.35 / 100,000 events Optional anomaly detection 100k events â†’ $0.35 Sample monthly scenarios\nMinimal (small project): 0.5 GB storage (mostly Standardâ€‘IA) + 10k data events\nâ†’ S3 â‰ˆ 0.5 * $0.0125 = $0.0063 ; Data events â‰ˆ (10k/100k)*$0.10 = $0.01\nâ†’ Total â‰ˆ $0.016 Typical (tens of GB, tens of thousands of events): 5 GB mixed classes + 50k data events\nâ†’ S3 (mix) â‰ˆ $0.02â€“$0.04 ; Data events â‰ˆ $0.05 ; Insights may add $0.00â€“$0.35\nâ†’ Total ~ $0.02â€“$0.05 (common for small projects) Notes:\nLifecycle transitions to IA/Glacier/Deep Archive are key for long-term cost reduction. Data events and Insights scale with event count â€” optimize sampling and only log whatâ€™s necessary. Verify actual spend using Billing / Cost Explorer to adjust assumptions. 5. Clean Up Resources (CLI guide) Warning: commands below will delete real resources. Confirm names (bucket, role, trail, key) before running.\n5.1 Delete CloudTrail PowerShell (AWS CLI):\n# Delete the trail (if name matches) aws cloudtrail delete-trail --name mlops-retail-prediction-audit-trail # Optionally disable CloudWatch Logs integration first aws cloudtrail update-trail --name mlops-retail-prediction-audit-trail --cloud-watch-logs-log-group-arn \u0026#34;\u0026#34; --cloud-watch-logs-role-arn \u0026#34;\u0026#34; 5.2 Delete S3 CloudTrail bucket and contents Note: bucket may be in us-east-1 per this guide. Verify with aws s3 ls / console before deleting.\n# Remove all objects recursively aws s3 rm s3://mlops-cloudtrail-logs-ap-southeast-1 --recursive # Delete the bucket # Delete the bucket aws s3api delete-bucket --bucket mlops-cloudtrail-logs-ap-southeast-1 --region us-east-1 5.3 Schedule KMS Key deletion KMS keys cannot be immediately deleted if in use. Schedule deletion (e.g., 7 days):\n# Find KeyId from alias # Find KeyId from alias $keyId = aws kms list-aliases --query \u0026#34;Aliases[?AliasName==\u0026#39;alias/mlops-retail-prediction-dev-cloudtrail-key\u0026#39;].TargetKeyId\u0026#34; --output text # Schedule key deletion (pending window: 7-30 days) aws kms schedule-key-deletion --key-id $keyId --pending-window-in-days 7 5.4 Remove IAM Roles \u0026amp; Policies (EKS / SageMaker / CloudTrail) Safe procedure: 1) Detach managed policies 2) Delete inline policies 3) Delete role.\n# Example: delete SageMaker execution role $roleName = \u0026#39;mlops-retail-prediction-dev-sagemaker-execution\u0026#39; # 1) List and detach attached managed policies aws iam list-attached-role-policies --role-name $roleName --query \u0026#39;AttachedPolicies[].PolicyArn\u0026#39; --output text | ForEach-Object { aws iam detach-role-policy --role-name $roleName --policy-arn $_ } # 2) Delete inline policies # 2) Delete inline policies aws iam list-role-policies --role-name $roleName --query \u0026#39;PolicyNames\u0026#39; --output text | ForEach-Object { aws iam delete-role-policy --role-name $roleName --policy-name $_ } # 3) Delete the role # 3) Delete the role aws iam delete-role --role-name $roleName # Repeat for other roles (EKS cluster/nodegroup, CloudTrail_CloudWatchLogs_Role, CI roles, etc.) 5.5 Remove Container Insights / CloudWatch integration # Delete CloudWatch log group (if exists) aws logs delete-log-group --log-group-name \u0026#34;/aws/containerinsights/mlops-retail-cluster/application\u0026#34; || Write-Host \u0026#39;Log group not found\u0026#39; # Delete CloudTrail log group aws logs delete-log-group --log-group-name \u0026#34;mlops-cloudtrail-log-group\u0026#34; || Write-Host \u0026#39;Log group not found\u0026#39; # Disable Container Insights addon from EKS (if applied) # Disable Container Insights addon from EKS (if applied) aws eks delete-addon --cluster-name mlops-retail-cluster --addon-name amazon-cloudwatch-observability 5.6 Delete ECR images (optional) # Delete images by tag # Delete images by tag aws ecr batch-delete-image --repository-name mlops/retail-api --image-ids imageTag=dev,imageTag=staging || Write-Host \u0026#39;No matching images or already deleted\u0026#39; # Delete untagged images (caution) aws ecr describe-images --repository-name mlops/retail-api --filter tagStatus=UNTAGGED --query \u0026#39;imageDetails[].imageDigest\u0026#39; --output text | ForEach-Object { aws ecr batch-delete-image --repository-name mlops/retail-api --image-ids imageDigest=$_ } 5.7 Stop / Delete SageMaker training jobs, endpoints, model packages # Stop in-progress training jobs by name pattern aws sagemaker list-training-jobs --name-contains \u0026#34;retail-\u0026#34; --status-equals InProgress --query \u0026#39;TrainingJobSummaries[].TrainingJobName\u0026#39; --output text | ForEach-Object { aws sagemaker stop-training-job --training-job-name $_ } # Delete failed endpoints aws sagemaker list-endpoints --name-contains \u0026#34;retail-\u0026#34; --query \u0026#39;Endpoints[?EndpointStatus==`Failed`].EndpointName\u0026#39; --output text | ForEach-Object { aws sagemaker delete-endpoint --endpoint-name $_ } # Delete pending model packages in model group (careful: keep approved ones) aws sagemaker list-model-packages --model-package-group-name \u0026#34;retail-forecast-models\u0026#34; --model-approval-status PendingManualApproval --query \u0026#39;ModelPackageSummaryList[].ModelPackageArn\u0026#39; --output text | ForEach-Object { aws sagemaker delete-model-package --model-package-name $_ } 5.8 Verification # Check trail deletion aws cloudtrail describe-trails --query \u0026#39;trailList[?Name==`mlops-retail-prediction-audit-trail`]\u0026#39; || Write-Host \u0026#39;Trail removed or not found\u0026#39; # Check bucket /* Lines 785-844 omitted */ "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Retail Price Sensitivity MLOps Platform on AWS End-to-End MLOps Pipeline: Data Lake â†’ Training â†’ Model Registry â†’ Container â†’ EKS API â†’ Monitoring â†’ CI/CD â†’ Cost Control 1. Executive Summary This workshop delivers a complete, production-oriented MLOps workflow on AWS for a Retail Prediction API that predicts BASKET_PRICE_SENSITIVITY (Low/Medium/High) from curated retail features. The system uses an S3-based data lake (raw/silver/gold), an automated training pipeline on Amazon SageMaker (RandomForest), Model Registry for versioning/approval, containerization with Amazon ECR, and scalable API deployment on Amazon EKS behind a public load balancer for demo usage. Observability is implemented with Amazon CloudWatch, and delivery is automated with a CI/CD pipeline (GitHub Actions or Jenkins). The workshop also emphasizes cost optimization and a clean teardown strategy to avoid unexpected charges.\n2. Problem Statement 2.1 Whatâ€™s the Problem? Deploying ML to production is more than training a model. Teams often struggle with:\nData pipelines that are not repeatable (manual ETL, inconsistent splits) No model governance (no registry, no approval workflow, no version traceability) Inference code that cannot be deployed reliably (no container standardization) Kubernetes deployment complexity (networking, IAM permissions, image pulls) Missing monitoring/logging (hard to debug outages and cost spikes) Costs that grow quickly when infrastructure runs 24/7 2.2 The Solution This workshop builds a unified MLOps system that:\nStandardizes data flow in S3: silver/ â†’ gold/ with deterministic train/val/test splits Trains a RandomForest classifier on SageMaker with evaluation targets (Accuracy â‰¥ 0.8, F1 â‰¥ 0.7) Stores artifacts in S3 artifacts/ and manages model versions in SageMaker Model Registry (register + approve) Packages inference API as a hardened FastAPI container and pushes to ECR with lifecycle + scan-on-push Deploys to EKS with HPA autoscaling and public endpoint for demo (/health, /docs, /predict) Adds CloudWatch monitoring (Container Insights, alarms, log retention) Adds CI/CD automation for buildâ€“scanâ€“pushâ€“deploy and optional retrain+register Applies cost controls (Spot, schedules, lifecycle rules, teardown scripts) 2.3 Benefits and Return on Investment Faster, repeatable deployment of ML models with clear version governance Easier debugging and reliability through monitoring and health checks Lower operational costs through Spot/scheduling and lifecycle policies A reusable reference architecture for future ML services (not limited to retail) 3. Solution Architecture 3.1 High-Level Architecture (Textual) Data Layer\nAmazon S3: raw/, silver/, gold/, artifacts/, logs/, tmp/ Automated ETL from silver partitions â†’ gold splits (train/val/test) Training \u0026amp; Governance\nAmazon SageMaker Training (RandomForest) Evaluation + metrics logging (CloudWatch) Model artifacts stored in S3 artifacts/ SageMaker Model Registry: Model Package Group + approve versions Container \u0026amp; Deployment\nAmazon ECR: mlops/retail-api (tag immutability, scan-on-push, lifecycle rules) Amazon EKS: mlops-retail-cluster (Production VPC, private subnets for nodes/pods) Public entry point for demo via LoadBalancer/ALB (enabled only during demos) Observability \u0026amp; Automation\nAmazon CloudWatch: Container Insights, log groups retention, alarms CI/CD: GitHub Actions (OIDC) or Jenkins pipeline for build/test/scan/push/deploy + optional retraining Cost Management: S3/ECR lifecycle, log retention, schedule stop/start, teardown scripts Region alignment requirement: All components are aligned to ap-southeast-1 to avoid cross-region issues (e.g., S3 301 redirects).\n3.2 AWS Services Used Amazon S3: data lake + model artifacts + logs Amazon SageMaker: training jobs + Model Registry Amazon ECR: private image registry for inference API Amazon EKS: production deployment platform for FastAPI Elastic Load Balancing (ALB/NLB): public demo endpoint for /predict and /docs Amazon CloudWatch: logs, metrics, Container Insights, alarms AWS IAM + IRSA: secure access from pods to AWS (S3/SageMaker/CloudWatch) Amazon VPC + VPC Endpoints: private connectivity for ECR/S3/CloudWatch Logs (no NAT gateway) CI/CD tooling: GitHub Actions (OIDC role) or Jenkins on EC2 3.3 Component Design ETL \u0026amp; Dataset Prep: Auto-generate gold splits from silver partitions; store under S3 gold/ Model Training: SageMaker training job writes artifacts to S3 artifacts/; outputs metrics Model Governance: Register model packages; only approved versions are used by inference Inference Service: FastAPI app loads approved model from registry, supports /predict K8s Reliability: health probes, multi-replica deployment, autoscaling via HPA Security: private subnets, endpoints, least-privilege IAM, image scanning 4. Technical Implementation 4.1 Implementation Phases (Mapped to Workshop Tasks) Task 4 â€“ Model Training Pipeline (SageMaker): ETL silverâ†’gold, train RF, evaluate, artifacts in S3, register/approve in Model Registry Task 5 â€“ Production Networking: Separate Production VPC (10.0.0.0/16), private subnets for EKS, public subnets for ALB (demo-only), VPC endpoints, no NAT Task 6 â€“ ECR: Hardened multi-stage FastAPI container, scan-on-push, lifecycle, push scripts Task 7 â€“ EKS Setup: Cluster + add-ons + IRSA + nodegroup, deploy sample app and verify pulls from ECR Task 8 â€“ API Deployment: Deploy retail API to namespace mlops, ServiceAccount with IRSA role, LB endpoint + HPA + tests Task 9 â€“ Load Balancing: Improve demo endpoint via ALB/Ingress (optional AWS Load Balancer Controller), TLS/DNS (optional) Task 10 â€“ Monitoring: Container Insights, log retention, alarms, Logs Insights queries Task 11 â€“ CI/CD: Automate build/test/scan/push/deploy; optional retrain+register; rollback hooks Task 12 â€“ Cost \u0026amp; Teardown: Spot/schedules/lifecycle/budgets and full teardown scripts 4.2 Technical Requirements AWS account access with permissions for EKS, SageMaker, ECR, S3, VPC, CloudWatch, IAM Docker + AWS CLI configured locally (or CloudShell) Kubernetes tooling (kubectl, eksctl optional), Helm (if using Load Balancer Controller) Repository structure aligned with workshop (e.g., aws/scripts/, aws/k8s/, aws/infra/) 5. Timeline \u0026amp; Milestones Milestone 1: Data pipeline + SageMaker training + Model Registry operational (model versioned \u0026amp; approved) Milestone 2: Containerized FastAPI image in ECR (scan/lifecycle enabled) Milestone 3: EKS cluster + IRSA working; API deployed with health checks \u0026amp; HPA Milestone 4: Public demo endpoint available; monitoring dashboards/alarms configured Milestone 5: CI/CD working end-to-end; cost controls validated; teardown verified 6. Budget Estimation The workshop emphasizes cost-minimized production-like setup, including:\nEKS nodes (prefer Spot / small dev instances for demos) Load balancer enabled only when presenting demos SageMaker training optimized (managed spot training if possible) CloudWatch logs retention (7â€“30 days) S3/ECR lifecycle policies to reduce storage costs Budget alerts to prevent overruns (Optional: attach AWS Pricing Calculator screenshot or a simple cost table from Task 12.)\n7. Risk Assessment 7.1 Risk Matrix (Typical Workshop Risks) Cost overruns: Medium impact, medium probability Networking issues (private subnets/endpoints): High impact, medium probability IAM/IRSA misconfiguration: High impact, medium probability EKS image pull failures (ECR auth/endpoint): Medium impact, medium probability Model mismatch / wrong approved version: Medium impact, low probability 7.2 Mitigation Strategies Enforce region alignment and naming conventions Use VPC endpoints (S3 gateway, ECR API/DKR, CloudWatch Logs) to avoid NAT dependency Use least-privilege IAM policies and validate IRSA early with a smoke test pod Health checks + rollback steps in CI/CD Budget alerts + teardown scripts run after demo 8. Expected Outcomes 8.1 Technical Outcomes Fully working MLOps workflow: S3 â†’ SageMaker Training â†’ Model Registry â†’ ECR â†’ EKS API Public demo endpoint with autoscaling and health checks Monitoring and logs for debugging and operational visibility CI/CD pipeline enabling repeatable deployments 8.2 Long-term Value A reusable blueprint for deploying additional ML APIs Clear governance and reproducibility for model versions and artifacts Cost-aware operational practices suitable for student projects and production prototypes "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.1-week1/",
	"title": "Week 1 - Introduction to AWS",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Familiarize with AWS Global Infrastructure (Regions, AZs) and Shared Responsibility Model. Master AWS Console \u0026amp; CLI configuration. Implement basic security practices (IAM User/Role, MFA). Tasks to be carried out this week: Day Task Reference 2 - Setup: Create AWS Account, enable MFA for root user.\n- IAM: Create IAM Groups/Users with Least Privilege principles. AWS IAM Docs 3 - CLI: Install AWS CLI v2, configure profiles (~/.aws/config, credentials).\n- Verify: Run aws sts get-caller-identity to check auth. AWS CLI Guide 4 - S3 Basics: Create a bucket with \u0026ldquo;Block Public Access\u0026rdquo; enabled.\n- Practice Upload/Download objects via Console \u0026amp; CLI. AWS S3 Docs 5 - Security: Study Shared Responsibility Model.\n- Define tagging conventions (Owner, Project, Env). FCJ Materials 6 - Review: Document setup process and verify billing alerts.\n- Weekly Report. - Week 1 Achievements: Secured AWS account with MFA and no root access for daily tasks. Configured AWS CLI profiles for different environments. Created secure S3 buckets with encryption and versioning enabled. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.3-blog3/",
	"title": "AWS Marketplace: Cháº¥t xÃºc tÃ¡c cho viá»‡c hiá»‡n Ä‘áº¡i hÃ³a mua sáº¯m liÃªn bang",
	"tags": [],
	"description": "",
	"content": "BÃ i viáº¿t gá»‘c Ä‘Æ°á»£c Ä‘Äƒng vÃ o ngÃ y 09 thÃ¡ng 12 nÄƒm 2025 | trong má»¥c ThÃ´ng bÃ¡o, AWS Marketplace, ChÃ­nh phá»§, Khu vá»±c cÃ´ng\nCáº£nh quan mua sáº¯m liÃªn bang cá»§a Hoa Ká»³ Ä‘ang tráº£i qua quÃ¡ trÃ¬nh chuyá»ƒn Ä‘á»•i máº¡nh máº½ nháº¥t trong nhiá»u tháº­p ká»·. Quy Ä‘á»‹nh Cáº£i cÃ¡ch Mua sáº¯m LiÃªn bang CÃ¡ch máº¡ng (FAR) vÃ  cÃ¡c lá»‡nh hÃ nh phÃ¡p nhÆ° \u0026ldquo;KhÃ´i phá»¥c LÃ½ luáº­n Chung cho Mua sáº¯m LiÃªn bang\u0026rdquo; (14005) vÃ  \u0026ldquo;NÃ¢ng cao Giáº£i phÃ¡p Hiá»‡u quáº£ Chi phÃ­, ThÆ°Æ¡ng máº¡i trong Há»£p Ä‘á»“ng LiÃªn bang\u0026rdquo; (14271) Ä‘áº¡i diá»‡n cho viá»‡c tÃ¡i tÆ°á»Ÿng tÆ°á»£ng cÃ¡ch cÃ¡c cÆ¡ quan chÃ­nh phá»§ mua hÃ ng hÃ³a vÃ  dá»‹ch vá»¥. Cáº£i cÃ¡ch nÃ y nháº±m má»¥c Ä‘Ã­ch má»Ÿ ra má»™t ká»· nguyÃªn má»›i vá» mua sáº¯m há»£p lÃ½, thÆ°Æ¡ng máº¡i-Ä‘áº§u tiÃªn.\nTrá»ng tÃ¢m cá»§a sá»± chuyá»ƒn Ä‘á»•i nÃ y lÃ  má»™t táº§m nhÃ¬n tÃ¡o báº¡o: Mua sáº¯m liÃªn bang nÃªn nhanh nháº¹n vÃ  hiá»‡u quáº£ nhÆ° thÆ°Æ¡ng máº¡i hiá»‡n Ä‘áº¡i. Giá»›i thiá»‡u AWS Marketplace - má»™t giáº£i phÃ¡p cÃ³ thá»ƒ giÃºp khÃ¡ch hÃ ng liÃªn bang Ä‘iá»u hÆ°á»›ng ká»· nguyÃªn má»›i nÃ y cá»§a mua sáº¯m liÃªn bang. AWS Marketplace cung cáº¥p nhá»¯ng lá»i há»©a cá»§a RFO: cÃ¡c giáº£i phÃ¡p cÃ³ sáºµn thÆ°Æ¡ng máº¡i, quy trÃ¬nh há»£p lÃ½, triá»ƒn khai nhanh chÃ³ng vÃ  quy trÃ¬nh mua sáº¯m Ä‘Æ¡n giáº£n hÃ³a - táº¥t cáº£ trong khi giÃºp khÃ¡ch hÃ ng chÃ­nh phá»§ duy trÃ¬ cÃ¡c yÃªu cáº§u báº£o máº­t vÃ  tuÃ¢n thá»§.\nAWS Marketplace Ä‘Ã£ cÃ³ máº·t trong hÆ¡n má»™t tháº­p ká»·, nhÆ°ng cÃ¡c phÆ°Æ¡ng phÃ¡p tÄƒng tá»‘c mua sáº¯m CNTT cá»§a nÃ³ Ä‘Ã¡p á»©ng thá»i Ä‘iá»ƒm hiá»‡n táº¡i:\n1. Mua sáº¯m há»£p lÃ½ AWS Marketplace cung cáº¥p má»™t danh má»¥c sá»‘ cho phÃ©p cÃ¡c cÆ¡ quan nhanh chÃ³ng khÃ¡m phÃ¡, mua sáº¯m vÃ  triá»ƒn khai pháº§n má»m vÃ  dá»‹ch vá»¥ cá»§a bÃªn thá»© ba. Äiá»u nÃ y giÃºp cÃ¡c cÆ¡ quan phÃ¹ há»£p vá»›i má»¥c tiÃªu cá»§a RFO vá» viá»‡c Ä‘Æ¡n giáº£n hÃ³a quy trÃ¬nh mua sáº¯m.\n2. Tiáº¿p cáº­n cÃ¡c giáº£i phÃ¡p thÆ°Æ¡ng máº¡i RFO nháº¥n máº¡nh viá»‡c sá»­ dá»¥ng cÃ¡c sáº£n pháº©m vÃ  dá»‹ch vá»¥ thÆ°Æ¡ng máº¡i. AWS Marketplace cung cáº¥p má»™t lá»±a chá»n rá»™ng lá»›n vá» pháº§n má»m thÆ°Æ¡ng máº¡i cÃ³ sáºµn (COTS) vÃ  giáº£i phÃ¡p tá»« hÃ ng nghÃ¬n nhÃ  cung cáº¥p, giÃºp cÃ¡c cÆ¡ quan dá»… dÃ ng tiáº¿p cáº­n Ä‘á»•i má»›i ngÃ nh cÃ´ng nghiá»‡p.\n3. Hiá»‡u quáº£ vÃ  tiáº¿t kiá»‡m chi phÃ­ Báº±ng cÃ¡ch há»£p nháº¥t mua sáº¯m thÃ´ng qua AWS Marketplace, cÃ¡c cÆ¡ quan cÃ³ thá»ƒ giáº£m chi phÃ­ hÃ nh chÃ­nh vÃ  tiá»m nÄƒng hÆ°á»Ÿng lá»£i tá»« viá»‡c mua hÃ ng theo quy mÃ´. Äiá»u nÃ y giÃºp cÃ¡c cÆ¡ quan Ä‘Ã¡p á»©ng má»¥c tiÃªu cá»§a RFO vá» viá»‡c táº¡o ra má»™t há»‡ thá»‘ng mua sáº¯m hiá»‡u quáº£ vÃ  tiáº¿t kiá»‡m chi phÃ­ hÆ¡n.\nCÃ¡c lá»£i Ã­ch bá»• sung cá»§a AWS Marketplace Ä‘á»‘i vá»›i ngÆ°á»i mua liÃªn bang bao gá»“m:\n1. TÃ¹y chá»n Ä‘á»‹nh giÃ¡ linh hoáº¡t AWS Marketplace cung cáº¥p cÃ¡c mÃ´ hÃ¬nh Ä‘á»‹nh giÃ¡ Ä‘a dáº¡ng, bao gá»“m tráº£ theo sá»­ dá»¥ng, Ä‘Äƒng kÃ½ hÃ ng nÄƒm vÃ  cÃ¡c Æ°u Ä‘Ã i riÃªng. TÃ­nh linh hoáº¡t nÃ y cho phÃ©p cÃ¡c cÆ¡ quan tá»‘i Æ°u hÃ³a chi tiÃªu vÃ  phÃ¹ há»£p vá»›i cÃ¡c háº¡n cháº¿ ngÃ¢n sÃ¡ch.\n2. TuÃ¢n thá»§ vÃ  báº£o máº­t Nhiá»u sáº£n pháº©m trong AWS Marketplace cÃ³ sáºµn cho sá»­ dá»¥ng chÃ­nh phá»§, bao gá»“m cÃ¡c giáº£i phÃ¡p cÃ³ sáºµn trong AWS GovCloud (US), AWS Secret vÃ  cÃ¡c vÃ¹ng AWS Top Secret thÃ´ng qua AWS Marketplace cho Cá»™ng Ä‘á»“ng TÃ¬nh bÃ¡o Hoa Ká»³ (IC2P). KhÃ¡ch hÃ ng cÃ³ thá»ƒ lá»c theo cÃ¡c dá»‹ch vá»¥ Ä‘Ã¡p á»©ng cÃ¡c cháº¿ Ä‘á»™ tuÃ¢n thá»§ nhÆ° FedRAMP. Äiá»u nÃ y giÃºp cÃ¡c cÆ¡ quan Ä‘Ã¡p á»©ng cÃ¡c yÃªu cáº§u tuÃ¢n thá»§ má»™t cÃ¡ch dá»… dÃ ng hÆ¡n.\n3. Quáº£n trá»‹ táº­p trung Vá»›i AWS Private Marketplace, cÃ¡c cÆ¡ quan cÃ³ thá»ƒ táº¡o danh má»¥c Ä‘Æ°á»£c tuyá»ƒn chá»n cá»§a cÃ¡c sáº£n pháº©m Ä‘Æ°á»£c phÃª duyá»‡t trÆ°á»›c, giÃºp há» tuÃ¢n thá»§ cÃ¡c chÃ­nh sÃ¡ch ná»™i bá»™ vÃ  há»£p lÃ½ hÃ³a quy trÃ¬nh phÃª duyá»‡t.\n4. TÃ­ch há»£p vá»›i há»£p Ä‘á»“ng hiá»‡n cÃ³ AWS Marketplace cÃ³ thá»ƒ Ä‘Æ°á»£c truy cáº­p thÃ´ng qua cÃ¡c phÆ°Æ¡ng tiá»‡n há»£p Ä‘á»“ng chÃ­nh phá»§ khÃ¡c nhau, cho phÃ©p cÃ¡c cÆ¡ quan sá»­ dá»¥ng cÃ¡c thá»a thuáº­n hiá»‡n cÃ³ vÃ  Ä‘Æ¡n giáº£n hÃ³a quy trÃ¬nh mua sáº¯m.\n5. Triá»ƒn khai nhanh chÃ³ng Sau khi mua sáº¯m, cÃ¡c giáº£i phÃ¡p tá»« AWS Marketplace cÃ³ thá»ƒ Ä‘Æ°á»£c triá»ƒn khai vÃ  tÃ­ch há»£p nhanh chÃ³ng vá»›i AWS Services, cho phÃ©p thá»i gian táº¡o ra giÃ¡ trá»‹ nhanh hÆ¡n cho cÃ¡c dá»± Ã¡n chÃ­nh phá»§.\nKhi cÃ¡c cÆ¡ quan liÃªn bang thÃ­ch á»©ng vá»›i RFO, AWS Marketplace Ä‘Ã£ sáºµn sÃ ng há»— trá»£ nhu cáº§u mua sáº¯m cá»§a há». Báº±ng cÃ¡ch sá»­ dá»¥ng AWS Marketplace, khÃ¡ch hÃ ng chÃ­nh phá»§ cÃ³ thá»ƒ há»£p lÃ½ hÃ³a quy trÃ¬nh mua sáº¯m, tiáº¿p cáº­n má»™t loáº¡t cÃ¡c giáº£i phÃ¡p thÆ°Æ¡ng máº¡i vÃ  thÃºc Ä‘áº©y Ä‘á»•i má»›i trong CNTT khu vá»±c cÃ´ng.\nLink bÃ i viáº¿t gá»‘c : https://aws.amazon.com/blogs/publicsector/aws-marketplace-a-catalyst-for-federal-procurement-modernization/\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/",
	"title": "Translated blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Announcing the Regional 2025 AWS Partners of the Year for Europe, Middle East and Africa This blog announces the 2025 AWS Partners of the Year for the Europe, Middle East and Africa (EMEA) region. These annual awards recognize AWS Partner Network (APN) members who consistently deliver outstanding results and drive innovation for our customers.\nBlog 2 - How to customize responses to layer 7 DDoS attacks using AWS WAF Anti-DDoS AMR This blog explains how AWS WAF Anti-DDoS AWS Managed Rules (Anti-DDoS AMR) works and how you can customize its behavior using labels and additional AWS WAF rules. You\u0026rsquo;ll be guided through three real-world scenarios, each illustrating a different customization technique.\nBlog 3 - Build AI agents for Amazon Bedrock using the AWS CDK and deploy with GitHub Actions This blog shows how to build AI agents for Amazon Bedrock using the AWS Cloud Development Kit (CDK) and deploy them using GitHub Actions. It provides a complete end-to-end solution for creating, testing, and deploying AI agents in a production environment.\nBlog 4 - S\u0026amp;P Global data integration expands Amazon QuickSight Research capabilities This blog announces the new integration between Amazon QuickSight Research and S\u0026amp;P Global, combining global energy news, research, and insights with comprehensive market intelligence to provide QuickSight Research customers with a deep research agent.\nBlog 5 - How Contra Costa County District Attorney\u0026rsquo;s Office modernized subpoena processing with AWS and CC Tech Digital This blog describes how the Contra Costa County District Attorney\u0026rsquo;s Office partnered with AWS and CC Tech Digital to modernize their subpoena processing workflow using a serverless, cloud-native solution, managing over 17,000 subpoenas annually with improved speed, accuracy, and compliance.\nBlog 6 - Streamlining collaboration across Departments of Motor Vehicles with Private Blockchain This blog discusses how blockchain can streamline driver\u0026rsquo;s license issuance and foster deeper collaboration among DMVs across all 50 states, explaining why blockchain is an attractive technology choice for improving current reciprocity models.\nBlog 7 - Introducing the AWS Infrastructure as Code MCP Server: AI-Powered CDK and CloudFormation Assistance This blog introduces the AWS IaC MCP Server (MCP standard), which enables AI assistants to help you search CDK/CloudFormation documentation, validate templates, run compliance checks, and troubleshoot deployment failures (with CloudTrail), with an emphasis on local execution for security.\nBlog 8 - AWS Clean Rooms launches privacy-enhancing synthetic dataset generation for ML model training This blog announces the ability to generate synthetic datasets in AWS Clean Rooms for training ML models (regression/classification) while protecting privacy: preserving statistical patterns, reducing re-identification risk, providing privacy parameters, and reporting quality metrics (fidelity/privacy).\nBlog 9 - AWS Partner Central now available in AWS Management Console This blog presents the Partner Central experience integrated directly into the AWS Console, creating a unified journey from AWS customer â†’ Partner â†’ Marketplace Seller, including key areas (Solutions, Profile, Opportunities, Partner connections), an identity foundation based on IAM/IAM Identity Center, and the role of APIs.\nBlog 10 - Announcing AWS Partner Central in the AWS Management Console This blog announces Partner Central in the AWS Console, highlighting integration and automation via APIs (Account/Connections, Solution, Benefits, Selling/Leads), guidance using Amazon Q chat, access control through IAM/SSO, and expanded solution discovery in AWS Marketplace; it also includes steps for getting started and migration.\nBlog 11 - Announcing Amazon EKS Capabilities for workload orchestration and cloud resource management This blog introduces Amazon EKS Capabilities (fully managed) to simplify workload orchestration and AWS/Kubernetes resource management, with launch capabilities such as Argo CD, ACK, and KRO, plus how to enable/configure them and key considerations around permissions, upgrades, and adoption.\nBlog 12 - Simplify IAM policy creation with IAM Policy Autopilot, a new open source MCP server for builders This blog introduces IAM Policy Autopilot (an open source MCP server) that analyzes code to generate valid IAM identity-based policies, supports integration with AI coding assistants, updates permissions as code changes, and emphasizes reviewing/refining for least privilege before deployment.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/3-s3-data-storage/",
	"title": "Data Pipeline Optimization",
	"tags": [],
	"description": "",
	"content": "ğŸ¯ Task 3 Objectives Create S3 bucket and organize data for MLOps pipeline following the standard raw â†’ silver â†’ gold â†’ artifacts, convert CSV â†’ Parquet using AWS Glue Studio (Visual ETL) and measure read/write performance benchmarks:\nMeasure on AWS CloudShell. Measure on local machine (Windows, 16GB RAM). Focus on:\nRead/write performance: CSV vs Parquet. Storage size: before/after compression. How-to: step-by-step specifics, reproducible. ğŸ’¡ Task 3 â€“ S3 Storage Optimization\nâœ… Optimize format: Parquet + Snappy instead of plain CSV. âœ… Optimize read/write performance for ETL \u0026amp; training. âœ… Optimize storage size (significantly reduce GB). âœ… Add real benchmarks: CloudShell + local. ğŸ“¥ Input from Task 2: IAM Roles \u0026amp; Audit â€” account ID, IAM roles/policies and CloudTrail/audit setup required to create buckets, Glue roles and permissions.\nğŸ”§ Real lab environment Account ID: 842676018087 Lab region: us-east-1 Lab region: us-east-1 Bucket: mlops-retail-prediction-dev-842676018087 Main dataset:\nraw/transactions.csv\nâ‰ˆ 4,593.65 MB, 33,850,823 rows Example 1 Parquet file after ETL:\nâ‰ˆ 4,593.65 MB, 33,850,823 rows Example 1 Parquet file after ETL:\nsilver/shop_week=200607/run-1761638745394-part-block-0-0-r-00000-snappy.parquet\nâ‰ˆ 458.45 MB, 33,850,823 rows â‰ˆ 458.45 MB, 33,850,823 rows 1. S3 bucket structure \u0026amp; organization 1.1. General storage structure Apply to all accounts, using {account-id} as placeholder:\ns3://mlops-retail-prediction-dev-{account-id}/ â”œâ”€â”€ raw/ # original CSV data, immutable â”œâ”€â”€ silver/ # cleaned/standardized Parquet data â”œâ”€â”€ gold/ # features, aggregated datasets for training/serving â””â”€â”€ artifacts/ # model, metadata, logs, reports Meaning:\nraw/: append only, no edit/delete â†’ serve audit \u0026amp; reprocessing. silver/: optimized Parquet storage (standard schema, clean). gold/: final dataset for training/inference. artifacts/: model.tar.gz, notebook export, log, benchmark CSV, etc. Tip: Use clear and consistent prefixes (e.g. raw/, silver/, gold/) to easily set up lifecycle rules, IAM policies and S3 analytics. Add ingest-date=YYYY-MM-DD/ if need to track by upload date.\n1.2. Actual structure in lab With your account ID:\nS3 Bucket: mlops-retail-prediction-dev-842676018087 â”œâ”€â”€ raw/ â”‚ â””â”€â”€ transactions.csv # original file ~4.59GB â”œâ”€â”€ silver/ â”‚ â”œâ”€â”€ transactions/ # output from Glue ETL (if not partitioned by week) â”‚ â””â”€â”€ shop_week=200607/ â”‚ â””â”€â”€ run-1761638745394-part-block-0-0-r-00000-snappy.parquet # ~458MB â”œâ”€â”€ gold/ â”‚ â””â”€â”€ (reserved for feature store / aggregated tables) â”‚ â””â”€â”€ (reserved for feature store / aggregated tables) â””â”€â”€ artifacts/ â””â”€â”€ (store benchmark results, model, logs, etc.) You can open S3 Console to confirm correct paths, especially:\nraw/transactions.csv A typical Parquet file in silver/shop_week=.../. 2. Create bucket \u0026amp; folders on AWS Console 2.1. Create S3 Bucket Go to AWS Console â†’ S3 â†’ Create bucket. Configuration: Bucket name: mlops-retail-prediction-dev-842676018087 Region: us-east-1 Block all public access: âœ… Enabled Versioning: (recommended) Enabled Versioning: (recommended) Enabled Default encryption: âœ… SSE-S3 2.2. Create 4 main folders In S3 Console:\nOpen bucket mlops-retail-prediction-dev-842676018087. Create folder sequentially: raw/ silver/ gold/ artifacts/ Warning: Avoid renaming bucket after deployment â€” bucket name is global and changes will affect all pipelines, IAM policies, and configs. Always enable Block all public access unless there\u0026rsquo;s a specific reason and it\u0026rsquo;s reviewed.\n3. Enable Intelligent-Tiering (cost optimization) Purpose: infrequently accessed data (e.g. old raw/, old artifacts/ logs) automatically moved to cheaper storage tiers, URL unchanged.\nSteps:\nGo to bucket â†’ Properties tab. Find Intelligent-Tiering archive configurations section â†’ Edit. Add configuration: Configuration name: storage-optimization Status: Enabled Scope: Entire bucket (or specific prefix: raw/, silver/, gold/, artifacts/) 4. Convert CSV â†’ Parquet using AWS Glue Studio (Visual ETL) 4.1. Upload transactions.csv to raw/ On S3 Console:\nOpen bucket â†’ folder raw/. Upload â†’ Add files â†’ select file transactions.csv from your machine. Upload. 4.2. Create Glue Job (Visual ETL) Go to AWS Glue Studio â†’ Jobs â†’ Create job â†’ Visual with a blank canvas. Set name: Job name: csv-to-parquet-converter Select/create IAM Role with permissions: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::mlops-retail-prediction-dev-842676018087/raw/*\u0026#34;, \u0026#34;arn:aws:s3:::mlops-retail-prediction-dev-842676018087/silver/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:*\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 4.3. Source node â€“ read CSV from S3 In Glue Studio canvas:\nAdd S3 Source. Configuration: Data source: S3 Format: CSV S3 URL: s3://mlops-retail-prediction-dev-842676018087/raw/transactions.csv First row as header: Enabled Delimiter: , Summary:\nField Value S3 bucket mlops-retail-prediction-dev-842676018087 Path raw/transactions.csv Format CSV Header Yes Delimiter , 4.4. Transform â€“ ApplyMapping (schema optimization) Add ApplyMapping node. Connect Source â†’ ApplyMapping. Data type mapping (example): Column Source type Target type Notes SHOP_WEEK long int int32 is enough SHOP_HOUR long tinyint 0â€“23 QUANTITY long smallint Quantity STORE_CODE string string Keep as is SPEND decimal decimal(10,2) Currency, 2 decimals BASKET_TYPE string string Categorical Benefits:\nReduce Parquet file size. Optimize scan \u0026amp; aggregation. Reduce RAM when reading data. 4.5. Target â€“ write Parquet (Snappy) to silver/ 4.5. Target â€“ write Parquet (Snappy) to silver/ Add S3 Target node. Connect ApplyMapping â†’ Target. Configuration: Data target: S3 Format: Parquet Compression: Snappy S3 path: s3://mlops-retail-prediction-dev-842676018087/silver/transactions/ Partition keys: SHOP_WEEK (recommended) Partition keys: SHOP_WEEK (recommended) Illustration:\n\u0026ndash; Target config: /imagess3-data-storage/target-config.png \u0026ndash; Full pipeline: /imagess3-data-storage/04-glue-etl.png\nSave \u0026amp; Run job â†’ monitor Job run details â†’ check output in silver/. Info: When partitioning, balance between number of partitions and file size â€” too many small files will slow down queries; consider running compaction step (Glue/Athena/EMR) to merge into larger files (e.g., 128â€“512 MB each) if needed.\nTip: Choose Snappy for balance between compression ratio and CPU usage. If need to save more I/O, try ZSTD (if your stack supports) for better ratio but check CPU cost.\n5. Real benchmark on AWS CloudShell (read directly from S3) 5.1. Dataset info \u0026amp; how to run Run on AWS CloudShell.\nRead directly:\nRun on AWS CloudShell.\nRead directly:\nraw/transactions.csv (~4,593.65 MB, 33,850,823 rows). 1 Parquet file (~458.45 MB, 33,850,823 rows). 1 Parquet file (~458.45 MB, 33,850,823 rows). You used scripts like:\nread_csv_s3(...) to measure CSV reading. read_parquet_s3(...) to measure Parquet reading. Detailed logs appeared in CloudShell.\nInfo: CloudShell has resource limits (vCPU/RAM/IO) â€” measured timings may differ from EC2/Glue worker sizes. When comparing, specify instance/worker size to reproduce accurate results.\nCSV read results: Parquet read results: 5.2. Measurement results (CloudShell) CSV â€“ read entire raw/transactions.csv from S3\n5 measurements:\n151.91s, 146.34s, 141.52s, 126.03s, 115.95s Average calculation (approximate):\nAvg time â‰ˆ 136.35 s Size = 4,593.65 MB Avg throughput â‰ˆ 33.7 MB/s Rows/s â‰ˆ ~248k rows/s Parquet â€“ read 1 file ~458.45 MB from S3\n5 measurements:\n61.37s, 53.65s, 52.51s, 49.66s, 49.55s Average calculation (approximate):\nAvg time â‰ˆ 53.35 s Size = 458.45 MB Avg throughput â‰ˆ 8.6 MB/s Rows/s â‰ˆ ~635k rows/s 5.3. Comparison table (CloudShell) Type Size on S3 Avg time (s) Avg throughput (MB/s) Rows Rows/s (approx) Relative rows/s CSV 4,593.65 MB 136.35 33.7 33,850,823 ~248k 1Ã— Parquet 458.45 MB 53.35 8.6 33,850,823 ~635k ~2.6Ã— Explanation: Explanation:\nBy MB/s, CSV seems \u0026ldquo;faster\u0026rdquo; because each run processes more MB (4.59 GB). But in terms of rows per second (rows/s), Parquet is ~2.6Ã— faster, suitable for ETL / training. CloudShell Conclusion\nParquet (Snappy) dramatically reduces storage size: 4.59 GB â†’ ~0.46 GB. With same 33.85M rows, Parquet processes ~2.6Ã— faster in rows/s. 6. Benchmark on local machine 6.1. Prepare directory \u0026amp; download data On Windows:\nmkdir s3-local-benchmark cd s3-local-benchmark Download 2 files:\naws s3 cp s3://mlops-retail-prediction-dev-842676018087/raw/transactions.csv ./transactions.csv aws s3 cp s3://mlops-retail-prediction-dev-842676018087/silver/shop_week=200607/run-1761638745394-part-block-0-0-r-00000-snappy.parquet ./transactions_200607.parquet 6.2. Benchmark script Create file local_benchmark.py:\nimport time import os import pandas as pd def bench_csv_stream(path: str, runs: int = 3): print(f\u0026#34;=== Benchmark CSV (streaming): {path} ===\u0026#34;) size_mb = os.path.getsize(path) / (1024 * 1024) for i in range(1, runs + 1): t0 = time.time() rows = 0 # Read in chunks to avoid RAM overflow for chunk in pd.read_csv(path, chunksize=500_000): rows += len(chunk) t1 = time.time() elapsed = t1 - t0 throughput = size_mb / elapsed print(f\u0026#34;[local_csv_stream] run={i} time={elapsed:.2f}s, \u0026#34; f\u0026#34;size={size_mb:.2f} MB, throughput={throughput:.2f} MB/s, rows={rows}\u0026#34;) def bench_parquet_stream(path: str, runs: int = 3): print(f\u0026#34;=== Benchmark Parquet (streaming): {path} ===\u0026#34;) size_mb = os.path.getsize(path) / (1024 * 1024) for i in range(1, runs + 1): t0 = time.time() df = pd.read_parquet(path) rows = len(df) t1 = time.time() elapsed = t1 - t0 throughput = size_mb / elapsed print(f\u0026#34;[local_parquet_stream] run={i} time={elapsed:.2f}s, \u0026#34; f\u0026#34;size={size_mb:.2f} MB, throughput={throughput:.2f} MB/s, rows={rows}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: bench_csv_stream(\u0026#34;transactions.csv\u0026#34;, runs=3) bench_parquet_stream(\u0026#34;transactions_200607.parquet\u0026#34;, runs=3) Run:\npython local_benchmark.py 6.3. Actual logs Observations:\nCSV full 4.59 GB: still processable thanks to chunk reading, throughput ~90â€“95 MB/s. Parquet (sample 1 week, 6.48 MB): read time ~0.05â€“0.09s â†’ extremely low latency. With many small Parquet files (partitioned by shop_week), querying by week/month will be very fast. Warning (Local): On local machine, reading entire Parquet dataset into memory (pd.read_parquet) can cause RAM shortage. Use reading by pieces (pyarrow dataset, iter_batches) or increase chunksize when using CSV to avoid OOM.\nTip (Local): When benchmarking on Windows, close other heavy programs and IO results may differ between HDD and SSD â€” note drive type (SSD/HDD) in benchmark report.\n7. IAM â€“ Minimum permissions for Glue Job (summary) Minimum required:\nS3: s3:GetObject for raw/* s3:PutObject for silver/* s3:GetObject for raw/* s3:PutObject for silver/* Glue: Permission to create/run job, read metadata (depends on environment). CloudWatch Logs: write job logs. Example policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::mlops-retail-prediction-dev-842676018087/raw/*\u0026#34;, \u0026#34;arn:aws:s3:::mlops-retail-prediction-dev-842676018087/silver/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:*\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 8. Task 3 Summary â€“ S3 Data Storage About architecture:\nDesign bucket following MLOps standard:\nraw/ â†’ silver/ â†’ gold/ â†’ artifacts/ Maintain raw/ immutable.\nStandardize data to Parquet (Snappy) in silver/.\nAbout performance (from your actual measurements):\n4.59 GB CSV â†’ ~0.46 GB Parquet for same 33.85M rows.\nOn CloudShell:\nCSV: ~136s, ~248k rows/s. Parquet: ~53s, ~635k rows/s â†’ ~2.6Ã— rows/s. On local machine (16GB RAM):\nOn local machine (16GB RAM):\nCSV 4.59 GB still processable with 500k rows chunk. Parquet sample 1 week (~6.48 MB) read in ~0.05â€“0.09s. About cost \u0026amp; operations:\nParquet + Snappy significantly reduces size â†’ reduce S3 costs. Intelligent-Tiering helps automatically downgrade storage tier for old data. Glue Visual ETL helps minimize coding, easy to show in reports. 9. Clean Up Resources (AWS CLI) 9.1. Delete all objects in S3 bucket # Delete all files in bucket aws s3 rm s3://mlops-retail-prediction-dev-842676018087 --recursive # Check bucket is empty aws s3 ls s3://mlops-retail-prediction-dev-842676018087 --recursive 9.2. Delete S3 bucket # Delete bucket (only when empty) aws s3 rb s3://mlops-retail-prediction-dev-842676018087 # Check bucket is deleted aws s3 ls | grep mlops-retail-prediction-dev 9.3. Delete Glue Job # List Glue jobs # List Glue jobs aws glue get-jobs --query \u0026#39;Jobs[?contains(Name, `csv-to-parquet`)].Name\u0026#39; # Delete Glue job aws glue delete-job --job-name csv-to-parquet-converter # Check job is deleted aws glue get-job --job-name csv-to-parquet-converter 9.4. Delete IAM Role (if created specifically for Glue) # Detach policies from role aws iam detach-role-policy --role-name GlueETLRole --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole # Delete inline policies (if any) # Delete inline policies (if any) aws iam delete-role-policy --role-name GlueETLRole --policy-name S3AccessPolicy # Delete role aws iam delete-role --role-name GlueETLRole Success tip: If you delete resources to avoid costs, check CloudWatch log groups and Athena query history â€” some logs or query history may still store metadata and cause small costs if not cleaned up.\n10. S3 Storage Pricing Table (ap-southeast-1) 10.1. Storage cost by class Storage Class Price (USD/GB/month) Minimum Duration Notes S3 Standard $0.025 None Frequent access S3 Standard-IA $0.0138 30 days Infrequent access S3 One Zone-IA $0.011 30 days Single AZ S3 Glacier Instant $0.005 90 days Archive, instant retrieval S3 Glacier Flexible $0.0045 90 days Archive, 1-12 hours retrieval S3 Deep Archive $0.002 180 days Long-term archive, 12+ hours 10.2. Request costs Request Type Price (USD/1000 requests) Notes PUT/POST/LIST $0.0055 Write operations GET/SELECT $0.00044 Read operations Data Transfer OUT $0.12/GB First 1GB free/month 10.3. Project cost estimate Current data:\nRaw CSV: 4.59 GB Silver Parquet: 0.46 GB Total: ~5 GB Monthly cost (S3 Standard): Monthly cost (S3 Standard):\nComponent Size Price/GB Monthly Cost Raw data (CSV) 4.59 GB $0.025 $0.11 Silver data (Parquet) 0.46 GB $0.025 $0.01 Gold + artifacts ~0.5 GB $0.025 $0.01 Total Storage ~5.5 GB $0.14 Requests (estimated) ~1000 req $0.0055 $0.006 Grand Total â‰ˆ $0.15/month With Intelligent Tiering:\nAfter 30 days: Raw data moves to Standard-IA â†’ save ~45% After 90 days: Old artifacts move to Glacier â†’ save ~80% Estimated savings: ~$0.05-0.08/month ğŸ’° Optimized Storage Cost\nCurrent: ~$0.15/month for 5.5GB With Intelligent Tiering: ~$0.07-0.10/month Parquet format: Reduces 90% storage size vs CSV ğŸ¯ Task 3 Completed\nClear S3 architecture, MLOps standard. CSV â†’ Parquet using Glue Studio (Visual, with illustrations). Real benchmarks on CloudShell and local, with specific numbers. Detailed clean up commands and pricing breakdown. Easy to present in reports \u0026amp; demo for professors. ğŸ“¹ Task 3 Implementation Video Next Step: Task 4: SageMaker Training\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.2-week2/",
	"title": "Week 2 - Networking on AWS",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Design and implement a Virtual Private Cloud (VPC). Configure routing, gateways (IGW, NAT), and security layers (SG, NACL). Tasks to be carried out this week: Day Task Reference 2 - VPC Design: Plan CIDR blocks for 2 AZs (Public/Private Subnets).\n- Create VPC, Subnets, and Route Tables. AWS VPC Docs 3 - Connectivity: Attach Internet Gateway (IGW) for public subnets.\n- Deploy NAT Gateway for private subnet egress. AWS Workshop 4 - Security: Configure Security Groups (Stateful) vs NACLs (Stateless).\n- Implement \u0026ldquo;Bastion Host\u0026rdquo; concept. FCJ Materials 5 - Access: SSH into private EC2 via Bastion or Session Manager.\n- Optimization: Setup VPC Endpoints for S3/DynamoDB. AWS Systems Manager 6 - Verification: Test connectivity (Public -\u0026gt; Internet, Private -\u0026gt; NAT).\n- Weekly Review. - Week 2 Achievements: Deployed a custom VPC with isolated Public/Private subnets across 2 AZs. Configured VPC Endpoints to secure internal AWS service traffic. Replaced SSH access with secure Session Manager. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.4-blog4/",
	"title": "TÃ­ch há»£p dá»¯ liá»‡u S&amp;P Global má»Ÿ rá»™ng kháº£ nÄƒng Amazon QuickSight Research",
	"tags": [],
	"description": "",
	"content": "BÃ i viáº¿t gá»‘c Ä‘Æ°á»£c Ä‘Äƒng vÃ o ngÃ y 08 thÃ¡ng 12 nÄƒm 2025\nHÃ´m nay, chÃºng tÃ´i hÃ¢n háº¡nh thÃ´ng bÃ¡o vá» tÃ­ch há»£p má»›i giá»¯a Amazon QuickSight Research vÃ  S\u0026amp;P Global. TÃ­ch há»£p nÃ y káº¿t há»£p tin tá»©c nÄƒng lÆ°á»£ng toÃ n cáº§u, nghiÃªn cá»©u vÃ  thÃ´ng tin chi tiáº¿t cá»§a S\u0026amp;P Global vá»›i kháº£ nÄƒng thá»‹ trÆ°á»ng toÃ n cáº§u S\u0026amp;P Global Ä‘á»ƒ cung cáº¥p cho khÃ¡ch hÃ ng QuickSight Research má»™t Ä‘áº¡i lÃ½ nghiÃªn cá»©u sÃ¢u.\nTÃ­ch há»£p S\u0026amp;P Global má»Ÿ rá»™ng kháº£ nÄƒng cá»§a QuickSight Research Ä‘á»ƒ cÃ¡c chuyÃªn gia kinh doanh cÃ³ thá»ƒ phÃ¢n tÃ­ch nhiá»u nguá»“n dá»¯ liá»‡u - bao gá»“m tin tá»©c nÄƒng lÆ°á»£ng toÃ n cáº§u vÃ  thÃ´ng tin tÃ i chÃ­nh cao cáº¥p - trong má»™t khÃ´ng gian lÃ m viá»‡c, loáº¡i bá» nhu cáº§u chuyá»ƒn Ä‘á»•i giá»¯a cÃ¡c ná»n táº£ng vÃ  chuyá»ƒn Ä‘á»•i hÃ ng tuáº§n nghiÃªn cá»©u thÃ nh phÃºt thÃ´ng tin táº­p trung. QuickSight Suite káº¿t ná»‘i thÃ´ng tin trÃªn cÃ¡c kho lÆ°u trá»¯ ná»™i bá»™, á»©ng dá»¥ng phá»• biáº¿n, dá»‹ch vá»¥ AWS vÃ  thÃ´ng qua Giao thá»©c Ngá»¯ cáº£nh MÃ´ hÃ¬nh (MCP) tÃ­ch há»£p vá»›i hÆ¡n 1.000 á»©ng dá»¥ng. á»¨ng dá»¥ng AI agentic nÃ y Ä‘ang Ä‘á»‹nh hÃ¬nh láº¡i cÃ¡ch thá»©c cÃ´ng viá»‡c Ä‘Æ°á»£c thá»±c hiá»‡n báº±ng cÃ¡ch chuyá»ƒn Ä‘á»•i cÃ¡ch cÃ¡c nhÃ³m tÃ¬m thÃ´ng tin chi tiáº¿t, tiáº¿n hÃ nh nghiÃªn cá»©u, tá»± Ä‘á»™ng hÃ³a nhiá»‡m vá»¥, trá»±c quan hÃ³a dá»¯ liá»‡u vÃ  tÃ¬m thÃ´ng tin chi tiáº¿t tá»« hÃ ng terabyte á»©ng dá»¥ng vÃ  dá»¯ liá»‡u.\nTrong bÃ i viáº¿t nÃ y, chÃºng ta khÃ¡m phÃ¡ cÃ¡c bá»™ dá»¯ liá»‡u cá»§a S\u0026amp;P Global vÃ  kiáº¿n trÃºc giáº£i phÃ¡p cá»§a tÃ­ch há»£p vá»›i QuickSight Research.\nTá»•ng quan giáº£i phÃ¡p S\u0026amp;P Global Ä‘Ã£ tiÃªn phong hai triá»ƒn khai mÃ¡y chá»§ MCP trÃªn AWS Ä‘á»ƒ cÃ¡c tá»• chá»©c cÃ³ thá»ƒ dá»… dÃ ng tÃ­ch há»£p cÃ¡c dá»‹ch vá»¥ tÃ i chÃ­nh Ä‘Ã¡ng tin cáº­y vÃ  ná»™i dung nÄƒng lÆ°á»£ng vÃ o quy trÃ¬nh lÃ m viá»‡c Ä‘Æ°á»£c há»— trá»£ bá»Ÿi AI trong khi duy trÃ¬ cháº¥t lÆ°á»£ng, báº£o máº­t vÃ  Ä‘á»™ tin cáº­y mÃ  cÃ¡c nhÃ  lÃ£nh Ä‘áº¡o kinh doanh yÃªu cáº§u.\n\u0026ldquo;Sá»± há»£p tÃ¡c cá»§a chÃºng tÃ´i vá»›i AWS má»Ÿ rá»™ng cÃ¡ch S\u0026amp;P Global cung cáº¥p thÃ´ng tin Ä‘Ã¡ng tin cáº­y thÃ´ng qua tháº¿ há»‡ tiáº¿p theo cá»§a tráº£i nghiá»‡m AI agentic. Báº±ng cÃ¡ch lÃ m viá»‡c cÃ¹ng vá»›i cÃ¡c cÃ´ng ty AI hÃ ng Ä‘áº§u, má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  Ä‘áº£m báº£o khÃ¡ch hÃ ng cÃ³ thá»ƒ truy cáº­p dá»¯ liá»‡u Ä‘Ã¡ng tin cáº­y vÃ  thÃ´ng tin chi tiáº¿t báº¥t cá»© nÆ¡i nÃ o quy trÃ¬nh lÃ m viá»‡c cá»§a há» diá»…n ra.\u0026rdquo;\nâ€” Bhavesh Dayalji, GiÃ¡m Ä‘á»‘c CÃ´ng nghá»‡ cá»§a S\u0026amp;P Global vÃ  CEO cá»§a Kensho.\nS\u0026amp;P Global Energy: ThÃ´ng tin toÃ n diá»‡n vá» hÃ ng hÃ³a vÃ  nÄƒng lÆ°á»£ng TÃ­ch há»£p S\u0026amp;P Global Energy, hiá»‡n cÃ³ sáºµn trong Amazon QuickSight Research, sá»­ dá»¥ng mÃ¡y chá»§ MCP AI Ready Data Ä‘á»ƒ cung cáº¥p quyá»n truy cáº­p toÃ n diá»‡n vÃ o thÃ´ng tin thá»‹ trÆ°á»ng nÄƒng lÆ°á»£ng bao gá»“m Crude, Clean Fuels, Natural Gas, Power, Coal, Metals, Chemicals, LNG, Petrochemicals, Clean Energy, NÃ´ng nghiá»‡p vÃ  Váº­n chuyá»ƒn trÃªn cÃ¡c thá»‹ trÆ°á»ng toÃ n cáº§u. ÄÆ°á»£c xÃ¢y dá»±ng dá»±a trÃªn danh tiáº¿ng cá»§a S\u0026amp;P Global nhÆ° má»™t cÆ¡ quan thá»‹ trÆ°á»ng Ä‘Ã¡ng tin cáº­y, mÃ¡y chá»§ MCP sá»­ dá»¥ng hÃ ng trÄƒm nghÃ¬n tÃ i liá»‡u do chuyÃªn gia táº¡o bao gá»“m phÃ¢n tÃ­ch, bÃ¬nh luáº­n, tin tá»©c vÃ  nghiÃªn cá»©u.\nGiáº£i phÃ¡p cung cáº¥p gÃ³c nhÃ¬n Ä‘a chiá»u Ä‘á»™c Ä‘Ã¡o, cung cáº¥p thÃ´ng tin tá»« cáº­p nháº­t thá»‹ trÆ°á»ng hÃ ng ngÃ y Ä‘áº¿n triá»ƒn vá»ng má»™t nÄƒm vÃ  má»Ÿ rá»™ng Ä‘áº¿n phÃ¢n tÃ­ch ká»‹ch báº£n 20+ nÄƒm. Vá»›i dá»¯ liá»‡u Ä‘Æ°á»£c lÃ m má»›i má»—i 30 phÃºt, cÃ¡c nhÃ  lÃ£nh Ä‘áº¡o kinh doanh cÃ³ quyá»n truy cáº­p gáº§n thá»i gian thá»±c vÃ o thÃ´ng tin hÃ ng hÃ³a vÃ  nÄƒng lÆ°á»£ng, tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ tá»‘c Ä‘á»™ quyáº¿t Ä‘á»‹nh khi khÃ¡m phÃ¡ cÃ¡c thÃ¡ch thá»©c quy Ä‘á»‹nh, cÆ¡ há»™i Ä‘áº§u tÆ° hoáº·c tÃ¡c Ä‘á»™ng mÃ´i trÆ°á»ng.\nS\u0026amp;P Global Market Intelligence: ThÃ´ng tin tÃ i chÃ­nh Ä‘Ã¡ng tin cáº­y TÃ­ch há»£p S\u0026amp;P Global Market Intelligence, hiá»‡n cÃ³ sáºµn trong Amazon QuickSight Research, sá»­ dá»¥ng mÃ¡y chá»§ MCP sáºµn sÃ ng API Kensho LLM Ä‘á»ƒ cung cáº¥p quyá»n truy cáº­p vÃ o trung tÃ¢m tÃ i chÃ­nh Ä‘á»•i má»›i cá»§a S\u0026amp;P Global. MÃ¡y chá»§ MCP lÃ m cho dá»¯ liá»‡u tÃ i chÃ­nh Ä‘Ã¡ng tin cáº­y cÃ³ thá»ƒ truy cáº­p thÃ´ng qua cÃ¡c truy váº¥n ngÃ´n ngá»¯ tá»± nhiÃªn, tÃ­ch há»£p liá»n máº¡ch vá»›i Amazon QuickSight Research. CÃ¡c chuyÃªn gia tÃ i chÃ­nh cÃ³ thá»ƒ truy cáº­p S\u0026amp;P Capital IQ Financials, báº£n ghi cuá»™c gá»i thu nháº­p, thÃ´ng tin cÃ´ng ty, giao dá»‹ch vÃ  nhiá»u hÆ¡n ná»¯a, chá»‰ báº±ng cÃ¡ch Ä‘áº·t cÃ¢u há»i.\nGiáº£i phÃ¡p Kensho giáº£i quyáº¿t má»™t thÃ¡ch thá»©c quan trá»ng trong dá»‹ch vá»¥ tÃ i chÃ­nh: lÃ m cho cÃ¡c kho lÆ°u trá»¯ rá»™ng lá»›n cá»§a dá»¯ liá»‡u tÃ i chÃ­nh cÃ³ thá»ƒ truy cáº­p ngay láº­p tá»©c mÃ  khÃ´ng yÃªu cáº§u hÃ ng giá» truy váº¥n hoáº·c chuyÃªn mÃ´n ká»¹ thuáº­t. CÃ¡c nhÃ³m ká»¹ thuáº­t, sáº£n pháº©m vÃ  kinh doanh cÃ³ thá»ƒ tiáº¿t kiá»‡m thá»i gian vÃ  tÃ i nguyÃªn Ä‘Ã¡ng ká»ƒ báº±ng cÃ¡ch chuyá»ƒn Ä‘á»•i nhá»¯ng gÃ¬ tá»«ng yÃªu cáº§u hÃ ng giá» trÃ­ch xuáº¥t dá»¯ liá»‡u thÃ nh cÃ¡c cuá»™c trÃ² chuyá»‡n tráº£ vá» thÃ´ng tin chÃ­nh xÃ¡c, Ä‘Ã¡ng tin cáº­y trong vÃ i giÃ¢y.\nKiáº¿n trÃºc giáº£i phÃ¡p Kiáº¿n trÃºc mÃ¡y chá»§ MCP cá»§a S\u0026amp;P Global Ä‘Æ°á»£c hiá»ƒn thá»‹ trong sÆ¡ Ä‘á»“ sau. Khi sá»­ dá»¥ng má»™t trong cÃ¡c tÃ­ch há»£p S\u0026amp;P, lÆ°u lÆ°á»£ng cháº£y tá»« QuickSight Research thÃ´ng qua Amazon API Gateway Ä‘áº¿n AWS Application Load Balancer vá»›i cÃ¡c dá»‹ch vá»¥ MCP Ä‘Æ°á»£c lÆ°u trá»¯ trÃªn Amazon Elastic Kubernetes Service (Amazon EKS). MÃ¡y chá»§ MCP sá»­ dá»¥ng dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trá»¯ trong Amazon S3 vÃ  AWS Relational Database Service cho PostgreSQL cho dá»¯ liá»‡u cÃ³ cáº¥u trÃºc, vÃ  Amazon OpenSearch Service cho lÆ°u trá»¯ vector. Kiáº¿n trÃºc nÃ y cung cáº¥p cÃ¡c mÃ¡y chá»§ MCP sáºµn sÃ ng cho doanh nghiá»‡p vá»›i báº£o máº­t chuyÃªn sÃ¢u, tá»± Ä‘á»™ng má»Ÿ rá»™ng vÃ  kháº£ nÄƒng quan sÃ¡t toÃ n diá»‡n.\nMCP lÃ  má»™t tiÃªu chuáº©n má»Ÿ há»— trá»£ giao tiáº¿p liá»n máº¡ch giá»¯a cÃ¡c Ä‘áº¡i lÃ½ AI vÃ  nguá»“n dá»¯ liá»‡u bÃªn ngoÃ i, cÃ´ng cá»¥ vÃ  dá»‹ch vá»¥. MCP hoáº¡t Ä‘á»™ng trÃªn kiáº¿n trÃºc client-server nÆ¡i mÃ¡y chá»§ MCP xá»­ lÃ½ cÃ¡c cuá»™c gá»i cÃ´ng cá»¥, thÆ°á»ng bao gá»“m nhiá»u cuá»™c gá»i API vÃ  hiá»ƒn thá»‹ triá»ƒn khai logic kinh doanh nhÆ° cÃ¡c hÃ m cÃ³ thá»ƒ gá»i. Äiá»u nÃ y cho phÃ©p cÃ¡c Ä‘áº¡i lÃ½ AI khÃ¡m phÃ¡ kháº£ nÄƒng má»™t cÃ¡ch Ä‘á»™ng, Ä‘Ã m phÃ¡n cÃ¡c tÃ­nh nÄƒng vÃ  chia sáº» ngá»¯ cáº£nh má»™t cÃ¡ch an toÃ n vá»›i táº¥t cáº£ cÃ¡c yÃªu cáº§u quan trá»ng cho cÃ¡c á»©ng dá»¥ng cáº¥p doanh nghiá»‡p.\nGiáº£i phÃ¡p cá»§a S\u0026amp;P Global cÃ³ cÃ¡c khá»‘i xÃ¢y dá»±ng chÃ­nh sau:\nÄÆ°á»ng á»‘ng dá»¯ liá»‡u tá»± Ä‘á»™ng vá»›i Amazon Bedrock Trá»ng tÃ¢m cá»§a giáº£i phÃ¡p lÃ  má»™t Ä‘Æ°á»ng á»‘ng Retrieval Augmented Generation (RAG) tá»± Ä‘á»™ng hÃ³a viá»‡c nháº­p dá»¯ liá»‡u báº±ng Amazon Textract. Äiá»u nÃ y chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u thá»‹ trÆ°á»ng thÃ´ thÃ nh cÃ¡c nhÃºng vector AI Ready sá»­ dá»¥ng mÃ´ hÃ¬nh Cohere Embed Ä‘Æ°á»£c lÆ°u trá»¯ trÃªn Bedrock. CÃ¡c tÃ i liá»‡u tá»« kho lÆ°u trá»¯ Ä‘á»™c quyá»n cá»§a S\u0026amp;P Global tráº£i qua xá»­ lÃ½ trÆ°á»›c, phÃ¢n tÃ¡ch vÃ  lÃ m giÃ u trÆ°á»›c khi Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh cÃ¡c nhÃºng vector báº±ng cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh Cohere Embed Ä‘Æ°á»£c lÆ°u trá»¯ trÃªn Bedrock. ÄÆ°á»ng á»‘ng nháº­p cháº¡y theo lá»‹ch trÃ¬nh, lÃ m má»›i kho lÆ°u trá»¯ vector OpenSearch má»—i 30 phÃºt Ä‘á»ƒ truy cáº­p gáº§n thá»i gian thá»±c vÃ o dá»¯ liá»‡u nÄƒng lÆ°á»£ng.\nTÃ¬m kiáº¿m vector vÃ  ngá»¯ nghÄ©a Amazon OpenSearch phá»¥c vá»¥ nhÆ° cÆ¡ sá»Ÿ dá»¯ liá»‡u vector, lÆ°u trá»¯ cÃ¡c nhÃºng Ä‘Æ°á»£c táº¡o bá»Ÿi Bedrock vÃ  cho phÃ©p kháº£ nÄƒng tÃ¬m kiáº¿m ngá»¯ nghÄ©a trÃªn dá»¯ liá»‡u nÄƒng lÆ°á»£ng cá»§a S\u0026amp;P Global. Kho lÆ°u trá»¯ vector OpenSearch Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho cÃ¡c tÃ¬m kiáº¿m tÆ°Æ¡ng tá»± cao chiá»u cho phÃ©p tÃ¬m kiáº¿m tÆ°Æ¡ng tá»± nhanh chÃ³ng mang láº¡i kháº£ nÄƒng truy xuáº¥t thÃ´ng tin liÃªn quan theo ngá»¯ cáº£nh Ä‘á»ƒ pháº£n há»“i cÃ¡c truy váº¥n ngÃ´n ngá»¯ tá»± nhiÃªn.\nKháº£ nÄƒng phá»¥c há»“i vÃ  quy mÃ´ Giáº£i phÃ¡p nÃ y sá»­ dá»¥ng Amazon EKS Ä‘á»ƒ lÆ°u trá»¯ táº¥t cáº£ cÃ¡c giáº£i phÃ¡p mÃ¡y chá»§ MCP vá»›i hai cá»¥m sáº£n xuáº¥t cho phÃ©p phÃ¢n chia lÆ°u lÆ°á»£ng vÃ  kháº£ nÄƒng chuyá»ƒn Ä‘á»•i dá»± phÃ²ng. CÃ¡ch tiáº¿p cáº­n cá»¥m kÃ©p nÃ y cung cáº¥p tÃ­nh kháº£ dá»¥ng liÃªn tá»¥c ngay cáº£ trong cÃ¡c lá»—i khÃ´ng mong Ä‘á»£i. Cáº£ Cluster Autoscaler vÃ  Horizontal Pod Autoscaler Ä‘á»u cho phÃ©p má»Ÿ rá»™ng Ä‘á»™ng dá»±a trÃªn nhu cáº§u. CÃ¡c mÃ¡y chá»§ tá»± Ä‘á»™ng má»Ÿ rá»™ng vá»›i khung FastMCP, cung cáº¥p hiá»‡u suáº¥t cao vÃ  cÃ¡c endpoint cÃ³ kháº£ nÄƒng phá»¥c há»“i tuÃ¢n thá»§ vá»›i Ä‘áº·c táº£ Streamable HTTP Transport Ä‘Æ°á»£c yÃªu cáº§u bá»Ÿi giao thá»©c MCP.\nBáº£o máº­t Báº£o máº­t Ä‘Æ°á»£c tÃ­ch há»£p vÃ o má»i lá»›p cá»§a giáº£i phÃ¡p. API Gateway phá»¥c vá»¥ nhÆ° Ä‘iá»ƒm cuá»‘i cho truy cáº­p mÃ¡y chá»§ MCP. Danh tÃ­nh doanh nghiá»‡p cá»§a S\u0026amp;P Global Ä‘Æ°á»£c cung cáº¥p bá»Ÿi OAuth authentication. API Gateway Ä‘Æ°á»£c báº£o máº­t hÆ¡n ná»¯a vá»›i AWS Web Application Firewall (WAF) vá»›i phÃ¡t hiá»‡n má»‘i Ä‘e dá»a nÃ¢ng cao. CÃ¡c vai trÃ² vÃ  chÃ­nh sÃ¡ch AWS IAM thá»±c thi Ä‘áº·c quyá»n Ã­t nháº¥t, Ä‘á»ƒ má»—i thÃ nh pháº§n chá»‰ cÃ³ cÃ¡c quyá»n mÃ  nÃ³ yÃªu cáº§u. AWS Secrets Manager lÆ°u trá»¯ an toÃ n thÃ´ng tin Ä‘Äƒng nháº­p, chá»©ng chá»‰ vÃ  dá»‹ch vá»¥ API. AWS Security Groups vÃ  VPC cung cáº¥p cÃ¡ch ly máº¡ng, trong khi TLS 1.2+ vá»›i AWS Certificate Manager xÃ¡c thá»±c táº¥t cáº£ dá»¯ liá»‡u trong quÃ¡ trÃ¬nh truyá»n váº«n Ä‘Æ°á»£c mÃ£ hÃ³a. Báº£o máº­t Ä‘a lá»›p nÃ y bao gá»“m cÃ¡c Ä‘iá»u khiá»ƒn báº£o máº­t chuyÃªn sÃ¢u.\nKháº£ nÄƒng quan sÃ¡t Amazon CloudWatch cung cáº¥p ghi nháº­t kÃ½ táº­p trung, thu tháº­p sá»‘ liá»‡u vÃ  giÃ¡m sÃ¡t thá»i gian thá»±c cá»§a toÃ n bá»™ Ä‘Æ°á»ng á»‘ng tá»« viá»‡c nháº­p dá»¯ liá»‡u thÃ´ng qua pháº£n há»“i mÃ¡y chá»§ MCP. AWS CloudTrail ghi láº¡i cÃ¡c nháº­t kÃ½ hoáº¡t Ä‘á»™ng API chi tiáº¿t vÃ  dáº¥u váº¿t kiá»ƒm toÃ¡n, cáº§n thiáº¿t cho viá»‡c tuÃ¢n thá»§ trong cÃ¡c ngÃ nh Ä‘Æ°á»£c quy Ä‘á»‹nh.\nKáº¿t luáº­n CÃ¹ng nhau, nhá»¯ng mÃ¡y chá»§ MCP nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn AWS vÃ  tÃ­ch há»£p vÃ o Amazon QuickSight Research thá»ƒ hiá»‡n táº§m nhÃ¬n cá»§a S\u0026amp;P Global cho tÆ°Æ¡ng lai cá»§a dá»‹ch vá»¥ tÃ i chÃ­nh vÃ  thÃ´ng tin nÄƒng lÆ°á»£ng: duy trÃ¬ sá»± tin cáº­y, chÃ­nh xÃ¡c vÃ  Ä‘á»™ sÃ¢u mÃ  cÃ¡c nhÃ  lÃ£nh Ä‘áº¡o kinh doanh yÃªu cáº§u trong khi náº¯m báº¯t tiá»m nÄƒng chuyá»ƒn Ä‘á»•i cá»§a AI Ä‘á»ƒ lÃ m cho thÃ´ng tin Ä‘Ã³ dá»… tiáº¿p cáº­n, cÃ³ thá»ƒ thá»±c hiá»‡n vÃ  tÃ­ch há»£p vÃ o quy trÃ¬nh lÃ m viá»‡c hiá»‡n Ä‘áº¡i.\nLink bÃ i viáº¿t gá»‘c : https://aws.amazon.com/blogs/machine-learning/sp-global-data-integration-expands-amazon-quick-research-capabilities/\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/4-sagemaker-training/",
	"title": "End-to-End Model Training Pipeline",
	"tags": [],
	"description": "",
	"content": "Task 4 Objectives Train a BASKET_PRICE_SENSITIVITY prediction model (Low/Medium/High) using Amazon SageMaker with automated ETL â†’ Training â†’ Model Registry pipeline.\nâ†’ The heart of MLOps pipeline - from raw data to production-ready model.\nInput\nAWS Account with SageMaker/S3/CloudWatch permissions S3 bucket with data (from Task 3) SageMaker IAM Role (from Task 2) Output\nModel achieving accuracy â‰¥ 80%, F1 â‰¥ 0.7 Model registered in Model Registry Artifacts stored in S3 Cost: ~$0.3-0.5/job (ml.m5.large, 10-15 minutes)\nğŸ’¡ Task 4 - MLOps Core Pipeline:\nAutomated ETL - Raw data â†’ Features Model training - Random Forest classifier Model evaluation - Accuracy, F1, Confusion Matrix Model Registry - Version control and approval 1. Environment preparation and prerequisites check 1.1. Check S3 bucket (from Task 3) AWS Console â†’ S3:\nFind bucket: mlops-retail-prediction-dev-[account-id]\nCheck actual structure:\nraw/ # transactions.csv + _select/ folder\rsilver/ # shop_week partitions (200607-200619) gold/ # processed features (to be created from silver/)\rartifacts/ # model outputs (to be created) 1.2. Verify IAM Role (from Task 2) AWS Console â†’ IAM â†’ Roles:\nFind role: mlops-retail-prediction-dev-sagemaker-execution Check permissions: âœ… AmazonSageMakerFullAccess âœ… AmazonS3FullAccess âœ… CloudWatchLogsFullAccess Warning: If your bucket uses SSE-KMS, the role needs decrypt/encrypt permissions for the KMS key; if using cross-account S3, check the trust policy as well.\n2. Create SageMaker Domain and Project 2.1. Access SageMaker Unified Studio AWS Console â†’ SageMaker:\nAccess URL: https://[domain-id].studio.sagemaker.[region].amazonaws.com Or from SageMaker Console â†’ Studio â†’ Open Studio Choose authentication method: Sign in with SSO (if SSO is set up) Sign in with AWS IAM (using IAM user/role) After login, you\u0026rsquo;ll see the SageMaker Unified Studio interface Dashboard displays: \u0026ldquo;Good morning\u0026rdquo; greeting Search bar: \u0026ldquo;Search for data products, assets, and projects\u0026rdquo; Discover section: Catalog, Generative AI playground, Shared generative AI assets Build section: ML and generative AI model development, Generative AI app development Browse all projects and Create project buttons Browse all projects and Create project buttons ğŸ’¡ SageMaker Unified Studio:\nUnified interface for data analytics, ML, and generative AI Project-based workspace with shared resources Built-in collaboration with team members and approval workflows Integrated tools: Notebooks, Visual ETL, Workflows, Chat agents 2.2. Create Project in Unified Studio In SageMaker Unified Studio dashboard:\nStep 1: Access Create Project\nIn Build section, click \u0026ldquo;Create project\u0026rdquo; (blue button) Or click \u0026ldquo;Browse all projects\u0026rdquo; â†’ \u0026ldquo;Create project\u0026rdquo; Step 2: Fill Project Information (Step 1)\nProject name: retail-ml-training Description: Retail price sensitivity model training Click Next to proceed to Step 2 Step 2.5: Choose Project Profile (Step 2)\nProject profile: Choose \u0026ldquo;All capabilities\u0026rdquo; (highlighted in blue) Description: \u0026ldquo;Analyze data and build machine learning and generative AI models and applications powered by Amazon Bedrock, Amazon EMR, AWS Glue, Amazon Athena, Amazon SageMaker AI and Amazon SageMaker Lakehouse\u0026rdquo; Tooling: LakeHouse Database, Workflows + 12 more capabilities Other options: Generative AI application development, SQL analytics Other options: Generative AI application development, SQL analytics Step 3: Blueprint Parameters\nS3 location: s3://amazon-sagemaker-[account-id]-ap-southeast-1-[random-id] Retention: 731 days Enable Project Repository Auto Sync: false Lakehouse Database: glue_db Step 4: Create Project\nReview the settings and click \u0026ldquo;Create project\u0026rdquo; Wait 2-3 minutes for the Project to be provisioned 2.3. Access Project Workspace After Project retail-ml-training is successfully created:\nStep 1: Enter Project Overview\nProject will appear in the list with status \u0026ldquo;Created\u0026rdquo; Click on project name retail-ml-training to enter Project overview Project overview displays: Project title: retail-ml-training Description: \u0026ldquo;Retail price sensitivity model training\u0026rdquo; Project files (6): .ipynb_checkpoints, workflows, .libs.json, .temp_sagemaker_unified_studio_debugging_info, README.md, getting_started.ipynb S3 path: /dzd-5kultpj28sm31d/cu2gr2js1w1wv (project workspace path) Actions and New dropdown buttons Actions and New dropdown buttons Project overview (active) Assets, Subscription requests, Data sources, Metadata entities Step 2: Create Notebook\nClick \u0026ldquo;New\u0026rdquo; dropdown (blue button) â†’ select \u0026ldquo;Notebook\u0026rdquo; New dropdown shows options (in order as shown in image): Notebook (select this option) Step 3: Project Welcome In the project overview, you\u0026rsquo;ll also see a Readme section displaying \u0026ldquo;Welcome\u0026rdquo; with guidance on getting started with the project.\n2.4. Verify EC2 Permissions (already configured from Task 2) EC2 permissions were fully configured in Task 2, including the inline policy SageMakerEC2Access in role mlops-retail-prediction-dev-sagemaker-execution.\nVerify existing EC2 permissions:\n# Check inline policy already added from Task 2 aws iam get-role-policy --role-name mlops-retail-prediction-dev-sagemaker-execution --policy-name SageMakerEC2Access # Test EC2 describe permissions # Test EC2 describe permissions aws ec2 describe-vpcs --region us-east-1 Role already has all 4 policies from Task 2:\nâœ… AmazonSageMakerFullAccess (AWS managed) âœ… AmazonS3FullAccess (AWS managed) âœ… CloudWatchLogsFullAccess (AWS managed) âœ… SageMakerEC2Access (inline policy for Project creation) Project creation ready: Role was fully configured from Task 2, can create Project immediately.\n2.5. Region Recommendations for Task 4 Summary: If the main data gold/ and artifacts/ currently reside in bucket mlops-retail-prediction-dev-842676018087 (region us-east-1), the recommendation is to create SageMaker Domain / Project in the same us-east-1 to avoid cross-region errors (S3 301), KMS complexity, and endpoint issues.\nBenefits of creating Project in us-east-1:\nEliminates \u0026lsquo;bucket must be addressed using the specified endpoint\u0026rsquo; errors when SageMaker loads data from S3. No need to maintain KMS keys or IAM policies across multiple regions. Less risk when running training jobs from Studio/Project. When needing to create Project in ap-southeast-1 (or other regions):\nMust transfer or copy gold/ and artifacts/ data to a bucket in that region or configure Cross-Region Replication (CRR). Create corresponding KMS keys and update policies/roles for the new bucket. If you want to keep the Project in ap-southeast-1, here\u0026rsquo;s an example command to create bucket and copy data (PowerShell / CloudShell):\n# Create new bucket in ap-southeast-1 aws s3 mb s3://mlops-retail-prediction-dev-842676018087-apse1 --region ap-southeast-1 # Sync gold/ and artifacts/ to new bucket aws s3 sync s3://mlops-retail-prediction-dev-842676018087/gold/ s3://mlops-retail-prediction-dev-842676018087-apse1/gold/ --acl bucket-owner-full-control --exact-timestamps aws s3 sync s3://mlops-retail-prediction-dev-842676018087/artifacts/ s3://mlops-retail-prediction-dev-842676018087-apse1/artifacts/ --acl bucket-owner-full-control --exact-timestamps # (Optional) Create KMS key in ap-southeast-1 and update bucket policy / IAM role # aws kms create-key --region ap-southeast-1 --description \u0026#34;KMS for mlops retail ap-southeast-1\u0026#34; Notes:\nIf the source bucket uses SSE-KMS, ensure you create corresponding KMS key in the destination region and update both bucket policy and role mlops-retail-prediction-dev-sagemaker-execution. If you want quick resolution with minimal IAM changes, choose to create Project/Domain in us-east-1 (where the bucket currently exists) â€” this is the recommended approach for labs and fast training runs. 3. ETL - Data preparation in SageMaker Studio ğŸ¯ Objective: Read ALL shop_week partitions from S3 silver/ and create train/test/validation splits\nInput: silver/shop_week=200607/ to silver/shop_week=200619/ (14 partitions)\nOutput: gold/train.parquet, gold/test.parquet, gold/validation.parquet\nStep 1: Create ETL Notebook in Project From Project overview:\nClick \u0026ldquo;New\u0026rdquo; dropdown â†’ select \u0026ldquo;Notebook\u0026rdquo; Notebook will open in a new browser tab Choose kernel: Python 3 (Data Science 3.0) Rename notebook: File â†’ Rename â†’ retail-etl-pipeline.ipynb Notebook will auto-save to S3 project path Note:\nNotebook runs on SageMaker\u0026rsquo;s managed compute instance Files automatically sync with S3 project storage Can be shared with team members in project Step 2: Execute ETL Pipeline Create and run the following cells in order: Create and run the following cells in order:\nCell 1: Install dependencies\n# Install all required packages pip install pandas pyarrow s3fs scikit-learn xgboost sagemaker boto3 joblib Cell 2: Setup configuration\nimport boto3 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split import json from datetime import datetime # Configuration - update bucket name for your project # If using project S3: amazon-sagemaker-[account-id]-ap-southeast-1-[random-id] # Or bucket from Task 3: mlops-retail-prediction-dev-[account-id] bucket_name = \u0026#39;amazon-sagemaker-842676018087-ap-southeast-1-f6cd5056a835\u0026#39; # \u0026lt;-- UPDATE for your project raw_prefix = \u0026#39;silver/\u0026#39; gold_prefix = \u0026#39;gold/\u0026#39; # Initialize AWS clients s3 = boto3.client(\u0026#39;s3\u0026#39;) print(f\u0026#39;âœ… AWS clients initialized. Bucket: {bucket_name}\u0026#39;) Cell 3: Load data from all partitions\nprint(f\u0026#39;ğŸ“Š Loading all partitioned data from s3://{bucket_name}/{raw_prefix}...\u0026#39;) # Discover all parquet files in silver/ response = s3.list_objects_v2(Bucket=bucket_name, Prefix=raw_prefix) parquet_files = [obj[\u0026#39;Key\u0026#39;] for obj in response.get(\u0026#39;Contents\u0026#39;, []) if obj[\u0026#39;Key\u0026#39;].endswith(\u0026#39;.parquet\u0026#39;) and obj[\u0026#39;Size\u0026#39;] \u0026gt; 0] print(f\u0026#39;ğŸ“ Found {len(parquet_files)} parquet files\u0026#39;) # Load all files into one DataFrame all_dataframes = [] total_rows = 0 for i, key in enumerate(parquet_files[:10]): # Limit to first 10 files for demo s3_path = f\u0026#39;s3://{bucket_name}/{key}\u0026#39; try: df = pd.read_parquet(s3_path) all_dataframes.append(df) total_rows += len(df) print(f\u0026#39; âœ… File {i+1}: {len(df):,} rows from {key.split(\u0026#34;/\u0026#34;)[-1]}\u0026#39;) except Exception as e: print(f\u0026#39; âŒ Failed to load {key}: {e}\u0026#39;) # Combine all data combined_data = pd.concat(all_dataframes, ignore_index=True) print(f\u0026#39;\\nğŸ¯ Combined dataset: {combined_data.shape}\u0026#39;) print(f\u0026#39;ğŸ“‹ Columns: {list(combined_data.columns)}\u0026#39;) Cell 4: Create features and target variable\nprint(\u0026#34;ğŸ“Œ STEP 1 â€” Columns in combined_data:\u0026#34;) print(list(combined_data.columns)) # Force lowercase column names for safety combined_data.columns = [c.lower() for c in combined_data.columns] print(\u0026#34;\\nğŸ“Œ STEP 2 â€” Columns after lowercase normalization:\u0026#34;) print(list(combined_data.columns)) print(\u0026#34;\\nğŸ“Œ STEP 3 â€” Checking required columns...\u0026#34;) if \u0026#39;basket_id\u0026#39; in combined_data.columns and \u0026#39;spend\u0026#39; in combined_data.columns: print(\u0026#34;âœ… Found basket_id and spend â€” proceeding with basket-level feature engineering.\u0026#34;) print(\u0026#34;\\nğŸ“Œ STEP 4 â€” Converting numeric columns...\u0026#34;) for col in [\u0026#39;spend\u0026#39;, \u0026#39;quantity\u0026#39;]: if col in combined_data.columns: combined_data[col] = pd.to_numeric(combined_data[col], errors=\u0026#39;coerce\u0026#39;) print(\u0026#34;\\nğŸ“Œ STEP 5 â€” Starting groupby aggregation...\u0026#34;) print(\u0026#34;âš™ï¸ Aggregating, this may take a moment...\u0026#34;) features = combined_data.groupby(\u0026#39;basket_id\u0026#39;).agg({ \u0026#39;spend\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;, \u0026#39;count\u0026#39;], \u0026#39;quantity\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;] if \u0026#39;quantity\u0026#39; in combined_data.columns else [] }).reset_index() print(\u0026#34;âœ… Aggregation complete.\u0026#34;) print(f\u0026#34;ğŸ“Š Features raw shape: {features.shape}\u0026#34;) # Flatten column names print(\u0026#34;\\nğŸ“Œ STEP 6 â€” Flattening column names...\u0026#34;) features.columns = ( [\u0026#39;basket_id\u0026#39;, \u0026#39;spend_sum\u0026#39;, \u0026#39;spend_mean\u0026#39;, \u0026#39;spend_count\u0026#39;] + ([\u0026#39;quantity_sum\u0026#39;, \u0026#39;quantity_mean\u0026#39;] if \u0026#39;quantity\u0026#39; in combined_data.columns else []) ) print(\u0026#34;ğŸ“‹ New feature columns:\u0026#34;, list(features.columns)) print(\u0026#34;\\nğŸ“Œ STEP 7 â€” Creating price_sensitivity target variable...\u0026#34;) median_spend = features[\u0026#39;spend_sum\u0026#39;].median() print(f\u0026#34;Median spend = {median_spend}\u0026#34;) features[\u0026#39;price_sensitivity\u0026#39;] = (features[\u0026#39;spend_sum\u0026#39;] \u0026gt; median_spend).astype(int) else: print(\u0026#34;âŒ Could NOT find required columns for basket-level engineering.\u0026#34;) print(\u0026#34;Available columns:\u0026#34;, list(combined_data.columns)) print(\u0026#34;\\nğŸ“Œ Fallback: Using transaction-level feature engineering...\u0026#34;) features = combined_data.copy() if \u0026#39;spend\u0026#39; in features.columns: features[\u0026#39;price_sensitivity\u0026#39;] = ( features[\u0026#39;spend\u0026#39;] \u0026gt; features[\u0026#39;spend\u0026#39;].median() ).astype(int) print(\u0026#34;âœ… Created price_sensitivity for transaction-level data.\u0026#34;) else: raise RuntimeError(\u0026#34;âŒ spend column not found â€” cannot create price_sensitivity.\u0026#34;) print(\u0026#34;\\nğŸ“Œ STEP 8 â€” Final feature table shape:\u0026#34;) print(features.shape) print(\u0026#34;\\nğŸ“Œ STEP 9 â€” Target distribution:\u0026#34;) print(features[\u0026#39;price_sensitivity\u0026#39;].value_counts(dropna=False)) print(\u0026#34;\\nğŸ“Œ STEP 10 â€” Sample output:\u0026#34;) print(features.head()) Cell 5: Create train/test/validation splits and save to S3\nprint(\u0026#39;ğŸ“‹ Creating train/test/validation splits...\u0026#39;) # Create stratified splits train_data, temp_data = train_test_split( features, test_size=0.3, random_state=42, stratify=features[\u0026#39;price_sensitivity\u0026#39;] ) test_data, val_data = train_test_split( temp_data, test_size=0.33, random_state=42, stratify=temp_data[\u0026#39;price_sensitivity\u0026#39;] ) splits = { \u0026#39;train\u0026#39;: train_data, \u0026#39;test\u0026#39;: test_data, \u0026#39;validation\u0026#39;: val_data } # Save to S3 print(f\u0026#39;ğŸ’¾ Saving splits to s3://{bucket_name}/{gold_prefix}...\u0026#39;) for split_name, split_data in splits.items(): s3_path = f\u0026#39;s3://{bucket_name}/{gold_prefix}{split_name}.parquet\u0026#39; split_data.to_parquet(s3_path, index=False) print(f\u0026#39; âœ… {split_name}: {len(split_data):,} rows saved to {split_name}.parquet\u0026#39;) print(\u0026#39;\\nğŸ‰ ETL Complete! Data ready for training.\u0026#39;) # Verify files created response = s3.list_objects_v2(Bucket=bucket_name, Prefix=gold_prefix) if \u0026#39;Contents\u0026#39; in response: print(f\u0026#39;\\nğŸ“ Files in gold/:\u0026#39;) for obj in response[\u0026#39;Contents\u0026#39;]: size_mb = obj[\u0026#39;Size\u0026#39;] / (1024*1024) print(f\u0026#39; ğŸ“„ {obj[\u0026#34;Key\u0026#34;]}: {size_mb:.2f} MB\u0026#39;) 4. Training - Model Training ğŸ¯ Objective: Train Random Forest model to classify customer price sensitivity\nInput: S3 gold/train.parquet, gold/test.parquet, gold/validation.parquet\nOutput: Trained Random Forest model in S3 artifacts/ with performance metrics\nStep 1: Create Training Notebook in Project In Studio interface, click File â†’ New â†’ Notebook Choose conda_python3 kernel (or Python 3 (Data Science)) Name notebook: notebooks/retail-model-training.ipynb Click Create ğŸ’¡ Note: Notebook will be saved in Project repository and can be committed to CodeCommit.\nStep 2: Execute Model Training Create and run the following cells in order: Create and run the following cells in order:\nCell 1: Setup SageMaker Configuration\nimport sagemaker import boto3 import os from sagemaker.sklearn.estimator import SKLearn # Initialize SageMaker session and get execution role sagemaker_session = sagemaker.Session() role = sagemaker.get_execution_role() # Configuration - update bucket name if different bucket_name = \u0026#39;mlops-retail-prediction-dev-842676018087\u0026#39; gold_prefix = \u0026#39;gold/\u0026#39; artifacts_prefix = \u0026#39;artifacts/\u0026#39; print(f\u0026#39;âœ… SageMaker Role: {role}\u0026#39;) print(f\u0026#39;ğŸ“Š Training Data: s3://{bucket_name}/{gold_prefix}\u0026#39;) print(f\u0026#39;ğŸ“¦ Model Artifacts: s3://{bucket_name}/{artifacts_prefix}\u0026#39;) Cell 2: Create Training Script\n%%writefile train_retail_model.py import pandas as pd import joblib import os import json from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report def main(): # Standard SageMaker paths train_dir = os.environ.get(\u0026#34;SM_CHANNEL_TRAIN\u0026#34;, \u0026#34;/opt/ml/input/data/train\u0026#34;) model_dir = os.environ.get(\u0026#34;SM_MODEL_DIR\u0026#34;, \u0026#34;/opt/ml/model\u0026#34;) # 1. Load data train_path = os.path.join(train_dir, \u0026#34;train.parquet\u0026#34;) print(f\u0026#34;ğŸ“– Loading training data from: {train_path}\u0026#34;) df = pd.read_parquet(train_path) print(f\u0026#34;ğŸ“Š Dataset shape: {df.shape}\u0026#34;) print(f\u0026#34;ğŸ“‹ Columns: {list(df.columns)}\u0026#34;) # 2. Prepare features \u0026amp; target target_col = \u0026#34;price_sensitivity\u0026#34; if \u0026#34;price_sensitivity\u0026#34; in df.columns else df.columns[-1] feature_cols = [c for c in df.columns if c not in [target_col, \u0026#34;basket_id\u0026#34;]] X = df[feature_cols] y = df[target_col] print(f\u0026#34;ğŸ”¢ Features: {len(feature_cols)} columns\u0026#34;) print(f\u0026#34;ğŸ¯ Target: {target_col}\u0026#34;) print(f\u0026#34;ğŸ“ˆ Target distribution: {dict(y.value_counts())}\u0026#34;) # 3. Train/validation split (stratified) X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y ) # 4. Train Random Forest model print(\u0026#34;\\nğŸŒ² Training Random Forest model...\u0026#34;) model = RandomForestClassifier( n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42 ) model.fit(X_train, y_train) # 5. Evaluate model y_pred = model.predict(X_val) accuracy = accuracy_score(y_val, y_pred) f1 = f1_score(y_val, y_pred, average=\u0026#34;macro\u0026#34;) precision = precision_score(y_val, y_pred, average=\u0026#34;macro\u0026#34;) recall = recall_score(y_val, y_pred, average=\u0026#34;macro\u0026#34;) # Classification report class_report = classification_report(y_val, y_pred, output_dict=True) print(f\u0026#34;\\nğŸ“Š Model Performance:\u0026#34;) print(f\u0026#34; Accuracy: {accuracy:.4f}\u0026#34;) print(f\u0026#34; Precision: {precision:.4f}\u0026#34;) print(f\u0026#34; Recall: {recall:.4f}\u0026#34;) print(f\u0026#34; F1-Score: {f1:.4f}\u0026#34;) # 6. Save model model_path = os.path.join(model_dir, \u0026#34;model.joblib\u0026#34;) joblib.dump(model, model_path) print(f\u0026#34;ğŸ’¾ Model saved to: {model_path}\u0026#34;) # 7. Save training results results = { \u0026#34;model_name\u0026#34;: \u0026#34;RandomForestClassifier\u0026#34;, \u0026#34;accuracy\u0026#34;: float(accuracy), \u0026#34;precision\u0026#34;: float(precision), \u0026#34;recall\u0026#34;: float(recall), \u0026#34;f1_score\u0026#34;: float(f1), \u0026#34;feature_count\u0026#34;: len(feature_cols), \u0026#34;training_samples\u0026#34;: len(X_train), \u0026#34;validation_samples\u0026#34;: len(X_val), \u0026#34;classification_report\u0026#34;: class_report } results_path = os.path.join(model_dir, \u0026#34;training_results.json\u0026#34;) with open(results_path, \u0026#34;w\u0026#34;) as f: json.dump(results, f, indent=2) # 8. Validate performance targets target_accuracy = 0.80 target_f1 = 0.70 print(f\u0026#39;\\nğŸ¯ Performance validation:\u0026#39;) print(f\u0026#39; ğŸ“Š Accuracy â‰¥ {target_accuracy}: {\u0026#34;âœ…\u0026#34; if accuracy \u0026gt;= target_accuracy else \u0026#34;âŒ\u0026#34;} ({accuracy:.3f})\u0026#39;) print(f\u0026#39; ğŸ“Š F1-Score â‰¥ {target_f1}: {\u0026#34;âœ…\u0026#34; if f1 \u0026gt;= target_f1 else \u0026#34;âŒ\u0026#34;} ({f1:.3f})\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: main() Cell 3: Submit the SageMaker Training Job\nprint(\u0026#34;ğŸš€ Submitting SageMaker Training Job...\u0026#34;) # Pre-flight: check region + data in gold/ # Pre-flight: check region + data in gold/ s3_client = boto3.client(\u0026#34;s3\u0026#34;) bucket_location = s3_client.get_bucket_location(Bucket=bucket_name) bucket_region = bucket_location[\u0026#34;LocationConstraint\u0026#34;] or \u0026#34;us-east-1\u0026#34; current_region = sagemaker_session.boto_region_name print(f\u0026#34;ğŸ“ Bucket region: {bucket_region}\u0026#34;) print(f\u0026#34;ğŸ“ SageMaker region: {current_region}\u0026#34;) # Check cross-region issue # Check cross-region issue if bucket_region != current_region: print(f\u0026#34;âš ï¸ Region mismatch detected!\u0026#34;) print(f\u0026#34; Bucket \u0026#39;{bucket_name}\u0026#39; is in {bucket_region}\u0026#34;) print(f\u0026#34; SageMaker session is in {current_region}\u0026#34;) print(f\u0026#34; This may cause S3 access errors during training\u0026#34;) print(f\u0026#34; Solutions:\u0026#34;) print(f\u0026#34; 1. Create SageMaker Domain in {bucket_region}\u0026#34;) print(f\u0026#34; 2. Copy data to bucket in {current_region}\u0026#34;) print(f\u0026#34; 3. Configure cross-region S3 access\u0026#34;) import warnings warnings.warn(f\u0026#34;Cross-region S3 access: {bucket_region} -\u0026gt; {current_region}\u0026#34;) else: print(f\u0026#34;âœ… Same region - no cross-region issues\u0026#34;) train_s3_uri = f\u0026#34;s3://{bucket_name}/{gold_prefix}\u0026#34; resp = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=gold_prefix) data_files = [o[\u0026#34;Key\u0026#34;] for o in resp.get(\u0026#34;Contents\u0026#34;, []) if o[\u0026#34;Key\u0026#34;].endswith(\u0026#34;.parquet\u0026#34;)] if not data_files: raise ValueError(f\u0026#34;âŒ No .parquet files found in {train_s3_uri}. Run ETL first.\u0026#34;) raise ValueError(f\u0026#34;âŒ No .parquet files found in {train_s3_uri}. Run ETL first.\u0026#34;) print(f\u0026#34;âœ… Found {len(data_files)} training files in {train_s3_uri}\u0026#34;) # Configure estimator estimator = SKLearn( entry_point=\u0026#34;train_retail_model.py\u0026#34;, instance_type=\u0026#34;ml.m5.large\u0026#34;, instance_count=1, role=role, py_version=\u0026#34;py3\u0026#34;, framework_version=\u0026#34;1.2-1\u0026#34;, sagemaker_session=sagemaker_session, ) print(f\u0026#34;ğŸ“Š Training data location: {train_s3_uri}\u0026#34;) # Run training job estimator.fit({\u0026#34;train\u0026#34;: train_s3_uri}, wait=True) job_name = estimator.latest_training_job.name model_artifacts = estimator.model_data print(\u0026#34;\\nğŸ‰ Training job completed!\u0026#34;) print(\u0026#34;ğŸ“ Job name: \u0026#34;, job_name) print(\u0026#34;ğŸ’¾ Model artifacts:\u0026#34;, model_artifacts) except Exception as e: print(f\u0026#39;âŒ Pre-flight check failed: {e}\u0026#39;) print(f\u0026#39; Common fixes:\u0026#39;) print(f\u0026#39; 1. Verify bucket name: {bucket_name}\u0026#39;) print(f\u0026#39; 2. Check gold/ folder has .parquet files\u0026#39;) print(f\u0026#39; 3. Verify IAM role has S3 access\u0026#39;) raise # Configure estimator for Unified Studio project estimator = SKLearn( entry_point=\u0026#39;train_retail_model.py\u0026#39;, instance_type=\u0026#39;ml.m5.large\u0026#39;, instance_count=1, role=role, py_version=\u0026#39;py3\u0026#39;, framework_version=\u0026#39;1.2-1\u0026#39;, sagemaker_session=sagemaker_session, output_path=f\u0026#39;s3://{bucket_name}/{artifacts_prefix}\u0026#39; ) print(f\u0026#39;ğŸ“Š Training data location: {train_s3_uri}\u0026#39;) # Start training job with error handling try: print(\u0026#39;â³ Starting training job (this will take 5â€“10 minutes)...\u0026#39;) estimator.fit({\u0026#39;train\u0026#39;: train_s3_uri}, wait=True) job_name = estimator.latest_training_job.name model_artifacts = estimator.model_data print(\u0026#39;\\nğŸ‰ Training job completed!\u0026#39;) print(\u0026#39;ğŸ“ Job name: \u0026#39;, job_name) print(\u0026#39;ğŸ’¾ Model artifacts:\u0026#39;, model_artifacts) except Exception as e: print(f\u0026#39;âŒ Training job failed: {e}\u0026#39;) print(\u0026#39; Check CloudWatch logs for details:\u0026#39;) print(\u0026#39; AWS Console â†’ CloudWatch â†’ Log groups â†’ /aws/sagemaker/TrainingJobs/\u0026#39;) print(f\u0026#39; Look for log group: {job_name}\u0026#39;) raise Cell 4: Download \u0026amp; Read Training Results\nimport os import tarfile import json import boto3 print(\u0026#34;ğŸ“Š Analyzing training results...\u0026#34;) # model_artifacts from Cell 3 (estimator.model_data) print(\u0026#34;ğŸ“¦ Artifact path:\u0026#34;, model_artifacts) # Extract bucket + key art_parts = model_artifacts.replace(\u0026#34;s3://\u0026#34;, \u0026#34;\u0026#34;).split(\u0026#34;/\u0026#34;, 1) art_bucket = art_parts[0] art_key = art_parts[1] s3 = boto3.client(\u0026#34;s3\u0026#34;) # Download model.tar.gz locally local_tar = \u0026#34;model.tar.gz\u0026#34; s3.download_file(art_bucket, art_key, local_tar) print(\u0026#34;ğŸ“¥ Downloaded\u0026#34;, local_tar) # Extract to model_artifacts/ directory extract_dir = \u0026#34;model_artifacts\u0026#34; os.makedirs(extract_dir, exist_ok=True) with tarfile.open(local_tar, \u0026#34;r:gz\u0026#34;) as tar: tar.extractall(extract_dir) print(\u0026#34;\\nğŸ“‚ Files inside model:\u0026#34;) print(os.listdir(extract_dir)) # Read training_results.json # Read training_results.json results_path = os.path.join(extract_dir, \u0026#34;training_results.json\u0026#34;) with open(results_path, \u0026#34;r\u0026#34;) as f: results = json.load(f) print(\u0026#34;\\nğŸŒ² RANDOM FOREST TRAINING RESULTS:\u0026#34;) print(f\u0026#34; ğŸ¤– Model: {results[\u0026#39;model_name\u0026#39;]}\u0026#34;) print(f\u0026#34; ğŸ“Š Accuracy: {results[\u0026#39;accuracy\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34; ğŸ“Š Precision: {results[\u0026#39;precision\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34; ğŸ“Š Recall: {results[\u0026#39;recall\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34; ğŸ“Š F1-Score: {results[\u0026#39;f1_score\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34; ğŸ”¢ Features: {results[\u0026#39;feature_count\u0026#39;]}\u0026#34;) print(f\u0026#34; ğŸ“š Train rows: {results[\u0026#39;training_samples\u0026#39;]:,}\u0026#34;) print(f\u0026#34; ğŸ§ª Val rows: {results[\u0026#39;validation_samples\u0026#39;]:,}\u0026#34;) print(\u0026#34;\\nğŸ“‹ Per-class Performance:\u0026#34;) class_report = results[\u0026#39;classification_report\u0026#39;] for class_name, metrics in class_report.items(): if isinstance(metrics, dict) and \u0026#39;f1-score\u0026#39; in metrics: print(f\u0026#34; Class {class_name}: F1={metrics[\u0026#39;f1-score\u0026#39;]:.3f}, Precision={metrics[\u0026#39;precision\u0026#39;]:.3f}, Recall={metrics[\u0026#39;recall\u0026#39;]:.3f}\u0026#34;) # Validate target acc_target = 0.80 f1_target = 0.70 print(\u0026#34;\\nğŸ¯ Performance validation:\u0026#34;) print(f\u0026#34; ğŸ“Š Accuracy â‰¥ {acc_target}: {\u0026#39;âœ…\u0026#39; if results[\u0026#39;accuracy\u0026#39;] \u0026gt;= acc_target else \u0026#39;âŒ\u0026#39;} ({results[\u0026#39;accuracy\u0026#39;]:.3f})\u0026#34;) print(f\u0026#34; ğŸ“Š F1-score â‰¥ {f1_target}: {\u0026#39;âœ…\u0026#39; if results[\u0026#39;f1_score\u0026#39;] \u0026gt;= f1_target else \u0026#39;âŒ\u0026#39;} ({results[\u0026#39;f1_score\u0026#39;]:.3f})\u0026#34;) Results âœ… Training Complete! Model achieves target performance and is ready for Model Registry.\n5. Monitoring \u0026amp; Results 5.1. Monitor Training Job in Studio In SageMaker Studio (Unified Studio): In SageMaker Studio (Unified Studio):\nOpen Build section in left sidebar\nSelect Training jobs\nFind job starting with: retail-prediction-training-\nClick on Training Job name to open details\nSelect Logs tab to view real-time logs\n(Optional) Click \u0026ldquo;Open in CloudWatch\u0026rdquo; to view full logs\nInfo:\nCloudWatch logs for Training Jobs are stored as: /aws/sagemaker/TrainingJobs/ If the job fails, Python errors will be at the end of the logs.\n6. Model Registry (New interface in Project) After the training job completes and creates model.tar.gz, the next step is to register the model in Model Registry.\nIn the new SageMaker Unified Studio interface, Model Registry is located within each Project, not separated as before.\n6.1. Open Model Registry in Project SageMaker Studio â†’ Projects â†’ mlops-retail-prediction â†’ Models â†’ Registered models\n6.2. Create Model Group Click Register model group Fill in: Click Register model group Fill in: Name: retail-price-sensitivity-models Description: Model group for retail price sensitivity prediction Click Register model group Click Register model group Model Group will appear in the list.\n6.3. Register Model Version after Training Go to Models â†’ Registered models versions â†’ Model groups Go to Models â†’ Registered models versions â†’ Model groups Select group: retail-price-sensitivity-models Click Register model Enter information: Model artifact location (S3)\nCopy from training job output (e.g., s3://bucket/artifacts/job-name/output/model.tar.gz) Approval status: Pending manual approval Click Register Click Register A new Model Version will be created.\n6.4. Approve a Model Version Open Model group â†’ retail-price-sensitivity-models Select Version 1 Click Update status Set to: Approved Save Task 4 Completion ğŸ“ Execution Notebook: notebooks/sagemaker-retail-etl-training.ipynb\nSuccessfully completed:\nâœ… Created SageMaker Domain and configuration âœ… Created Project and opened Studio workspace âœ… ETL entire dataset - All shop_week partitions â†’ Gold Parquet âœ… Auto-detect partitions - Scan all available shop_week âœ… Train Random Forest with optimal hyperparameters âœ… Single model focus to optimize performance âœ… Spot instances - Cost optimization with auto-scaling âœ… Complete notebook - 4 cells with detailed logging 7. Clean Up Resources (AWS CLI) 7.1. Delete SageMaker Training Jobs # List training jobs # List training jobs aws sagemaker list-training-jobs --name-contains \u0026#34;retail-prediction-training\u0026#34; --query \u0026#39;TrainingJobSummaries[*].[TrainingJobName,TrainingJobStatus]\u0026#39; --output table # Stop running training job (if any) aws sagemaker stop-training-job --training-job-name \u0026lt;job-name\u0026gt; # Training jobs auto-cleanup after completion (no manual deletion needed) 7.2. Delete Model Registry # List model packages # List model packages aws sagemaker list-model-packages --model-package-group-name retail-price-sensitivity-models --query \u0026#39;ModelPackageSummaryList[*].[ModelPackageArn,ModelPackageStatus]\u0026#39; --output table # Delete each model package version # Delete each model package version aws sagemaker delete-model-package --model-package-name \u0026lt;model-package-arn\u0026gt; # Delete model package group aws sagemaker delete-model-package-group --model-package-group-name retail-price-sensitivity-models 7.3. Delete SageMaker Domain and Project # List domains # List domains aws sagemaker list-domains --query \u0026#39;Domains[*].[DomainId,DomainName,Status]\u0026#39; --output table # Delete user profiles first aws sagemaker list-user-profiles --domain-id \u0026lt;domain-id\u0026gt; --query \u0026#39;UserProfiles[*].UserProfileName\u0026#39; --output text # Delete each user profile # Delete each user profile aws sagemaker delete-user-profile --domain-id \u0026lt;domain-id\u0026gt; --user-profile-name \u0026lt;user-profile-name\u0026gt; # Delete domain (after deleting all user profiles) aws sagemaker delete-domain --domain-id \u0026lt;domain-id\u0026gt; 7.4. Clean up S3 artifacts # Delete model artifacts # Delete model artifacts aws s3 rm s3://amazon-sagemaker-\u0026lt;account-id\u0026gt;-\u0026lt;region\u0026gt;-\u0026lt;random-id\u0026gt;/artifacts/ --recursive # Delete gold datasets # Delete gold datasets aws s3 rm s3://amazon-sagemaker-\u0026lt;account-id\u0026gt;-\u0026lt;region\u0026gt;-\u0026lt;random-id\u0026gt;/gold/ --recursive # Check what\u0026#39;s left in project bucket aws s3 ls s3://amazon-sagemaker-\u0026lt;account-id\u0026gt;-\u0026lt;region\u0026gt;-\u0026lt;random-id\u0026gt;/ --recursive 7.5. Delete CloudWatch Logs # List SageMaker log groups # List SageMaker log groups aws logs describe-log-groups --log-group-name-prefix \u0026#34;/aws/sagemaker/TrainingJobs\u0026#34; --query \u0026#39;logGroups[*].logGroupName\u0026#39; # Delete training job logs # Delete training job logs aws logs delete-log-group --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training-\u0026lt;timestamp\u0026gt;\u0026#34; 8. SageMaker Pricing Table 8.1. Training Instance Costs Instance Type vCPU RAM Price (USD/hour) Suitable for ml.m5.large 2 8 GB $0.138 Small datasets, prototyping ml.m5.xlarge 4 16 GB $0.276 Medium datasets (used) ml.m5.2xlarge 8 32 GB $0.552 Large datasets ml.c5.xlarge 4 8 GB $0.238 CPU-intensive training ml.p3.2xlarge 8 61 GB $4.284 GPU deep learning 8.2. SageMaker Studio Costs Component Price (USD) Notes Studio Notebooks $0.0582/hour ml.t3.medium default Domain Setup Free One-time setup Data Wrangler $0.42/hour Visual data prep Processing Jobs Instance pricing Same as training 8.3. Model Registry \u0026amp; Endpoints Costs Service Price (USD) Notes Model Registry Free Model versioning Real-time Endpoint $0.076/hour ml.t2.medium Batch Transform Instance pricing Pay per job Multi-model Endpoint $0.076/hour + storage Cost optimization 8.4. Task 4 Cost Estimate Actual Training Job:\nInstance: ml.m5.xlarge Duration: ~10â€“15 minutes Training cost: $0.276 Ã— 0.25h = $0.07 SageMaker Studio:\nNotebook development: ~2 hours Instance: ml.t3.medium Studio cost: $0.0582 Ã— 2h = $0.12 Storage \u0026amp; Model Registry:\nModel artifacts: ~50MB S3 storage: ~$0.001 Model Registry: Free Total Task 4 Cost:\nComponent Duration Cost Training (ml.m5.xlarge) 15 mins $0.07 Studio Notebooks 2 hours $0.12 S3 Storage Monthly $0.001 Total â‰ˆ $0.19 Comparison with other options:\nApproach Instance Duration Cost Performance Current (ml.m5.xlarge) 4 vCPU, 16GB 15 min $0.07 âœ… Balanced Smaller (ml.m5.large) 2 vCPU, 8GB 25 min $0.06 Slower Larger (ml.m5.2xlarge) 8 vCPU, 32GB 8 min $0.07 Faster, similar cost Spot instance Same specs 15 min $0.02â€“0.05 60â€“70% savings ğŸ’° Cost Optimization Tips:\nSpot instances: 60-70% cheaper for non-critical training Smaller instances: OK for datasets \u0026lt; 1GB Studio auto-shutdown: Auto-stop notebooks after 1h idle Batch jobs: Instead of real-time endpoints for inference ğŸ“Š SageMaker Unified Studio Benefits:\nIntegrated Workspace: Project-based collaboration with shared resources Managed Infrastructure: Auto-provisioned compute for notebooks and training Cross-Region Support: Built-in handling of S3 cross-region access Asset Catalog: Automatic registration of models and datasets Team Collaboration: Shared notebooks, workflows, and approval processes Cost Optimization: Managed compute with automatic scaling Unified Interface: Single pane for data, ML, and generative AI workflows ğŸ“¹ Task 4 Implementation Video Next Step: Task 05: Production VPC\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.3-week3/",
	"title": "Week 3 - Compute VM Service on AWS",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Master EC2 deployment and management. Implement High Availability with Auto Scaling (ASG) and Load Balancing (ALB). Tasks to be carried out this week: Day Task Reference 2 - EC2: Create Launch Templates with User Data scripts.\n- Configure IAM Roles for EC2 (S3 access). AWS EC2 Docs 3 - Load Balancing: Deploy Application Load Balancer (ALB).\n- Configure Target Groups and Health Checks (/health). AWS ELB Docs 4 - Auto Scaling: Create Auto Scaling Group (ASG) across Multi-AZ.\n- Set scaling policies (e.g., CPU \u0026gt; 50%). AWS ASG Docs 5 - Monitoring: Install CloudWatch Agent via User Data.\n- Test scaling events (stress test). Hands-on Lab 6 - Review: Verify ALB routing and ASG dynamic scaling.\n- Weekly Report. - Week 3 Achievements: Automated EC2 provisioning using Launch Templates and User Data. Deployed a highly available web app with ALB and ASG. Secured EC2 access using IAM Roles instead of hardcoded keys. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.5-blog5/",
	"title": "CÃ¡ch VÄƒn phÃ²ng Tá»•ng chÆ°á»Ÿng lÃ½ Quáº­n Contra Costa hiá»‡n Ä‘áº¡i hÃ³a quy trÃ¬nh xá»­ lÃ½ trÃ¡t Ä‘Ã²i háº§u tÃ²a vá»›i AWS vÃ  CC Tech Digital",
	"tags": [],
	"description": "",
	"content": "BÃ i viáº¿t gá»‘c Ä‘Æ°á»£c Ä‘Äƒng vÃ o ngÃ y 08 thÃ¡ng 12 nÄƒm 2025\nChuyá»ƒn Ä‘á»•i sá»‘ trong chÃ­nh phá»§ khÃ´ng cÃ²n lÃ  tÃ¹y chá»n - Ä‘Ã³ lÃ  Ä‘iá»u báº¯t buá»™c. VÄƒn phÃ²ng Tá»•ng chÆ°á»Ÿng lÃ½ Quáº­n Contra Costa Ä‘Ã£ há»£p tÃ¡c vá»›i AWS (Amazon Web Services) vÃ  CC Tech Digital, má»™t Äá»‘i tÃ¡c Cáº¥p cao AWS, Ä‘á»ƒ hiá»‡n Ä‘áº¡i hÃ³a quy trÃ¬nh xá»­ lÃ½ trÃ¡t Ä‘Ã²i háº§u tÃ²a báº±ng giáº£i phÃ¡p serverless, cloud-native trÃªn AWS.\nBáº±ng cÃ¡ch tá»± Ä‘á»™ng hÃ³a viá»‡c táº£i lÃªn tÃ i liá»‡u, tÃ­ch há»£p trá»±c tiáº¿p vá»›i Microsoft Word, vÃ  Ä‘á»‹nh tuyáº¿n an toÃ n trÃ¡t Ä‘Ã²i háº§u tÃ²a Ä‘áº¿n cÃ¡c cÆ¡ quan Ä‘Ãºng, há»‡ thá»‘ng má»›i hiá»‡n quáº£n lÃ½ hÆ¡n 17.000 trÃ¡t Ä‘Ã²i háº§u tÃ²a hÃ ng nÄƒm - vá»›i tá»‘c Ä‘á»™, Ä‘á»™ chÃ­nh xÃ¡c vÃ  tuÃ¢n thá»§ Ä‘Æ°á»£c cáº£i thiá»‡n.\nThÃ¡ch thá»©c: Quy trÃ¬nh trÃ¡t Ä‘Ã²i háº§u tÃ²a thá»§ cÃ´ng lÃ m cháº­m hoáº¡t Ä‘á»™ng TrÆ°á»›c khi hiá»‡n Ä‘áº¡i hÃ³a, viá»‡c xá»­ lÃ½ trÃ¡t Ä‘Ã²i háº§u tÃ²a á»Ÿ Quáº­n Contra Costa chá»§ yáº¿u dá»±a vÃ o quy trÃ¬nh lÃ m viá»‡c thá»§ cÃ´ng gÃ¢y ra sá»± kÃ©m hiá»‡u quáº£ táº¡i nhiá»u Ä‘iá»ƒm tiáº¿p xÃºc:\nTrÃ¡t Ä‘Ã²i háº§u tÃ²a Ä‘Æ°á»£c táº£i lÃªn cÃ¡c chia sáº» tá»‡p táº¡i chá»— Tá»‡p PDF Ä‘Æ°á»£c chia tÃ¡ch vÃ  xem xÃ©t thÃ´ng qua cÃ¡c bÆ°á»›c thá»§ cÃ´ng Giao tiáº¿p email dá»±a trÃªn danh sÃ¡ch liÃªn há»‡ tÄ©nh Kháº£ nÄƒng theo dÃµi vÃ  bÃ¡o cÃ¡o tá»‘i thiá»ƒu Phá»‘i há»£p liÃªn há»‡ thá»‘ng Ä‘Ã²i há»i sá»± tham gia Ä‘Ã¡ng ká»ƒ cá»§a IT Máº·c dÃ¹ cÃ¡ch tiáº¿p cáº­n nÃ y hoáº¡t Ä‘á»™ng trong nhiá»u nÄƒm, nhÆ°ng nÃ³ tá»‘n thá»i gian vÃ  dá»… cÃ³ sá»± khÃ´ng nháº¥t quÃ¡n. Khi nhu cáº§u minh báº¡ch vÃ  giao tiáº¿p phÃ¡p lÃ½ ká»‹p thá»i tÄƒng lÃªn, nhá»¯ng quy trÃ¬nh lÃ m viá»‡c thá»§ cÃ´ng nÃ y trá»Ÿ nÃªn khÃ³ duy trÃ¬ hÆ¡n.\nGiáº£i phÃ¡p: Ná»n táº£ng trÃ¡t Ä‘Ã²i háº§u tÃ²a serverless, cloud-native Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn AWS Äá»ƒ giáº£i quyáº¿t nhá»¯ng thÃ¡ch thá»©c nÃ y, CC Tech Digital vÃ  AWS Ä‘Ã£ cung cáº¥p má»™t á»©ng dá»¥ng hoÃ n toÃ n tá»± Ä‘á»™ng, cloud-native Ä‘Æ°á»£c há»— trá»£ bá»Ÿi kiáº¿n trÃºc serverless cá»§a AWS. Giáº£i phÃ¡p nÃ y tÃ­ch há»£p liá»n máº¡ch vá»›i cÃ¡c há»‡ thá»‘ng quáº­n hiá»‡n cÃ³, bao gá»“m Microsoft Word, vÃ  chá»‰ yÃªu cáº§u Ä‘Ã o táº¡o tá»‘i thiá»ƒu cho nhÃ¢n viÃªn phÃ¡p lÃ½.\nGá»­i trÃ¡t Ä‘Ã²i háº§u tÃ²a Ä‘Æ°á»£c há»£p lÃ½ hÃ³a qua Microsoft Word NhÃ¢n viÃªn phÃ¡p lÃ½ hiá»‡n cÃ³ thá»ƒ táº£i lÃªn trÃ¡t Ä‘Ã²i háº§u tÃ²a trá»±c tiáº¿p tá»« Word báº±ng Add-In Microsoft Office Ä‘Æ°á»£c tÃ¹y chá»‰nh. Chá»‰ vá»›i má»™t cÃº nháº¥p chuá»™t, ngÆ°á»i dÃ¹ng gá»­i tÃ i liá»‡u vÃ o pipeline xá»­ lÃ½ - mÃ  khÃ´ng cáº§n rá»i khá»i khÃ´ng gian lÃ m viá»‡c quen thuá»™c cá»§a há». Add-in xÃ¡c thá»±c Ä‘á»‹nh dáº¡ng tÃ i liá»‡u, sá»‘ trang vÃ  cÃ¡c trÆ°á»ng báº¯t buá»™c theo thá»i gian thá»±c, giáº£m cÃ¡c láº§n gá»­i tháº¥t báº¡i vÃ  Ä‘áº£m báº£o tuÃ¢n thá»§ ban Ä‘áº§u.\nHÃ¬nh 1. Máº«u tÃ i liá»‡u trÃ¡t Ä‘Ã²i háº§u tÃ²a vÃ  cá»•ng táº£i lÃªn Add-in Microsoft\nXá»­ lÃ½ PDF thÃ´ng minh vÃ  gá»­i email trÃªn AWS Khi má»™t tÃ i liá»‡u Ä‘Æ°á»£c táº£i lÃªn, má»™t pipeline serverless hoÃ n toÃ n sáº½ Ä‘áº£m nháº­n:\nPhÃ¡t hiá»‡n tÃ i liá»‡u\nCÃ¡c tá»‡p Ä‘Æ°á»£c táº£i lÃªn Amazon Simple Storage Service (Amazon S3) sáº½ Ä‘Æ°á»£c phÃ¡t hiá»‡n bá»Ÿi AWS Lambda, kÃ­ch hoáº¡t quy trÃ¬nh lÃ m viá»‡c xá»­ lÃ½.\nChia tÃ¡ch PDF tá»± Ä‘á»™ng\nTrÃ¡t Ä‘Ã²i háº§u tÃ²a nhiá»u trang (20-30 trang) Ä‘Æ°á»£c chia thÃ nh nhiá»u PDF, vá»›i má»—i cÆ¡ quan chá»‰ nháº­n hai trang liÃªn quan, theo chÃ­nh sÃ¡ch phÃ¡p lÃ½.\nTrÃ­ch xuáº¥t trÆ°á»ng dá»±a trÃªn AI\nSá»­ dá»¥ng Amazon Textract, cÃ¡c trÆ°á»ng chÃ­nh nhÆ° sá»‘ vá»¥ Ã¡n, tÃªn cÆ¡ quan vÃ  metadata trÃ¡t Ä‘Ã²i háº§u tÃ²a Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« tÃ i liá»‡u.\nÄá»‹nh tuyáº¿n Ä‘á»™ng vÃ  gá»­i email\nMá»™t tra cá»©u Ä‘Æ°á»£c thá»±c hiá»‡n trong Amazon DynamoDB Ä‘á»ƒ xÃ¡c Ä‘á»‹nh ngÆ°á»i nháº­n cÆ¡ quan Ä‘Ãºng. PDF tÆ°Æ¡ng á»©ng sau Ä‘Ã³ Ä‘Æ°á»£c gá»­i email tá»± Ä‘á»™ng qua Amazon Simple Email Service (Amazon SES), loáº¡i bá» Ä‘á»‹nh tuyáº¿n thá»§ cÃ´ng.\nXá»­ lÃ½ lá»—i vÃ  thÃ´ng bÃ¡o quáº£n trá»‹ viÃªn\nNáº¿u há»‡ thá»‘ng gáº·p pháº£i sá»± khÃ´ng khá»›p hoáº·c lá»—i, nÃ³ tá»± Ä‘á»™ng thÃ´ng bÃ¡o cho admin vá»›i cháº©n Ä‘oÃ¡n lá»—i vÃ  hÆ°á»›ng dáº«n.\nHÃ¬nh 2. Máº«u email vá»›i thÃ´ng tin Ä‘Ã­nh kÃ¨m Ä‘Æ°á»£c cÃ¡ nhÃ¢n hÃ³a cho ngÆ°á»i nháº­n\nBáº£o máº­t, tuÃ¢n thá»§ vÃ  tá»‘i Æ°u hÃ³a chi phÃ­ Ä‘Æ°á»£c tÃ­ch há»£p sáºµn Báº£o máº­t vÃ  tuÃ¢n thá»§ lÃ  ná»n táº£ng cho thiáº¿t káº¿ cá»§a giáº£i phÃ¡p:\nAWS WAF: Quyá»n truy cáº­p táº£i lÃªn Ä‘Æ°á»£c háº¡n cháº¿ Ä‘á»‘i vá»›i cÃ¡c Ä‘á»‹a chá»‰ IP Ä‘Æ°á»£c á»§y quyá»n cá»§a Quáº­n Contra Costa, giáº£m rá»§i ro gá»­i khÃ´ng Ä‘Æ°á»£c á»§y quyá»n.\nAmazon S3: Táº¥t cáº£ tÃ i liá»‡u Ä‘Æ°á»£c mÃ£ hÃ³a khi lÆ°u trá»¯ báº±ng AWS Key Management Service (KMS) vá»›i cÃ¡c mÃ´-Ä‘un mÃ£ hÃ³a Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c thá»±c FIPS 140-2 vÃ  Ä‘Æ°á»£c mÃ£ hÃ³a trong quÃ¡ trÃ¬nh truyá»n táº£i qua TLS báº±ng cÃ¡c Ä‘iá»ƒm cuá»‘i tuÃ¢n thá»§ FIPS.\nAWS Identity and Access Management (IAM): CÃ¡c chÃ­nh sÃ¡ch dá»±a trÃªn vai trÃ² Ä‘áº£m báº£o ráº±ng chá»‰ nhÃ¢n viÃªn Ä‘Æ°á»£c á»§y quyá»n má»›i cÃ³ thá»ƒ táº£i lÃªn tá»‡p trÃ¡t Ä‘Ã²i háº§u tÃ²a.\nChÃ­nh sÃ¡ch vÃ²ng Ä‘á»i Amazon S3: TÃ i liá»‡u tá»± Ä‘á»™ng chuyá»ƒn Ä‘á»•i sang lÆ°u trá»¯ láº¡nh tiáº¿t kiá»‡m chi phÃ­ (vÃ­ dá»¥: S3 Glacier) sau 90 ngÃ y Ä‘á»ƒ Ä‘Ã¡p á»©ng nhu cáº§u lÆ°u giá»¯ dá»¯ liá»‡u vÃ  ngÃ¢n sÃ¡ch.\nNhá»¯ng tÃ­nh nÄƒng nÃ y giÃºp VÄƒn phÃ²ng Tá»•ng chÆ°á»Ÿng lÃ½ Quáº­n Contra Costa Ä‘Ã¡p á»©ng cÃ¡c tiÃªu chuáº©n báº£o vá»‡ dá»¯ liá»‡u nghiÃªm ngáº·t mÃ  khÃ´ng lÃ m phá»©c táº¡p cho nhÃ¢n viÃªn hoáº·c nhÃ³m IT.\nKáº¿t quáº£: TÃ¡c Ä‘á»™ng cÃ³ thá»ƒ Ä‘o lÆ°á»ng trong toÃ n tá»• chá»©c Báº£ng dÆ°á»›i Ä‘Ã¢y ná»•i báº­t cÃ¡c káº¿t quáº£ Ä‘Æ°á»£c VÄƒn phÃ²ng Tá»•ng chÆ°á»Ÿng lÃ½ Quáº­n Contra Costa Ä‘áº¡t Ä‘Æ°á»£c sau khi triá»ƒn khai giáº£i phÃ¡p serverless, cloud-native má»›i:\nDanh má»¥c TÃ¡c Ä‘á»™ng Hiá»‡u quáº£ Giáº£m cÃ¡c tÃ¡c vá»¥ thá»§ cÃ´ng hÆ¡n 80% Äá»™ chÃ­nh xÃ¡c Cáº£i thiá»‡n Ä‘á»‹nh tuyáº¿n tÃ i liá»‡u vá»›i AI vÃ  tra cá»©u Ä‘á»™ng Tráº£i nghiá»‡m ngÆ°á»i dÃ¹ng Táº£i lÃªn má»™t cÃº nháº¥p trá»±c tiáº¿p tá»« Word vá»›i pháº£n há»“i thá»i gian thá»±c TuÃ¢n thá»§ \u0026amp; Báº£o máº­t Kiá»ƒm soÃ¡t truy cáº­p Ä‘Æ°á»£c thá»±c thi, lÆ°u trá»¯ Ä‘Æ°á»£c mÃ£ hÃ³a vÃ  kháº£ nÄƒng hiá»ƒn thá»‹ kiá»ƒm toÃ¡n Ä‘áº§y Ä‘á»§ Kháº£ nÄƒng má»Ÿ rá»™ng KhÃ´ng cÃ³ cÆ¡ sá»Ÿ háº¡ táº§ng Ä‘á»ƒ quáº£n lÃ½; xá»­ lÃ½ hÆ¡n 17.000 trÃ¡t Ä‘Ã²i háº§u tÃ²a hÃ ng nÄƒm - vÃ  con sá»‘ nÃ y dá»± kiáº¿n sáº½ tÄƒng \u0026ldquo;CC Tech Ä‘Ã£ giÃºp chÃºng tÃ´i chuyá»ƒn Ä‘á»•i cÃ¡ch chÃºng tÃ´i xá»­ lÃ½ trÃ¡t Ä‘Ã²i háº§u tÃ²a - cáº£i thiá»‡n hiá»‡u quáº£, minh báº¡ch vÃ  tuÃ¢n thá»§ vá»›i giáº£i phÃ¡p cloud-native hiá»‡n Ä‘áº¡i trÃªn AWS.\u0026rdquo;\nâ€” James Mount, GiÃ¡m Ä‘á»‘c IT, VÄƒn phÃ²ng Tá»•ng chÆ°á»Ÿng lÃ½ Quáº­n Contra Costa\nKáº¿t luáº­n: Má»™t báº£n thiáº¿t káº¿ cÃ³ thá»ƒ má»Ÿ rá»™ng cho hiá»‡n Ä‘áº¡i hÃ³a khu vá»±c cÃ´ng Dá»± Ã¡n nÃ y lÃ  má»™t mÃ´ hÃ¬nh vá» cÃ¡ch cÃ¡c cÆ¡ quan chÃ­nh phá»§ Ä‘á»‹a phÆ°Æ¡ng cÃ³ thá»ƒ hiá»‡n Ä‘áº¡i hÃ³a quy trÃ¬nh lÃ m viá»‡c tÃ i liá»‡u báº±ng AWS. VÄƒn phÃ²ng Tá»•ng chÆ°á»Ÿng lÃ½ Quáº­n Contra Costa hiá»‡n Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i tá»« má»™t ná»n táº£ng trÃ¡t Ä‘Ã²i háº§u tÃ²a hoÃ n toÃ n tá»± Ä‘á»™ng, Ä‘Æ°á»£c tÄƒng cÆ°á»ng bá»Ÿi AI, loáº¡i bá» cÃ¡c bÆ°á»›c thá»§ cÃ´ng, cáº£i thiá»‡n tuÃ¢n thá»§ vÃ  má»Ÿ rá»™ng dá»… dÃ ng theo nhu cáº§u.\nBáº±ng cÃ¡ch há»£p tÃ¡c vá»›i CC Tech Digital vÃ  táº­n dá»¥ng cÃ¡c dá»‹ch vá»¥ AWS nhÆ° Amazon Textract, AWS Lambda, Amazon SES vÃ  AWS WAF, cÃ¡c tá»• chá»©c khu vá»±c cÃ´ng cÃ³ thá»ƒ hiá»‡n Ä‘áº¡i hÃ³a cÃ¡c hoáº¡t Ä‘á»™ng quan trá»ng trong tÆ°Æ¡ng lai mÃ  khÃ´ng lÃ m giÃ¡n Ä‘oáº¡n cÃ¡c cÃ´ng cá»¥ hoáº·c nhÃ³m hiá»‡n cÃ³ cá»§a há».\nLink bÃ i viáº¿t gá»‘c : https://aws.amazon.com/blogs/publicsector/how-contra-costa-county-district-attorneys-office-modernized-subpoena-processing-with-aws-and-cc-tech-digital/\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "AWS MLOps Retail Prediction Platform\rEnd-to-end MLOps pipeline for Retail Prediction with Infrastructure as Code and Model Deployment\nğŸ‘¨â€ğŸ’» Author 1\rNguyá»…n Thanh NhÃ¢n\nCloud Engineer\nnpthanhnhan2003@gmail.com\rnguyen-nhan-732a66247\ruit-ntn\rUIT.NTN.13\rğŸ‘©â€ğŸ’» Author 2\rHÃ  Kháº£ NguyÃªn\nData Engineer\nhakhanguyen09052004@gmail.com\rLinkedIn\rNguyenHK64\rFacebook\rğŸ›’ Retail Prediction MLOps Technology Stack\rğŸ—ï¸ Infrastructure\rTerraform + EKS + VPC + IAM\rğŸ¤– ML Training\rSageMaker + Model Registry + S3\rğŸ³ Containers\rEKS + ECR + Docker + HPA\rğŸ’¾ Data \u0026 Storage\rS3 Data Lake + Versioning + Lifecycle\rğŸ”„ CI/CD \u0026 Automation\rJenkins + Pipeline Automation\rğŸ“Š Monitoring \u0026 Security\rCloudWatch + KMS + CloudTrail\rğŸ“š Retail Prediction MLOps Workshop\r12 Complete MLOps Tasks for Retail Prediction\rAn end-to-end MLOps pipeline from Infrastructure as Code to Model Deployment, including Monitoring and Cost Optimization for Retail Prediction.\rğŸ—ï¸ Infrastructure\rğŸ¤– ML Training\rğŸš€ Deployment\rğŸ“Š Monitoring\rğŸ”„ CI/CD\rğŸ’° Cost Optimization\rğŸ—ï¸\rInfrastructure Foundation\râ–¸Retail prediction architecture \u0026 objectives\râ–¸VPC, subnets, NAT, security groups\râ–¸IAM roles \u0026 IRSA configuration\râ–¸EKS control plane setup\râ–¸EC2 managed node groups\rğŸ¤–\rML Training \u0026 Registry\râ–¸S3 data lake \u0026 model artifacts\râ–¸SageMaker retail prediction training \u0026 model registry\rğŸš€\rContainer Deployment\râ–¸ECR container registry\râ–¸EKS cluster setup\râ–¸API containerization\râ–¸Deploy to Kubernetes\râ–¸Load balancing\rğŸ“Š\rMonitoring \u0026 Observability\râ–¸CloudWatch monitoring \u0026 logs\râ–¸CI/CD pipeline\rğŸ’°\rCost \u0026 Teardown\râ–¸Cost optimization \u0026 teardown\rğŸš€\rGet Started with the Retail Prediction MLOps Workshop\rğŸ“‹ Prerequisites\rAWS Account, Terraform, kubectl, Docker\râ±ï¸ Duration\r12â€“15 hours (end-to-end MLOps)\rğŸ“ˆ Level\rIntermediate to Advanced\rğŸ¯ Start with Task 1: Retail Prediction Architecture Overview\râ†’\râœ¨ Workshop Highlights\rğŸ—ï¸\rInfrastructure as Code\rTerraform automation for all AWS resources\rğŸ¤–\rSageMaker Training\rModel training and registry for retail prediction\rğŸš€\rEKS Deployment\rKubernetes deployment for the retail prediction API\rğŸ”’\rSecurity-first\rKMS encryption + CloudTrail auditing\rğŸ”„\rCI/CD Pipeline\rAutomated build â†’ train â†’ deploy\rğŸ’°\rCost Optimized\rAuto-scaling + Spot instances\râ¬†ï¸\r"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/5-vpc/",
	"title": "Production Networking",
	"tags": [],
	"description": "",
	"content": "ğŸ¯ Task 5 Objectives Set up Production VPC for EKS deployment and public API demo (separate from SageMaker training VPC):\nProduction EKS Infrastructure - EKS Cluster and Pods in private subnets Public API Access - ALB in public subnets for demo endpoint /predict High-Performance Internal Networking - VPC Endpoints for S3/ECR access \u0026lt; 50ms latency Cost Optimization - Remove NAT Gateway, only enable ALB when demo VPC Separation Strategy\nTask 4: SageMaker training uses default VPC (Quick setup) - simple, cost-effective Task 5: EKS production uses separate VPC - better security, control No conflict: 2 independent VPCs, can connect via VPC Peering if needed **ğŸ¯ Production VPC Architecture:**\râœ… Private Subnets: EKS Pods (secure, no direct Internet) âœ… Public Subnets: ALB only (public API demo access) âœ… Internal Communication: EKS â†” S3 via VPC Endpoints âœ… Demo Ready: Public API endpoint via ALB with SSL/health checks ğŸ“¥ Input\nAWS Account with VPC permissions CIDR planning: 10.0.0.0/16 (production EKS VPC) Demo requirements: Public API access via ALB Task 4 completed: SageMaker training running in default VPC Input from Task 4: Task 4 (SageMaker training) â€” training VPC choices and requirements Input from Task 2: Task 2 (IAM Roles \u0026amp; Audit) â€” roles and policies required for VPC, EKS and ECR access 1. Hybrid VPC Infrastructure Setup 1.1. Create VPC Access VPC Dashboard: AWS Console â†’ VPC service â†’ \u0026ldquo;Create VPC\u0026rdquo; VPC Configuration: VPC Name: mlops-retail-forecast-hybrid-vpc\rIPv4 CIDR: 10.0.0.0/16\rIPv6 CIDR: No IPv6 CIDR block\rTenancy: Default\rEnable DNS hostnames: âœ… (Required for VPC Endpoints)\rEnable DNS resolution: âœ… (Required for VPC Endpoints) 1.2. Create Subnets VPC â†’ Subnets â†’ Create subnet\nPublic Subnets (ALB only) Public Subnet 1 (ap-southeast-1a)\nName: mlops-hybrid-public-alb-ap-southeast-1a\rAZ: ap-southeast-1a\rCIDR: 10.0.1.0/24 Public Subnet 2 (ap-southeast-1b)\nName: mlops-hybrid-public-alb-ap-southeast-1b\rAZ: ap-southeast-1b\rCIDR: 10.0.2.0/24 Private Subnets (EKS workloads) Private Subnet 1 (ap-southeast-1a)\nName: mlops-eks-private-workloads-ap-southeast-1a\rAZ: ap-southeast-1a\rCIDR: 10.0.101.0/24 Private Subnet 2 (ap-southeast-1b)\nName: mlops-eks-private-workloads-ap-southeast-1b\rAZ: ap-southeast-1b\rCIDR: 10.0.102.0/24 1.3 Internet Gateway (for public ALB) Create Internet Gateway: \u0026ldquo;Internet Gateways\u0026rdquo; â†’ \u0026ldquo;Create internet gateway\u0026rdquo; Name: mlops-hybrid-igw\rPurpose: Public access for ALB demo endpoint Attach to VPC: Select Internet Gateway â†’ \u0026ldquo;Actions\u0026rdquo; â†’ \u0026ldquo;Attach to VPC\u0026rdquo; Select: mlops-retail-forecast-hybrid-vpc 1.4. Route Tables Configuration 1.4.1. Public Route Table (ALB Traffic) Create route table:\nName: mlops-hybrid-public-alb-rt\rVPC: mlops-retail-forecast-hybrid-vpc Add route:\n0.0.0.0/0 â†’ mlops-hybrid-igw Associate both public subnets to this route table.\nPrivate Route Table (workload subnets) Create route table:\nName: mlops-hybrid-private-workloads-rt\rVPC: mlops-retail-forecast-hybrid-vpc Keep only:\n10.0.0.0/16 â†’ local Do NOT add 0.0.0.0/0 (no NAT, no IGW).\nAssociate both private subnets to this route table.\n1.5 Enable â€œAuto-assign public IPv4â€ for Public Subnets VPC â†’ Subnets â†’ select each public subnet â†’ Actions â†’ Modify auto-assign IP settings\nâœ… Enable auto-assign public IPv4 address If this is OFF, your Internet-facing ALB may fail or behave incorrectly.\n2) Security Groups (Layered Model) 2.1 ALB Security Group (Internet â†’ ALB) Name: mlops-hybrid-alb-sg\rVPC: mlops-retail-forecast-hybrid-vpc Inbound:\nTCP 80 from 0.0.0.0/0 (optional; redirect to 443) TCP 443 from 0.0.0.0/0 (recommended) Outbound:\nTCP 80 â†’ EKS Nodes SG (or all traffic to EKS Nodes SG for simplicity in demo) 2.2 EKS Nodes SG (ALB â†’ Nodes/Pods) Name: mlops-hybrid-eks-nodes-sg\rVPC: mlops-retail-forecast-hybrid-vpc Inbound:\nTCP 80 from ALB SG (API traffic) TCP 443 from ALB SG (if you terminate TLS at pod/service) All traffic from self (node-to-node) All traffic from EKS Control Plane SG (cluster management) Outbound:\nTCP 443 to VPC Endpoints SG (AWS services over PrivateLink) All traffic to self 2.3 EKS Control Plane SG Name: mlops-hybrid-eks-control-plane-sg\rVPC: mlops-retail-forecast-hybrid-vpc Inbound:\nTCP 443 from EKS Nodes SG Outbound:\nAll traffic to EKS Nodes SG 2.4 VPC Endpoints SG Name: mlops-hybrid-vpc-endpoints-sg\rVPC: mlops-retail-forecast-hybrid-vpc Inbound:\nğŸ¯ Security Groups Complete!\n4 Security Groups Created:\nALB SG: Public Internet access (80/443) EKS Nodes SG: Private workloads EKS Control Plane SG: Cluster management VPC Endpoints SG: AWS services access Note: SageMaker will use default SG in default VPC (Task 4)\n1.6. Enable Auto-assign Public IP for ALB Subnets Important for ALB functionality:\nNavigate to Public Subnets:\nVPC Dashboard â†’ Subnets Enable Auto-assign Public IP:\nSelect each public subnet Actions â†’ \u0026ldquo;Modify auto-assign IP settings\u0026rdquo; âœ… Enable auto-assign public IPv4 address âš ï¸ Critical for ALB: Public subnets must have auto-assign public IP enabled, otherwise ALB creation will fail.\n1.7. Console Setup Complete ğŸ¯ Hybrid VPC Console Setup Complete!\nSecurity Architecture:\nLayer 1: Internet â†’ ALB (80/443 from 0.0.0.0/0) Layer 2: ALB â†’ EKS Nodes (controlled access) Layer 3: EKS â†’ VPC Endpoints (AWS services only) Layer 4: Private subnets completely isolated from Internet Demo Ready: ALB can accept public traffic and route to private EKS API pods\n2. VPC Endpoints for High-Performance Internal Networking This step is MANDATORY to ensure EKS â†” S3 â†” SageMaker latency \u0026lt; 50ms:\n2.1. S3 Gateway Endpoint (FREE - Model/Data Access) Create S3 Gateway Endpoint: VPC Dashboard â†’ \u0026ldquo;Endpoints\u0026rdquo; â†’ \u0026ldquo;Create endpoint\u0026rdquo; Endpoint name: mlops-hybrid-s3-gateway-endpoint\rService: com.amazonaws.ap-southeast-1.s3\rType: Gateway\rVPC: mlops-retail-forecast-hybrid-vpc\rRoute Tables: mlops-hybrid-private-workloads-rt\rPolicy: Full Access (demo purposes) Purpose: EKS Pods load model artifacts from S3 \u0026lt; 50ms latency\nCreate these as Interface endpoints:\nECR API Name: mlops-hybrid-ecr-api-endpoint\rService: com.amazonaws.ap-southeast-1.ecr.api\rType: Interface\rSubnets: both private subnets\rSecurity group: mlops-hybrid-vpc-endpoints-sg\rPrivate DNS: âœ… Enabled ECR DKR Purpose: EKS pull container images from ECR repository\n2.3. ECR DKR Interface Endpoint (Docker Registry) Create ECR DKR Endpoint: Endpoint name: mlops-hybrid-ecr-dkr-endpoint\rService: com.amazonaws.ap-southeast-1.ecr.dkr\rType: Interface\rVPC: mlops-retail-forecast-hybrid-vpc\rSubnets: Both private workload subnets\rSecurity Groups: mlops-hybrid-vpc-endpoints-sg\rPrivate DNS: âœ… Enabled CloudWatch Logs Purpose: Docker layer downloads for EKS container runtime\n2.4. CloudWatch Logs Interface Endpoint Create CloudWatch Logs Endpoint: Endpoint name: mlops-hybrid-logs-endpoint\rService: com.amazonaws.ap-southeast-1.logs\rType: Interface\rVPC: mlops-retail-forecast-hybrid-vpc\rSubnets: Both private workload subnets\rSecurity Groups: mlops-hybrid-vpc-endpoints-sg\rPrivate DNS: âœ… Enabled Purpose: Monitoring and logging for EKS/SageMaker workloads\n2.5. VPC Endpoints Verification Expected Results:\nS3 Gateway: Route added to private route table automatically 3x Interface Endpoints: ENI created in each private subnet Private DNS: All endpoints resolvable via internal DNS ğŸ¯ VPC Endpoints Complete!\nHigh-Performance Internal Network:\nEKS â†” S3: \u0026lt; 50ms (Gateway Endpoint) EKS â†” ECR: \u0026lt; 50ms (Interface Endpoints) EKS â†” CloudWatch: \u0026lt; 50ms (Logs Endpoint) No Internet dependency for AWS services Cost Optimized: ~$7.2/month savings (no need for SageMaker VPC Endpoint)\n3. Application Load Balancer Setup 3.1. Create Application Load Balancer Navigate to Load Balancers:\nEC2 Dashboard â†’ Load Balancers â†’ \u0026ldquo;Create load balancer\u0026rdquo; ALB Configuration:\nName: mlops-hybrid-api-demo-alb\rScheme: Internet-facing\rIP address type: IPv4\rVPC: mlops-retail-forecast-hybrid-vpc\rMappings: Both public ALB subnets\rSecurity groups: mlops-hybrid-alb-sg 5.2 Create Target Group (for EKS later) EC2 â†’ Target Groups â†’ Create\nTarget type: IP addresses\rName: mlops-hybrid-eks-api-tg\rProtocol: HTTP\rPort: 80\rVPC: mlops-retail-forecast-hybrid-vpc\rHealth check path: /health\rSuccess codes: 200 5.3 Listener HTTP 80 â†’ (optional) redirect to HTTPS 443 HTTPS 443 â†’ forward to target group (needs ACM cert if real TLS) ğŸ’¡ SSL Certificate Options:\nProduction: Use AWS Certificate Manager (ACM) with domain Demo: Create self-signed certificate or use HTTP only Development: Skip SSL, use HTTP listener only 6) CLI Verification (CloudShell-friendly) 6.1 Verify VPC and Subnets # Enable VPC Flow Logs for security monitoring aws ec2 create-flow-logs \\ --resource-type VPC \\ --resource-ids vpc-xxxxxxxxx \\ --traffic-type ALL \\ --log-destination-type cloud-watch-logs \\ --log-group-name VPCFlowLogs \\ --deliver-logs-permission-arn arn:aws:iam::ACCOUNT:role/flowlogsRole 4.2. Network Performance Testing Test EKS â†” S3 Latency:\n# From EKS Pod (after cluster setup) kubectl run test-pod --image=amazonlinux --restart=Never -- sleep 3600 kubectl exec -it test-pod -- bash # Inside pod yum install -y awscli time aws s3 ls s3://your-model-bucket/ # Expected: \u0026lt; 50ms for VPC Endpoint Test ALB â†” EKS Connectivity:\n# Test from outside VPC curl -I http://your-alb-dns-name/health # Expected: HTTP 200 OK when EKS API is running 4.3. Cost Monitoring Setup CloudWatch Cost Alarms:\n# Create cost alarm for VPC Endpoints aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;VPC-Endpoints-Cost-Alert\u0026#34; \\ --alarm-description \u0026#34;Alert when VPC Endpoints cost \u0026gt; $30/month\u0026#34; \\ --metric-name EstimatedCharges \\ --namespace AWS/Billing \\ --statistic Maximum \\ --period 86400 \\ --threshold 30 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=Currency,Value=USD Name=ServiceName,Value=AmazonVPC 5. Terraform Outputs ğŸ’¡ When to use Terraform outputs:\nâœ… Task 7-10 will use Terraform (EKS cluster, ALB controller) âœ… Need automated deployment across environments âœ… Want to reference infrastructure programmatically If Task 7-10 use Console: Skip this section completely!\n5.1. Data Sources (Reference Console-created Resources) File: aws/infra/vpc-data-sources.tf\n# Reference VPC infrastructure from Console data \u0026#34;aws_vpc\u0026#34; \u0026#34;hybrid\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;mlops-retail-forecast-hybrid-vpc\u0026#34;] } } # Public subnets (ALB) data \u0026#34;aws_subnets\u0026#34; \u0026#34;public_alb\u0026#34; { filter { name = \u0026#34;vpc-id\u0026#34; values = [data.aws_vpc.hybrid.id] } filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;*public-alb*\u0026#34;] } } # Private subnets (EKS + SageMaker) data \u0026#34;aws_subnets\u0026#34; \u0026#34;private_workloads\u0026#34; { filter { name = \u0026#34;vpc-id\u0026#34; values = [data.aws_vpc.hybrid.id] } filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;*private-workloads*\u0026#34;] } } # Security Groups data \u0026#34;aws_security_group\u0026#34; \u0026#34;alb\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;mlops-hybrid-alb-sg\u0026#34;] } } data \u0026#34;aws_security_group\u0026#34; \u0026#34;eks_nodes\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;mlops-hybrid-eks-nodes-sg\u0026#34;] } } data \u0026#34;aws_security_group\u0026#34; \u0026#34;eks_control_plane\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;mlops-hybrid-eks-control-plane-sg\u0026#34;] } } # ALB data \u0026#34;aws_lb\u0026#34; \u0026#34;api_demo\u0026#34; { name = \u0026#34;mlops-hybrid-api-demo-alb\u0026#34; } data \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;eks_api\u0026#34; { name = \u0026#34;mlops-hybrid-eks-api-tg\u0026#34; } # VPC Endpoints data \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;s3\u0026#34; { vpc_id = data.aws_vpc.hybrid.id service_name = \u0026#34;com.amazonaws.ap-southeast-1.s3\u0026#34; } data \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;ecr_api\u0026#34; { vpc_id = data.aws_vpc.hybrid.id service_name = \u0026#34;com.amazonaws.ap-southeast-1.ecr.api\u0026#34; } data \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;ecr_dkr\u0026#34; { vpc_id = data.aws_vpc.hybrid.id service_name = \u0026#34;com.amazonaws.ap-southeast-1.ecr.dkr\u0026#34; } data \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;sagemaker_runtime\u0026#34; { vpc_id = data.aws_vpc.hybrid.id service_name = \u0026#34;com.amazonaws.ap-southeast-1.sagemaker.runtime\u0026#34; } 5.2. Outputs for EKS/ALB Integration File: aws/infra/networking-outputs.tf\n# VPC Information output \u0026#34;vpc_id\u0026#34; { description = \u0026#34;Hybrid VPC ID\u0026#34; value = data.aws_vpc.hybrid.id } output \u0026#34;vpc_cidr_block\u0026#34; { description = \u0026#34;VPC CIDR block\u0026#34; value = data.aws_vpc.hybrid.cidr_block } # Subnet Information output \u0026#34;public_alb_subnet_ids\u0026#34; { description = \u0026#34;Public subnet IDs for ALB\u0026#34; value = data.aws_subnets.public_alb.ids } output \u0026#34;private_workload_subnet_ids\u0026#34; { description = \u0026#34;Private subnet IDs for EKS and SageMaker\u0026#34; value = data.aws_subnets.private_workloads.ids } # Security Group Information output \u0026#34;alb_security_group_id\u0026#34; { description = \u0026#34;ALB Security Group ID\u0026#34; value = data.aws_security_group.alb.id } output \u0026#34;eks_nodes_security_group_id\u0026#34; { description = \u0026#34;EKS Nodes Security Group ID\u0026#34; value = data.aws_security_group.eks_nodes.id } output \u0026#34;eks_control_plane_security_group_id\u0026#34; { description = \u0026#34;EKS Control Plane Security Group ID\u0026#34; value = data.aws_security_group.eks_control_plane.id } # ALB Information output \u0026#34;alb_arn\u0026#34; { description = \u0026#34;ALB ARN for API demo\u0026#34; value = data.aws_lb.api_demo.arn } output \u0026#34;alb_dns_name\u0026#34; { description = \u0026#34;ALB DNS name for public API access\u0026#34; value = data.aws_lb.api_demo.dns_name } output \u0026#34;alb_zone_id\u0026#34; { description = \u0026#34;ALB Zone ID for Route53 integration\u0026#34; value = data.aws_lb.api_demo.zone_id } output \u0026#34;eks_api_target_group_arn\u0026#34; { description = \u0026#34;Target group ARN for EKS API pods\u0026#34; value = data.aws_lb_target_group.eks_api.arn } # VPC Endpoints Information output \u0026#34;s3_vpc_endpoint_id\u0026#34; { description = \u0026#34;S3 VPC Endpoint ID\u0026#34; value = data.aws_vpc_endpoint.s3.id } output \u0026#34;ecr_api_vpc_endpoint_id\u0026#34; { description = \u0026#34;ECR API VPC Endpoint ID\u0026#34; value = data.aws_vpc_endpoint.ecr_api.id } output \u0026#34;sagemaker_runtime_vpc_endpoint_id\u0026#34; { description = \u0026#34;SageMaker Runtime VPC Endpoint ID\u0026#34; value = data.aws_vpc_endpoint.sagemaker_runtime.id } # Demo Configuration output \u0026#34;api_demo_config\u0026#34; { description = \u0026#34;Configuration for API demo\u0026#34; value = { public_endpoint = \u0026#34;https://${data.aws_lb.api_demo.dns_name}\u0026#34; health_check = \u0026#34;https://${data.aws_lb.api_demo.dns_name}/health\u0026#34; predict_endpoint = \u0026#34;https://${data.aws_lb.api_demo.dns_name}/predict\u0026#34; } } 5.3. Deploy Terraform Outputs # Navigate to infrastructure directory cd aws/infra # Initialize Terraform terraform init # Plan (should show 0 resources to create, only data sources) terraform plan # Apply outputs terraform apply -auto-approve # Verify outputs terraform output api_demo_config Expected output:\n{ \u0026#34;health_check\u0026#34; = \u0026#34;https://mlops-hybrid-api-demo-alb-123456789.ap-southeast-1.elb.amazonaws.com/health\u0026#34; \u0026#34;predict_endpoint\u0026#34; = \u0026#34;https://mlops-hybrid-api-demo-alb-123456789.ap-southeast-1.elb.amazonaws.com/predict\u0026#34; \u0026#34;public_endpoint\u0026#34; = \u0026#34;https://mlops-hybrid-api-demo-alb-123456789.ap-southeast-1.elb.amazonaws.com\u0026#34; } 6. Verification \u0026amp; Performance Testing 6.1. Network Architecture Verification Verify Hybrid VPC Setup:\n# Check VPC configuration aws ec2 describe-vpcs \\ --filters \u0026#34;Name=tag:Name,Values=mlops-retail-forecast-hybrid-vpc\u0026#34; \\ --query \u0026#39;Vpcs[0].{VpcId:VpcId,Cidr:CidrBlock,State:State}\u0026#39; --output table aws ec2 describe-subnets \\ --filters \u0026#34;Name=vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; \\ --query \u0026#39;Subnets[*].{SubnetId:SubnetId,AZ:AvailabilityZone,CIDR:CidrBlock,MapPublicIp:MapPublicIpOnLaunch}\u0026#39; \\ --output table 6.2 Verify Route Tables aws ec2 describe-route-tables \\ --filters \u0026#34;Name=vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; \\ --query \u0026#39;RouteTables[*].{RT:RouteTableId,Routes:Routes}\u0026#39; \\ --output json 6.3 Verify Endpoints aws ec2 describe-vpc-endpoints \\ --filters \u0026#34;Name=vpc-id,Values=$(terraform output -raw vpc_id)\u0026#34; \\ --query \u0026#39;VpcEndpoints[*].{Service:ServiceName,PrivateDnsEnabled:PrivateDnsEnabled}\u0026#39; 7.2. ALB Issues Problem: ALB not accessible from Internet\n# Check Internet Gateway attached aws ec2 describe-internet-gateways \\ --filters \u0026#34;Name=attachment.vpc-id,Values=$(terraform output -raw vpc_id)\u0026#34; # Check public subnet route table aws ec2 describe-route-tables \\ --filters \u0026#34;Name=tag:Name,Values=mlops-hybrid-public-alb-rt\u0026#34; \\ --query \u0026#39;RouteTables[0].Routes\u0026#39; # Should have 0.0.0.0/0 â†’ Internet Gateway Problem: ALB can\u0026rsquo;t reach EKS targets\n# Check security group rules aws ec2 describe-security-groups \\ --group-names mlops-hybrid-eks-nodes-sg \\ --query \u0026#39;SecurityGroups[0].IpPermissions[?IpProtocol==`tcp` \u0026amp;\u0026amp; FromPort==`80`]\u0026#39; # Should allow port 80 from ALB security group 7.3. Performance Issues Problem: High latency EKS â†” S3\n# Verify using VPC endpoint kubectl exec -it network-test -- traceroute s3.ap-southeast-1.amazonaws.com # Should not go through Internet (no public IPs in trace) # Check VPC endpoint policy aws ec2 describe-vpc-endpoints \\ --filters \u0026#34;Name=service-name,Values=com.amazonaws.ap-southeast-1.s3\u0026#34; \\ --query \u0026#39;VpcEndpoints[0].PolicyDocument\u0026#39; ğŸ‘‰ Task 5 Results âœ… Hybrid VPC Architecture - Public ALB + Private workloads security model\nâœ… Public API Demo Ready - ALB configured for demo endpoint /predict\nâœ… High-Performance Internal - VPC Endpoints \u0026lt; 50ms latency EKS â†” S3\nâœ… Cost Optimized - $21.6/month base + $0.02/hour ALB demo usage\nâœ… Production Security - Layered security groups, no Internet access for workloads\nArchitecture Delivered âœ… Hybrid VPC Foundation:\r- Public Subnets: ALB demo access (Internet facing)\r- Private Subnets: EKS + SageMaker (secure, no Internet)\r- Multi-AZ: High availability for API demo\râœ… Public Demo Capability:\r- ALB: Public endpoint for API demo\r- Target Groups: Ready for EKS API pods\r- SSL/Health checks: Production-ready demo\râœ… High-Performance Internal Network:\r- S3 Gateway Endpoint: FREE, \u0026lt; 50ms model access\r- ECR Interface Endpoints: Fast container pulls\r- SageMaker Runtime Endpoint: Low-latency inference\râœ… Security Architecture:\rInternet â†’ ALB SG â†’ EKS SG â†’ VPC Endpoints SG\r(Layered access control) ğŸ¯ Task 5 Complete - Demo-Ready Hybrid VPC!\nPublic Access: ALB provides secure public API demo endpoint\nPrivate Security: EKS/SageMaker workloads completely isolated\nHigh Performance: \u0026lt; 50ms internal AWS services latency\nCost Efficient: $21.6 base + demo usage only when needed\nProduction Ready: SSL, health checks, multi-AZ availability\nğŸš€ Next Steps:\nTask 6: ECR container registry for API container images Task 7: EKS cluster deployment in private subnets Task 8: EKS node groups with auto-scaling Task 10: Deploy API service with ALB integration Task 11: ALB ingress controller configuration Demo Commands Ready:\n# Public API demo endpoint (after deployment) curl https://your-alb-dns/predict -d \u0026#39;{\u0026#34;data\u0026#34;: \u0026#34;your-input\u0026#34;}\u0026#39; # Health check endpoint curl https://your-alb-dns/health ğŸ“Š Performance Benchmarks Achieved:\nEKS â†” S3 Latency: \u0026lt; 50ms (VPC Gateway Endpoint) EKS â†” ECR Latency: \u0026lt; 100ms (Interface Endpoints) ALB â†” EKS Latency: \u0026lt; 10ms (same VPC) Internet â†” ALB: Standard Internet latency Cost: $21.6/month base + $0.02/hour demo usage Availability: Multi-AZ (99.99% SLA) Next Step: Task 6: ECR Container Registry\nğŸš€ Next Steps:\nTask 3: IAM Roles \u0026amp; IRSA using VPC infrastructure Task 4: EKS cluster deployment with VPC Endpoints integration Task 5: EKS managed node groups in cost-optimized private subnets 8. Clean Up Resources (AWS CLI) 8.1. Delete Application Load Balancer and Target Groups # List ALB aws elbv2 describe-load-balancers --names mlops-hybrid-api-demo-alb --query \u0026#39;LoadBalancers[*].[LoadBalancerArn,DNSName]\u0026#39; --output table # Delete ALB (automatically deletes listeners) aws elbv2 delete-load-balancer --load-balancer-arn \u0026lt;alb-arn\u0026gt; # Delete Target Groups aws elbv2 describe-target-groups --names mlops-hybrid-eks-api-tg --query \u0026#39;TargetGroups[*].TargetGroupArn\u0026#39; --output text | xargs -I {} aws elbv2 delete-target-group --target-group-arn {} 8.2. Delete VPC Endpoints # List VPC Endpoints aws ec2 describe-vpc-endpoints --filters \u0026#34;Name=vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; --query \u0026#39;VpcEndpoints[*].[VpcEndpointId,ServiceName]\u0026#39; --output table # Delete Interface Endpoints (ECR, CloudWatch Logs) aws ec2 delete-vpc-endpoints --vpc-endpoint-ids \u0026lt;ecr-api-endpoint-id\u0026gt; \u0026lt;ecr-dkr-endpoint-id\u0026gt; \u0026lt;logs-endpoint-id\u0026gt; # Delete Gateway Endpoint (S3) aws ec2 delete-vpc-endpoints --vpc-endpoint-ids \u0026lt;s3-gateway-endpoint-id\u0026gt; 8.3. Delete Security Groups # List Security Groups (except default) aws ec2 describe-security-groups --filters \u0026#34;Name=vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; --query \u0026#39;SecurityGroups[?GroupName!=`default`].[GroupId,GroupName]\u0026#39; --output table # Delete Security Groups (in reverse dependency order) aws ec2 delete-security-group --group-id \u0026lt;vpc-endpoints-sg-id\u0026gt; aws ec2 delete-security-group --group-id \u0026lt;eks-control-plane-sg-id\u0026gt; aws ec2 delete-security-group --group-id \u0026lt;eks-nodes-sg-id\u0026gt; aws ec2 delete-security-group --group-id \u0026lt;alb-sg-id\u0026gt; 8.4. Delete Subnets and Route Tables # List Route Tables (except main) aws ec2 describe-route-tables --filters \u0026#34;Name=vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; --query \u0026#39;RouteTables[?Associations[0].Main!=`true`].[RouteTableId,Tags[0].Value]\u0026#39; --output table # Delete Route Tables aws ec2 delete-route-table --route-table-id \u0026lt;public-rt-id\u0026gt; aws ec2 delete-route-table --route-table-id \u0026lt;private-rt-id\u0026gt; # List Subnets aws ec2 describe-subnets --filters \u0026#34;Name=vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; --query \u0026#39;Subnets[*].[SubnetId,Tags[0].Value,CidrBlock]\u0026#39; --output table # Delete Subnets aws ec2 delete-subnet --subnet-id \u0026lt;public-subnet-1a-id\u0026gt; aws ec2 delete-subnet --subnet-id \u0026lt;public-subnet-1b-id\u0026gt; aws ec2 delete-subnet --subnet-id \u0026lt;private-subnet-1a-id\u0026gt; aws ec2 delete-subnet --subnet-id \u0026lt;private-subnet-1b-id\u0026gt; 8.5. Delete Internet Gateway and VPC # Detach and delete Internet Gateway aws ec2 describe-internet-gateways --filters \u0026#34;Name=attachment.vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; --query \u0026#39;InternetGateways[*].InternetGatewayId\u0026#39; --output text | xargs -I {} aws ec2 detach-internet-gateway --internet-gateway-id {} --vpc-id \u0026lt;vpc-id\u0026gt; aws ec2 delete-internet-gateway --internet-gateway-id \u0026lt;igw-id\u0026gt; # Delete VPC (last) aws ec2 delete-vpc --vpc-id \u0026lt;vpc-id\u0026gt; # Verify clean up aws ec2 describe-vpcs --vpc-ids \u0026lt;vpc-id\u0026gt; 8.6. Clean Up Helper Script #!/bin/bash # vpc-cleanup.sh VPC_ID=\u0026#34;vpc-xxxxxxxxx\u0026#34; # Replace with actual VPC ID echo \u0026#34;ğŸ§¹ Cleaning up VPC resources for $VPC_ID...\u0026#34; # 1. Delete ALB and Target Groups echo \u0026#34;Deleting ALB...\u0026#34; ALB_ARN=$(aws elbv2 describe-load-balancers --names mlops-hybrid-api-demo-alb --query \u0026#39;LoadBalancers[0].LoadBalancerArn\u0026#39; --output text 2\u0026gt;/dev/null) if [ \u0026#34;$ALB_ARN\u0026#34; != \u0026#34;None\u0026#34; ] \u0026amp;\u0026amp; [ ! -z \u0026#34;$ALB_ARN\u0026#34; ]; then aws elbv2 delete-load-balancer --load-balancer-arn $ALB_ARN echo \u0026#34;ALB deleted: $ALB_ARN\u0026#34; fi # 2. Delete VPC Endpoints echo \u0026#34;Deleting VPC Endpoints...\u0026#34; ENDPOINTS=$(aws ec2 describe-vpc-endpoints --filters \u0026#34;Name=vpc-id,Values=$VPC_ID\u0026#34; --query \u0026#39;VpcEndpoints[*].VpcEndpointId\u0026#39; --output text) for endpoint in $ENDPOINTS; do aws ec2 delete-vpc-endpoints --vpc-endpoint-ids $endpoint echo \u0026#34;VPC Endpoint deleted: $endpoint\u0026#34; done # 3. Wait for resources to be deleted echo \u0026#34;Waiting for resources to be deleted...\u0026#34; sleep 60 # 4. Delete Security Groups echo \u0026#34;Deleting Security Groups...\u0026#34; SECURITY_GROUPS=$(aws ec2 describe-security-groups --filters \u0026#34;Name=vpc-id,Values=$VPC_ID\u0026#34; --query \u0026#39;SecurityGroups[?GroupName!=`default`].GroupId\u0026#39; --output text) for sg in $SECURITY_GROUPS; do aws ec2 delete-security-group --group-id $sg 2\u0026gt;/dev/null echo \u0026#34;Security Group deleted: $sg\u0026#34; done echo \u0026#34;âœ… VPC cleanup completed for $VPC_ID\u0026#34; 9. VPC and Networking Pricing Table (ap-southeast-1) 9.1. VPC Core Components Cost Component Price (USD) Notes VPC Free Unlimited VPCs Subnets Free Unlimited subnets Route Tables Free Routing configuration Internet Gateway Free One per VPC Security Groups Free Firewall rules 9.2. VPC Endpoints Cost Endpoint Type Price (USD/hour) Price (USD/month) Data Transfer Gateway Endpoint (S3) Free Free Free Interface Endpoint $0.01 $7.2 $0.01/GB PrivateLink Endpoint $0.01 $7.2 $0.01/GB VPC Endpoints cost for Task 5:\nS3 Gateway: Free ECR API Interface: $7.2/month ECR DKR Interface: $7.2/month CloudWatch Logs Interface: $7.2/month Total: $21.6/month 9.3. Application Load Balancer Cost Component Price (USD/hour) Price (USD/month) Notes ALB Fixed Cost $0.0225 $16.2 Always running LCU (Load Balancer Capacity Unit) $0.008 $5.76 Per LCU-hour Rule Evaluations $0.008 $5.76 Per million requests Estimated ALB cost:\nBase ALB: $16.2/month 1 LCU (basic usage): $5.76/month Total ALB: ~$22/month continuous 9.4. NAT Gateway Cost (Not used in Task 5) Component Price (USD/hour) Price (USD/month) Data Transfer NAT Gateway $0.045 $32.4 $0.045/GB Data Processing $0.045/GB Savings: $32.4/month by using VPC Endpoints instead of NAT Gateway\n9.5. Data Transfer Pricing Transfer Type Price (USD/GB) Notes VPC Internal Free Same AZ Cross-AZ $0.01 Different AZ in region VPC Endpoints $0.01 Interface endpoints Internet OUT $0.12 First 1GB free/month S3 Transfer Free Via Gateway endpoint 9.6. Estimated Total Cost for Task 5 Monthly Baseline Cost:\nComponent Monthly Cost Purpose VPC + Subnets + IGW $0 Core networking VPC Endpoints (3x Interface) $21.6 ECR + CloudWatch S3 Gateway Endpoint $0 Model access Subtotal $21.6 Always running Demo Usage Cost:\nUsage Pattern ALB Cost Total Cost Use Case Development (8h/day) $5.4/month $27/month Daily development Demo only (3h/day) $2.0/month $23.6/month Presentation demos Production (24/7) $22/month $43.6/month Live production Testing (1h/day) $0.7/month $22.3/month Occasional testing 9.7. Cost Comparison with Traditional Setup Task 5 (VPC Endpoints) vs Traditional (NAT Gateway):\nArchitecture Monthly Cost Performance Security VPC Endpoints $21.6 \u0026lt; 50ms latency Private network NAT Gateway $32.4 + data Variable Internet routing Savings -$10.8 Better Higher 9.8. Cost Optimization Tips Immediate Savings:\nâœ… Use S3 Gateway Endpoint (Free instead of $7.2/month Interface) âœ… Skip NAT Gateway (-$32.4/month) âœ… Turn off ALB when not demoing (-$22/month) Long-term Optimization:\nUse Spot instances for EKS nodes (60-70% savings) S3 Intelligent Tiering for model storage CloudWatch Logs retention policy (7-30 days) Demo Cost Management:\n# Enable ALB only when demo aws elbv2 create-load-balancer --name demo-alb --type application # Disable ALB after demo aws elbv2 delete-load-balancer --load-balancer-arn \u0026lt;arn\u0026gt; ğŸ’° Cost Summary for Task 5:\nBaseline: $21.6/month (VPC Endpoints, always on) Demo usage: $0.02/hour ALB (only when needed) Savings: $10.8/month compared to NAT Gateway approach Performance: \u0026lt; 50ms internal latency guaranteed Console-created resources ready for subsequent tasks:\nVPC ID, subnet IDs for EKS cluster creation Security Group IDs for EKS and ALB configuration VPC Endpoint IDs for cost-optimized AWS services access ğŸ“¹ Task 5 Implementation Video Next Step: Task 06: ECR Registry\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.4-week4/",
	"title": "Week 4 - Storage Service on AWS",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Explore advanced storage options: S3, EFS, and EBS. Implement content delivery with CloudFront. Tasks to be carried out this week: Day Task Reference 2 - S3 Advanced: Configure Versioning, Lifecycle policies, and Encryption.\n- Setup Pre-signed URLs for secure uploads. AWS S3 Docs 3 - CDN: Setup CloudFront with Origin Access Control (OAC) for S3.\n- Test caching and invalidation. AWS CloudFront 4 - File Storage: Create and mount Amazon EFS to EC2 instances.\n- Configure Access Points and Mount Targets. AWS EFS Docs 5 - Block Storage: Manage EBS Volumes (Snapshot, Restore, Resize).\n- Implement Backup plans. AWS Backup 6 - Review: Validate secure content delivery and storage persistence.\n- Weekly Report. - Week 4 Achievements: Served static content securely via CloudFront + S3 OAC. Implemented shared storage using EFS for multiple instances. Automated EBS backups using Data Lifecycle Manager. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.6-blog6/",
	"title": "Há»£p lÃ½ hÃ³a sá»± há»£p tÃ¡c giá»¯a cÃ¡c Sá»Ÿ Giao thÃ´ng Váº­n táº£i vá»›i Private Blockchain",
	"tags": [],
	"description": "",
	"content": "BÃ i viáº¿t gá»‘c Ä‘Æ°á»£c Ä‘Äƒng vÃ o ngÃ y 19 thÃ¡ng 9 nÄƒm 2024\nTheo Cá»¥c Thá»‘ng kÃª Hoa Ká»³, hÆ¡n 7,9 triá»‡u ngÆ°á»i Má»¹ Ä‘Ã£ chuyá»ƒn tá»« bang nÃ y sang bang khÃ¡c chá»‰ trong nÄƒm 2021. Má»™t trong nhá»¯ng nhiá»‡m vá»¥ mÃ  cÃ¡ nhÃ¢n pháº£i hoÃ n thÃ nh khi chuyá»ƒn tá»« bang nÃ y sang bang khÃ¡c lÃ  Ä‘á»•i giáº¥y phÃ©p lÃ¡i xe tá»« bang cÆ° trÃº cÅ© Ä‘á»ƒ láº¥y giáº¥y phÃ©p á»Ÿ bang cÆ° trÃº má»›i. Sá»Ÿ Giao thÃ´ng Váº­n táº£i (DMV) cá»§a má»—i bang chá»‹u trÃ¡ch nhiá»‡m cáº¥p vÃ  quáº£n lÃ½ giáº¥y phÃ©p lÃ¡i xe trong bang Ä‘Ã³, vÃ  Ä‘iá»u nÃ y Ä‘Ã²i há»i sá»± há»£p tÃ¡c vá»›i cÃ¡c DMV bang khÃ¡c Ä‘á»ƒ thu tháº­p dá»¯ liá»‡u quan trá»ng nhÆ° vi pháº¡m giao thÃ´ng xáº£y ra ngoÃ i bang.\nTrong bÃ i viáº¿t nÃ y, chÃºng tÃ´i tháº£o luáº­n vá» cÃ¡ch blockchain cÃ³ thá»ƒ há»£p lÃ½ hÃ³a viá»‡c cáº¥p giáº¥y phÃ©p lÃ¡i xe vÃ  thÃºc Ä‘áº©y sá»± há»£p tÃ¡c sÃ¢u sáº¯c hÆ¡n giá»¯a cÃ¡c DMV á»Ÿ táº¥t cáº£ 50 bang vÃ  lÃ½ do táº¡i sao blockchain lÃ  má»™t lá»±a chá»n cÃ´ng nghá»‡ háº¥p dáº«n.\nCáº£i thiá»‡n Há»£p Ä‘á»“ng Giáº¥y phÃ©p LÃ¡i xe Hiá»‡n táº¡i, cÃ³ cÃ¡c cÆ¡ cháº¿ Ä‘á»ƒ táº¡o Ä‘iá»u kiá»‡n há»£p tÃ¡c vÃ  chia sáº» dá»¯ liá»‡u giá»¯a cÃ¡c tá»• chá»©c DMV cá»§a bang, cháº³ng háº¡n nhÆ° Thá»a thuáº­n Giáº¥y phÃ©p LÃ¡i xe (DLC) vÃ  Thá»a thuáº­n KhÃ´ng vi pháº¡m CÆ° dÃ¢n (NR). CÃ¡c chÆ°Æ¡ng trÃ¬nh nÃ y cung cáº¥p khung phÃ¡p lÃ½ cho cÃ¡c bang Ä‘á»ƒ há»£p tÃ¡c vÃ  chia sáº» dá»¯ liá»‡u liÃªn quan Ä‘áº¿n viá»‡c cáº¥p giáº¥y phÃ©p lÃ¡i xe vÃ  cÃ³ tÃ­nh tÆ°Æ¡ng há»—, thÃ´ng tin vi pháº¡m giao thÃ´ng, vÃ  nhiá»u hÆ¡n ná»¯a. Tuy nhiÃªn, cÃ¡c chÆ°Æ¡ng trÃ¬nh nÃ y khÃ´ng cÃ³ sá»± cháº¥p nháº­n hoáº·c cam káº¿t thÃ nh viÃªn tá»« táº¥t cáº£ 50 bang.\nSá»­ dá»¥ng cÃ´ng nghá»‡ blockchain, mÃ´ hÃ¬nh hiá»‡n táº¡i cho tÃ­nh tÆ°Æ¡ng há»— giáº¥y phÃ©p lÃ¡i xe, chia sáº» há»“ sÆ¡ lÃ¡i xe vÃ  cáº¥p giáº¥y phÃ©p qua cÃ¡c bang cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n hÆ¡n ná»¯a, vÃ  giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c trong viá»‡c thu Ä‘Æ°á»£c sá»± cháº¥p nháº­n tá»« cÃ¡c DMV bang. Blockchain cho phÃ©p há»£p tÃ¡c sÃ¢u sáº¯c hÆ¡n giá»¯a cÃ¡c bang trá»±c tiáº¿p mÃ  khÃ´ng yÃªu cáº§u tÃ­ch há»£p má»™t-má»™t hoáº·c má»™t Ä‘iá»u phá»‘i viÃªn trung tÃ¢m Ä‘á»ƒ xÃ¢y dá»±ng giáº£i phÃ¡p thá»‘ng nháº¥t DMV cá»§a má»—i bang. Viá»‡c xÃ¢y dá»±ng máº¡ng lÆ°á»›i chia sáº» chung giá»¯a cÃ¡c DMV bang sáº½ trá»Ÿ thÃ nh má»™t ná»n táº£ng mÃ  trÃªn Ä‘Ã³ cÃ¡c tÃ­nh nÄƒng vÃ  tiá»‡n Ã­ch bá»• sung cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¢y dá»±ng, cháº³ng háº¡n nhÆ° giáº¥y phÃ©p lÃ¡i xe ká»¹ thuáº­t sá»‘ cung cáº¥p tÃ­nh tÆ°Æ¡ng há»— trÃªn táº¥t cáº£ 50 bang.\nTáº¡i sao blockchain? HÃ£y báº¯t Ä‘áº§u báº±ng cÃ¡ch Ä‘Æ°a ra cÃ¡i nhÃ¬n tá»•ng quan ngáº¯n gá»n vá» blockchain lÃ  gÃ¬. Blockchain lÃ  má»™t sá»• cÃ¡i báº¥t biáº¿n Ä‘Æ°á»£c chia sáº» giá»¯a má»™t nhÃ³m phÃ¢n tÃ¡n cÃ¡c thÃ nh viÃªn nÆ¡i cÃ¡c cáº­p nháº­t cho sá»• cÃ¡i - Ä‘Æ°á»£c ghi lÃ  cÃ¡c giao dá»‹ch - pháº£i Ä‘Æ°á»£c thá»‘ng nháº¥t thÃ´ng qua sá»± Ä‘á»“ng thuáº­n máº¡ng lÆ°á»›i.\nSá»• cÃ¡i lÃ  cÆ¡ cháº¿ lÆ°u trá»¯ cÆ¡ báº£n hoáº·c cÆ¡ sá»Ÿ dá»¯ liá»‡u trong blockchain. Khi báº¡n nghÄ© vá» má»™t sá»• cÃ¡i, loáº¡i sá»• cÃ¡i Ä‘áº§u tiÃªn cÃ³ thá»ƒ náº£y ra trong Ä‘áº§u cÃ³ thá»ƒ lÃ  má»™t sá»• cÃ¡i káº¿ toÃ¡n. Trong má»™t sá»• cÃ¡i káº¿ toÃ¡n Ä‘iá»ƒn hÃ¬nh, báº¡n sáº½ lÆ°u giá»¯ cÃ¡c khoáº£n ghi ná»£ vÃ  tÃ­n dá»¥ng. Trong blockchain, cÃ¡c báº£n ghi nÃ y Ä‘Æ°á»£c gá»i lÃ  giao dá»‹ch. Thuáº­t ngá»¯ khÃ¡c cáº§n hiá»ƒu lÃ  sá»± Ä‘á»“ng thuáº­n máº¡ng lÆ°á»›i. Sá»± Ä‘á»“ng thuáº­n máº¡ng lÆ°á»›i Ä‘á» cáº­p Ä‘áº¿n giao thá»©c quy Ä‘á»‹nh cÃ¡ch cÃ¡c node(s) thuá»™c sá»Ÿ há»¯u cá»§a cÃ¡c thÃ nh viÃªn trong máº¡ng blockchain Ä‘á»“ng bá»™ vá»›i nhau vÃ  thá»‘ng nháº¥t vá»›i cÃ¡c bá»™ giao dá»‹ch ghi dá»¯ liá»‡u vÃ o sá»• cÃ¡i chia sáº». ThÃ nh pháº§n cuá»‘i cÃ¹ng báº¡n cáº§n hiá»ƒu lÃ  há»£p Ä‘á»“ng thÃ´ng minh. Há»£p Ä‘á»“ng thÃ´ng minh lÃ  cÃ¡c cÆ¡ cháº¿ Ä‘Æ°á»£c tÃ­ch há»£p sáºµn vÃ o báº¡n cÃ³ thá»ƒ biá»ƒu thá»‹ logic kinh doanh dÆ°á»›i dáº¡ng mÃ£ vá» cÃ¡ch dá»¯ liá»‡u Ä‘Æ°á»£c ghi vÃ  Ä‘á»c tá»« sá»• cÃ¡i.\nBÃ¢y giá» chÃºng ta Ä‘Ã£ giáº£i quyáº¿t chÃ­nh xÃ¡c blockchain lÃ  gÃ¬, hÃ£y nÃ³i vá» lÃ½ do táº¡i sao báº¡n muá»‘n sá»­ dá»¥ng blockchain Ä‘á»ƒ há»£p lÃ½ hÃ³a cÃ¡c quy trÃ¬nh cáº­p nháº­t giáº¥y phÃ©p cá»§a báº¡n khi chuyá»ƒn tá»« bang nÃ y sang bang khÃ¡c. CÃ´ng nghá»‡ Blockchain thÃºc Ä‘áº©y há»£p tÃ¡c giá»¯a cÃ¡c thá»±c thá»ƒ trong máº¡ng lÆ°á»›i phi táº­p trung, trong Ä‘Ã³ má»i thÃ nh viÃªn giá»¯ má»™t báº£n sao cá»§a sá»• cÃ¡i Ä‘Æ°á»£c cáº­p nháº­t báº±ng cÃ¡ch cÃ¡c giao dá»‹ch mÃ  táº¥t cáº£ cÃ¡c thÃ nh viÃªn Ä‘á»“ng Ã½ thÃ´ng qua sá»± Ä‘á»“ng thuáº­n máº¡ng lÆ°á»›i. Äiá»u nÃ y Ä‘áº£m báº£o ráº±ng má»i thÃ nh viÃªn hoáº·c node Ä‘á»u á»Ÿ trÃªn cÃ¹ng má»™t trang khi nÃ³i Ä‘áº¿n dá»¯ liá»‡u nÃ o Ä‘ang Ä‘Æ°á»£c lÆ°u trá»¯ trÃªn máº¡ng, vÃ  nÃ³ loáº¡i bá» nhu cáº§u xá»­ lÃ½ giáº¥y tá» hoáº·c cÃ¡c quy trÃ¬nh TT tÃ­ch há»£p. HÆ¡n ná»¯a, nÃ³ loáº¡i bá» nhu cáº§u trung gian hoáº·c cÆ¡ quan trung Æ°Æ¡ng, cÃ³ thá»ƒ giáº£m chi phÃ­ vÃ  tÄƒng hiá»‡u quáº£ báº±ng cÃ¡ch cho phÃ©p tÆ°Æ¡ng tÃ¡c trá»±c tiáº¿p ngang hÃ ng giá»¯a cÃ¡c bang trong quÃ¡ trÃ¬nh cáº¥p vÃ  xÃ¡c minh giáº¥y phÃ©p lÃ¡i xe. HÆ¡n ná»¯a, khÃ­a cáº¡nh há»£p Ä‘á»“ng thÃ´ng minh cá»§a cÃ¡c máº¡ng blockchain báº£n Ä‘á»‹a cho phÃ©p cÃ¡c bang má»Ÿ rá»™ng chá»©c nÄƒng cá»§a máº¡ng chia sáº» nÃ y theo thá»i gian mÃ  khÃ´ng cáº§n xÃ¢y dá»±ng cÃ¡c giáº£i phÃ¡p hoÃ n toÃ n má»›i tá»« Ä‘áº§u. TÃ­nh láº­p trÃ¬nh nÃ y trÃªn Ä‘áº§u cá»§a má»™t sá»• cÃ¡i chia sáº» chung sau Ä‘Ã³ táº¡o tiá»n Ä‘á» cho cÃ¡c á»©ng dá»¥ng nÃ¢ng cao hÆ¡n nhÆ° giáº¥y phÃ©p lÃ¡i xe ká»¹ thuáº­t sá»‘ cung cáº¥p tÃ­nh tÆ°Æ¡ng há»— trÃªn táº¥t cáº£ 50 bang trong tÆ°Æ¡ng lai.\nLá»±a chá»n giá»¯a blockchain cÃ´ng khai vÃ  riÃªng tÆ° BÃ¢y giá» chÃºng ta Ä‘Ã£ tháº£o luáº­n vá» lÃ½ do táº¡i sao báº¡n nÃªn chá»n blockchain, hÃ£y tháº£o luáº­n vá» cÃ¡c loáº¡i blockchain khÃ¡c nhau, chÃºng khÃ¡c nhau nhÆ° tháº¿ nÃ o vÃ  loáº¡i nÃ o phÃ¹ há»£p vá»›i nhu cáº§u cá»§a chÃºng ta tá»‘t nháº¥t. CÃ³ hai loáº¡i giao thá»©c blockchain: blockchain cÃ´ng khai vÃ  riÃªng tÆ°.\nBlockchain cÃ´ng khai lÃ  máº¡ng má»Ÿ, khÃ´ng cáº§n phÃ©p, cÃ³ nghÄ©a lÃ  báº¥t ká»³ ai cÅ©ng cÃ³ thá»ƒ tham gia vÃ  tÆ°Æ¡ng tÃ¡c vá»›i máº¡ng blockchain. Blockchain cÃ´ng khai thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng khi kháº£ nÄƒng xÃ¡c minh cÃ´ng khai vá» dá»¯ liá»‡u hoáº·c kháº£ nÄƒng truy cáº­p phi táº­p trung cá»§a má»™t á»©ng dá»¥ng lÃ  quan trá»ng, cháº³ng háº¡n nhÆ° trong tiá»n Ä‘iá»‡n tá»­ cho cÃ¡c nhÃ  cung cáº¥p dá»‹ch vá»¥ ká»¹ thuáº­t sá»‘. Máº·c dÃ¹ cÃ´ng nghá»‡ Ä‘áº±ng sau nhÆ° danh tÃ­nh phi táº­p trung cho tháº¥y nhiá»u há»©a háº¹n trong viá»‡c báº£o vá»‡ quyá»n riÃªng tÆ° trÃªn blockchain cÃ´ng khai, chÃºng váº«n cÃ²n má»›i. Máº·t khÃ¡c, blockchain riÃªng tÆ° lÃ  máº¡ng Ä‘Ã³ng, cÃ³ phÃ©p nÆ¡i cÃ¡c tá»• chá»©c tham gia Ä‘Æ°á»£c biáº¿t Ä‘áº¿n láº«n nhau trÆ°á»›c. CÃ¡c máº¡ng nÃ y cÃ³ xu hÆ°á»›ng cung cáº¥p cÃ¡c tÃ­nh nÄƒng quyá»n riÃªng tÆ° nÃ¢ng cao hÆ¡n vÃ  cÃ³ thá»ƒ há»— trá»£ thÃ´ng lÆ°á»£ng giao dá»‹ch cao hÆ¡n so vá»›i cÃ¡c thÃ nh viÃªn cá»§a blockchain cÃ´ng khai á»Ÿ chi phÃ­ cá»§a viá»‡c phi táº­p trung hÆ¡n. Blockchain riÃªng tÆ° cÃ³ thá»ƒ lÃ  má»™t lá»±a chá»n tá»‘t cho cÃ¡c doanh nghiá»‡p cáº§n Ä‘áº¡t Ä‘Æ°á»£c lá»£i Ã­ch thá»±c táº¿ cao hÆ¡n cá»§a viá»‡c phi táº­p trung nhÆ°ng khÃ´ng muá»‘n phÆ¡i bÃ y dá»¯ liá»‡u nháº¡y cáº£m cho cÃ´ng chÃºng.\nTrong HÃ¬nh 1 dÆ°á»›i Ä‘Ã¢y, chÃºng tÃ´i minh há»a cÃ¡ch cÃ¡c DMV cá»§a cÃ¡c bang khÃ¡c nhau cÃ³ thá»ƒ tÆ°Æ¡ng tÃ¡c trá»±c tiáº¿p vá»›i nhau trÃªn má»™t há»‡ thá»‘ng chung, chia sáº» há»“ sÆ¡ mÃ  khÃ´ng yÃªu cáº§u nhiá»u tÃ­ch há»£p 1:1 vá»›i nhau.\n[HÃ¬nh 1 minh há»a cÃ¡c DMV bang khÃ¡c nhau tÆ°Æ¡ng tÃ¡c trá»±c tiáº¿p]\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During the Retail MLOps on AWS workshop, I had the opportunity to apply Cloud/DevOps/MLOps knowledge to a practical end-to-end pipeline: from S3-based data organization, model training and evaluation on SageMaker, model versioning with SageMaker Model Registry, containerizing a FastAPI inference service, deploying to EKS, monitoring with CloudWatch, implementing CI/CD automation, and optimizing cost with teardown procedures after demos.\nThrough this workshop, I improved the following skills:\nDesigning a production-like MLOps architecture (networking, security, scalability) Building data-to-model pipelines with clear quality metrics and verification steps Working with AWS services: S3, SageMaker, ECR, EKS, IAM/IRSA, CloudWatch, VPC endpoints Containerizing inference services (multi-stage builds, non-root, healthcheck, image scanning) Deploying and operating Kubernetes workloads (manifests, Service/Ingress, HPA, debugging) Cost optimization and operational hygiene (Spot, lifecycle policies, log retention, scheduled start/stop, teardown scripts) To reflect objectively on my progress after completing the workshop, I self-assess using the criteria below:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding MLOps concepts, implementing training/deployment pipeline, output quality âœ… â˜ â˜ 2 Ability to learn Learning and applying AWS/EKS/SageMaker/ECR/CloudWatch quickly âœ… â˜ â˜ 3 Proactiveness Taking initiative in debugging (pods, image pull, IRSA), improving configs and cleanup âœ… â˜ â˜ 4 Sense of responsibility Delivering based on task checklist, meeting model metrics and verification requirements âœ… â˜ â˜ 5 Discipline Following naming conventions, region consistency, and teardown steps to avoid cost leakage â˜ âœ… â˜ 6 Progressive mindset Accepting feedback and iterating (Spot, lifecycle, log retention, scheduling) âœ… â˜ â˜ 7 Communication Explaining architecture choices, reporting progress, documenting verification steps â˜ âœ… â˜ 8 Teamwork (if applicable) Coordinating modules (data/training/deploy/monitor/cost) and integrating deliverables â˜ âœ… â˜ 9 Professional conduct Working systematically, respecting conventions, emphasizing least privilege and security âœ… â˜ â˜ 10 Problem-solving skills Identifying root causes and proposing fixes (VPC endpoints, IRSA/RBAC, LB/SG, logs) â˜ âœ… â˜ 11 Contribution to project/team Producing a reusable end-to-end pipeline plus operational/teardown scripts âœ… â˜ â˜ 12 Overall Overall completion level and ability to reproduce the system independently âœ… â˜ â˜ Needs Improvement Stronger documentation quality: clearer architecture diagrams, prerequisites, and a structured troubleshooting checklist. More robust CI/CD gates: add stronger quality checks (tests, policy checks, clear rollback strategy) and separate dev/stage/prod environments. Better observability maturity: dashboards/alerts aligned with real SLOs (latency, error rate), log sampling, and optional tracing. Deeper cost controls: automated schedules aligned with demo windows, stricter log ingestion control, and periodic ECR cleanup. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.5-week5/",
	"title": "Week 5 - Security Services on AWS",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Implement comprehensive security controls (IAM, KMS, WAF, Secrets). Enhance visibility with auditing tools (CloudTrail, Config, GuardDuty). Tasks to be carried out this week: Day Task Reference 2 - IAM \u0026amp; Secrets: Review IAM Roles (Least Privilege).\n- Use Secrets Manager/Parameter Store for credentials. AWS IAM/Secrets 3 - Encryption: Create Customer Managed Keys (CMK) in KMS.\n- Encrypt S3/EBS with KMS keys. AWS KMS Docs 4 - Edge Security: Deploy AWS WAF (Web Application Firewall) for ALB/CloudFront.\n- Test SQLi/XSS blocking rules. AWS WAF Docs 5 - Compliance: Enable AWS Config and GuardDuty.\n- Check Security Hub findings. AWS Security Hub 6 - Review: Conduct security audit and remediation.\n- Weekly Report. - Week 5 Achievements: Centralized secret management using Secrets Manager/SSM. Protected endpoints with AWS WAF rules against common attacks. Enabled continuous compliance monitoring with AWS Config. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/6-ecr-registry/",
	"title": "Amazon ECR Container Registry (MLOps)",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Task 6 Objectives: Set up Amazon Elastic Container Registry (ECR) for MLOps pipeline:\nCreate ECR Repository: Repository for API container Security Configuration: Image scanning, IAM policy, lifecycle rules Build \u0026amp; Push Image: Upload FastAPI container to ECR Manual Build \u0026amp; Push: Guide for build/push using script (CLI / PowerShell) ğŸ“¥ Input from Previous Tasks:\nTask 2 (IAM Roles \u0026amp; Audit): IAM roles, policies and permissions for ECR/EKS/S3 access Task 5 (Production VPC): VPC endpoints, networking and security groups to allow EKS pull images from ECR ğŸ“¦ Output:\nInference Container: server/ code â†’ FastAPI API serving predictions in EKS Overview Amazon ECR (Elastic Container Registry) is a fully managed Docker container registry service by AWS, deeply integrated with EKS and CI/CD pipeline. ECR provides secure storage, management, and deployment capabilities for container images in MLOps workflow.\n1. ECR Repositories Setup 1.1. Create ECR Repositories Navigate to ECR Console: Login to AWS Console Navigate to Amazon ECR service Region: ap-southeast-1 Select \u0026ldquo;Create repository\u0026rdquo; API Repository Configuration: Repository Created Successfully:\nAfter creating the repository, you will see the interface as shown below with information:\nRepository name: mlops/retail-api Repository URI: \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api Status: \u0026ldquo;No active images\u0026rdquo; (no images have been pushed yet) Tabs: Summary, Images, Permissions, Lifecycle policy, Repository tags Repository Setup Complete:\nAPI repository is ready for containerized FastAPI application.\nRepository Management Interface:\nIn the repository management interface, you can:\nImages tab: View list of images, filter by tags View push commands: Commands to push image to repository Copy URI: Copy repository URI for use Scan: Scan vulnerabilities for images Delete: Delete repository when not needed Tip: Enable tag immutability for production tags (e.g., v*) to avoid accidental overwrite. Use semantic tags (v1.2.3, commit-\u0026lt;sha\u0026gt;) to help with rollback and audit.\n1.2. Lifecycle Policy Setup API Repository Lifecycle Policy: Select repository mlops/retail-api Click tab \u0026ldquo;Lifecycle policy\u0026rdquo; Click \u0026ldquo;Create rule\u0026rdquo; to create lifecycle policy Configure API Lifecycle Rules:\nRule 1 - Keep Latest Production Images:\nRule priority: 1\rDescription: Keep latest 10 production images\rImage status: Tagged (wildcard matching)\rImage tag filters: v*\rMatch criteria:\r- Count type: imageCountMoreThan\r- Count number: 10\rAction: expire Rule 2 - Keep Latest Development Images:\nRule priority: 2\rDescription: Keep latest 5 development images\rImage status: Tagged (wildcard matching)\rImage tag filters: dev*, feature*, main*\rMatch criteria:\r- Count type: imageCountMoreThan\r- Count number: 5\rAction: expire Rule 3 - Remove Old Untagged Images:\nRule priority: 3\rDescription: Delete untagged images after 1 day\rImage status: Untagged\rMatch criteria:\r- Days since image created: 1\rAction: expire Training Repository Lifecycle Policy:\n1.3. Image Scanning \u0026amp; Push Commands Check Scan Settings:\nSelect repository from list Check \u0026ldquo;Scan on push\u0026rdquo; is enabled Review enhanced scanning options if needed View Push Commands:\nClick \u0026ldquo;View push commands\u0026rdquo; button in repository interface AWS will display commands to authenticate and push image Copy these commands for use from local machine or CI/CD pipeline ğŸ¯ ECR Repositories Setup Complete!\nCreated Repository:\nâœ… mlops/retail-api: FastAPI prediction service container âœ… Repository URI: \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api âœ… Private repository with tag immutability enabled âœ… Image scanning enabled on push âœ… Lifecycle policies configured for cost optimization âœ… Push commands available in console âœ… IAM access policies for EKS integration Tip: Document lifecycle rule priorities in team docs and test rules on non-prod repos before applying to production to avoid accidentally deleting images.\n2. API Containerization Workflow 2.1. Dockerfile Configuration Create server/Dockerfile - Multi-stage build:\n# ---- builder ---- FROM python:3.11-slim AS builder WORKDIR /app ENV PIP_DISABLE_PIP_VERSION_CHECK=1 PIP_NO_CACHE_DIR=1 PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1 RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends build-essential \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* COPY requirements.txt . RUN python -m venv /opt/venv \u0026amp;\u0026amp; /opt/venv/bin/pip install --upgrade pip \u0026amp;\u0026amp; /opt/venv/bin/pip install -r requirements.txt # ---- runtime ---- FROM python:3.11-slim AS runtime WORKDIR /app ENV PATH=\u0026#34;/opt/venv/bin:$PATH\u0026#34; PYTHONDONTWRITEBYTECODE=1 PYTHONUNBUFFERED=1 # Create non-root user RUN addgroup --system app \u0026amp;\u0026amp; adduser --system --ingroup app app COPY --from=builder /opt/venv /opt/venv COPY . . # Healthcheck endpoint should exist in your FastAPI app HEALTHCHECK --interval=30s --timeout=3s --start-period=20s --retries=3 CMD python -c \u0026#34;import urllib.request; urllib.request.urlopen(\u0026#39;http://127.0.0.1:8000/health\u0026#39;).read()\u0026#34; || exit 1 EXPOSE 8000 USER app # Uvicorn entrypoint CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] Create server/.dockerignore:\n__pycache__/\r*.pyc\r*.pyo\r*.pyd\r*.log\r.env\r.venv/\rvenv/\rtests/\rdist/\rbuild/\r.git/\r.github/\r*.ipynb 4) Authenticate and push to ECR 4.1 Bash (Linux/macOS) script Create scripts/ecr_push.sh:\n# Navigate to server directory cd retail-price-sensitivity-prediction/server # Build Docker image docker build -t mlops/retail-api:latest . # Test locally docker run -d --name test -p 8000:8000 mlops/retail-api:latest curl http://localhost:8000/health docker stop test \u0026amp;\u0026amp; docker rm test Warning: Docker login tokens (ECR auth) have expiration; CI agents should refresh token (aws ecr get-login-password) per job. Avoid hardcoding credentials in scripts or environment files.\n2.3. View Push Commands from AWS Console In ECR Console:\nSelect repository mlops/retail-api Click \u0026ldquo;View push commands\u0026rdquo; button AWS will display commands to build and push Push commands will be like (Windows PowerShell):\n# 1. Retrieve an authentication token and authenticate Docker client (Get-ECRLoginCommand).Password | docker login --username AWS --password-stdin 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com # 2. Build your Docker image docker build -t mlops/retail-api . # 3. Tag your image docker tag mlops/retail-api:latest 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest # 4. Push image to ECR docker push 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest Or use AWS CLI:\n# 1. Retrieve an authentication token and authenticate Docker client aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com # 2. Build your Docker image docker build -t mlops/retail-api . # 3. Tag your image docker tag mlops/retail-api:latest 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest # 4. Push image to ECR docker push 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest Info: On Windows/PowerShell, prefer using aws ecr get-login-password --region \u0026lt;region\u0026gt; | docker login --username AWS --password-stdin \u0026lt;registry\u0026gt; in CI to avoid deprecated commands. ECR tokens typically expire after ~12 hours; re-authenticate for long-running sessions.\n2.2. Verify ECR Push Success Check in AWS Console:\nNavigate to ECR Console:\nGo to AWS Console â†’ ECR service Select repository mlops/retail-api Check \u0026ldquo;Images\u0026rdquo; tab to see if image has been pushed Expected Result:\nImage with tag latest appears in the list Image size displayed (~927MB) Vulnerability scan status (if enabled) Push timestamp Check using CLI:\nCheck using console:\nTip: Reduce image size using multi-stage builds and lightweight base images (e.g., python:3.9-slim or distroless). Smaller images help with faster push/pull and reduce storage/transfer costs.\n2.5. Container Environment \u0026amp; Testing Environment Variables:\n# Basic configuration AWS_DEFAULT_REGION=ap-southeast-1 MODEL_BUCKET=mlops-retail-forecast-models LOG_LEVEL=INFO PORT=8000 Test Docker Image Locally:\n# Test API container locally docker run -d \\ --name retail-api-test \\ -p 8000:8000 \\ -e AWS_DEFAULT_REGION=ap-southeast-1 \\ -e MODEL_BUCKET=mlops-retail-prediction-dev-842676018087 \\ 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest # Test health endpoint curl http://localhost:8000/health # Test API documentation open http://localhost:8000/docs # Clean up docker stop retail-api-test \u0026amp;\u0026amp; docker rm retail-api-test Warning (Local): When running image on local machine, avoid mounting secrets or AWS credentials into container. Use environment variables only for non-sensitive values and prefer IAM roles for production environment.\nLocal container test for retail-api : Complete! ğŸ‰\nECR registry has been set up and integrated with EKS cluster mlops-retail-cluster. Docker image for retail API is ready to deploy on Kubernetes in Task 10.\nTask 6 Results âœ… ECR Repository - mlops/retail-api repository\nâœ… Container Image - FastAPI prediction service\nâœ… Cost Optimization - Lifecycle policies, multi-stage builds, ~$0.15/month\nğŸ¯ Task 6 Complete - ECR Registry + API Containerization!\nâœ… ECR Setup: Repository with lifecycle policies \u0026amp; image scanning\nâœ… Dockerfile: Multi-stage build, non-root user, health checks\nâœ… Build \u0026amp; Push: Local build â†’ ECR push workflow\nâœ… Testing: Container verification \u0026amp; API validation\nâœ… Ready: Ready for EKS deployment in Task 7\nInfo (Vulnerability Scanning): Basic image scanning is free; enhanced scanning (Inspector) may incur charges per image/month. Consider scanning only production tags or integrating scanning into CI with conditions to control costs.\nğŸš€ Next Steps:\nTask 7: EKS cluster deployment with ECR integration Task 8: Deploy API container to EKS with ALB Task 9: Load balancing and scaling configuration Benchmark Results (Production):\nImage size: FastAPI ~500MB (multi-stage optimized) Build time: ~3-5 minutes (with cache) Storage cost: ~$0.15/month (total ~1.5GB) Security: Running non-root, vulnerability scanned Availability: Multi-tag strategy (latest, commit, branch) CI/CD: Automated on every commit 3. Clean Up Resources (AWS CLI) 3.1. Delete Images from ECR Repository # List images in repository aws ecr describe-images --repository-name mlops/retail-api --region ap-southeast-1 --query \u0026#39;imageDetails[*].[imageDigest,imageTags[0],imagePushedAt]\u0026#39; --output table # Delete specific image tag aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --image-ids imageTag=latest \\ --region ap-southeast-1 # Delete all images in repository aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --image-ids \u0026#34;$(aws ecr describe-images --repository-name mlops/retail-api --region ap-southeast-1 --query \u0026#39;imageDetails[*].{imageDigest:imageDigest}\u0026#39; --output json)\u0026#34; \\ --region ap-southeast-1 3.2. Delete ECR Repositories # Delete repository (must be empty first) aws ecr delete-repository --repository-name mlops/retail-api --region ap-southeast-1 --force # Verify repository has been deleted aws ecr describe-repositories --region ap-southeast-1 --query \u0026#39;repositories[?repositoryName==`mlops/retail-api`]\u0026#39; 3.3. Delete Lifecycle Policies # Delete lifecycle policy (automatically deleted when repository is deleted) aws ecr delete-lifecycle-policy --repository-name mlops/retail-api --region ap-southeast-1 # List remaining repositories aws ecr describe-repositories --region ap-southeast-1 --query \u0026#39;repositories[*].[repositoryName,repositoryUri]\u0026#39; --output table 3.4. Clean Up Local Docker Images # Remove local Docker images docker rmi mlops/retail-api:latest docker rmi 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest # Clean up Docker build cache docker system prune -f # Remove unused images docker image prune -a -f 3.5. ECR Cleanup Helper Script #!/bin/bash # ecr-cleanup.sh REPOSITORY_NAME=\u0026#34;mlops/retail-api\u0026#34; REGION=\u0026#34;ap-southeast-1\u0026#34; echo \u0026#34;ğŸ§¹ Cleaning up ECR repository: $REPOSITORY_NAME...\u0026#34; # 1. Delete all images echo \u0026#34;Deleting all images...\u0026#34; IMAGE_IDS=$(aws ecr describe-images --repository-name $REPOSITORY_NAME --region $REGION --query \u0026#39;imageDetails[*].{imageDigest:imageDigest}\u0026#39; --output json) if [ \u0026#34;$IMAGE_IDS\u0026#34; != \u0026#34;[]\u0026#34; ]; then aws ecr batch-delete-image \\ --repository-name $REPOSITORY_NAME \\ --image-ids \u0026#34;$IMAGE_IDS\u0026#34; \\ --region $REGION echo \u0026#34;Images deleted\u0026#34; else echo \u0026#34;No images to delete\u0026#34; fi # 2. Delete repository echo \u0026#34;Deleting repository...\u0026#34; aws ecr delete-repository \\ --repository-name $REPOSITORY_NAME \\ --region $REGION \\ --force # 3. Clean up local Docker echo \u0026#34;Cleaning up local Docker images...\u0026#34; docker rmi mlops/retail-api:latest 2\u0026gt;/dev/null || true docker rmi 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/$REPOSITORY_NAME:latest 2\u0026gt;/dev/null || true echo \u0026#34;âœ… ECR cleanup completed\u0026#34; 4. Báº£ng giÃ¡ ECR 4.1. Chi phÃ­ ECR Storage Storage Type GiÃ¡ (USD/GB/thÃ¡ng) Ghi chÃº ECR Storage $0.10 Compressed image size Free Tier 500MB free First 12 months Data Transfer IN Free Push images to ECR Data Transfer OUT $0.12/GB Pull from Internet Data Transfer VPC Free Pull via VPC Endpoints 4.2. Image Scanning Cost Scan Type Price (USD) Notes Basic Scanning Free CVE database scanning Enhanced Scanning $0.09/image/month Inspector integration OS Package Scanning Free Basic vulnerability detection Language Package Scanning $0.09/image/month Enhanced scanning only 4.3. Estimated Cost for Task 6 Container Images:\nFastAPI image: ~500MB (compressed) Total storage: ~0.5GB Monthly Costs:\nComponent Size Price Monthly Cost ECR Storage 0.5GB $0.10/GB $0.05 Basic Scanning 1 image Free $0.00 VPC Endpoint Transfer ~1GB/month Free $0.00 Total $0.05 4.4. Cost Comparison with Alternatives ECR vs Docker Hub:\nFeature ECR Docker Hub Winner Storage (500MB) $0.05/month Free (public) Docker Hub Private repos âœ… Native $5/month ECR AWS Integration âœ… Native Manual setup ECR VPC Endpoints âœ… Free transfer âŒ Internet only ECR IAM Integration âœ… Native âŒ Token-based ECR Vulnerability Scanning âœ… Built-in âŒ Extra cost ECR 4.5. Data Transfer Costs ECR Pull Scenarios:\nPull Location Cost Use Case Same Region (VPC) Free EKS production Same Region (Internet) $0.12/GB CI/CD outside AWS Cross Region $0.12/GB + transfer Multi-region deployment Internet (outside AWS) $0.12/GB Local development 4.6. Lifecycle Policy Cost Savings Without Lifecycle Policies:\n50 images Ã— 500MB = 25GB storage Cost: 25GB Ã— $0.10 = $2.50/month With Lifecycle Policies (Task 6):\nKeep 10 production images = 5GB Keep 5 development images = 2.5GB Total: 7.5GB Ã— $0.10 = $0.75/month Savings: $1.75/month (70%) 4.7. Cost Optimization Tips Storage Optimization:\n# Multi-stage builds reduce image size FROM node:16 as builder # ... build steps FROM node:16-alpine as production # Smaller base image COPY --from=builder /app/dist ./dist Registry Management:\n# Automated cleanup with lifecycle policies aws ecr put-lifecycle-policy \\ --repository-name mlops/retail-api \\ --lifecycle-policy-text file://lifecycle-policy.json Free Tier Usage:\nUse 500MB free tier for development Production images in separate repositories VPC Endpoints to avoid data transfer charges ğŸ’° Cost Summary for Task 6:\nStorage: $0.05/month (500MB images) Scanning: Free (basic vulnerability detection) Data Transfer: Free (VPC Endpoints to EKS) Total: $0.05/month (vs $5/month Docker Hub private) Savings: $4.95/month with ECR + lifecycle policies Success Tip: Before deleting repos/images for cleanup, snapshot deployment manifests and CI references if archival is needed. Prefer using lifecycle policies for automated retention management instead of manual deletion to avoid data loss.\nğŸ¬ Task 6 Implementation Video Next Step: Task 7: EKS Cluster Setup\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.7-blog7/",
	"title": "Introducing the AWS Infrastructure as Code MCP Server: AI-Powered CDK and CloudFormation Assistance",
	"tags": [],
	"description": "",
	"content": "Author: Idriss Laouali Abdou â€” 28 NOV 2025\nStreamline your AWS infrastructure development with AI-powered documentation search, validation, and troubleshooting\nIntroduction Today, weâ€™re excited to introduce the AWS Infrastructure-as-Code (IaC) MCP Server, a new tool that bridges the gap between AI assistants and your AWS infrastructure development workflow. Built on the Model Context Protocol (MCP), this server enables AI assistants like Kiro CLI, Claude, or Cursor to help you search AWS CloudFormation and Cloud Development Kit (CDK) documentation, validate templates, troubleshoot deployments, and follow best practices â€“ all while maintaining the security of local execution.\nWhether youâ€™re writing AWS CloudFormation templates or AWS Cloud Development Kit (CDK) code, the IaC MCP Server acts as an intelligent companion that understands your infrastructure needs and provides contextual assistance throughout your development lifecycle.\nThe Model Context Protocol (MCP) is an open standard that enables AI assistants to securely connect to external data sources and tools. Think of it as a universal adapter that lets AI models interact with your development tools while keeping sensitive operations local and under your control.\nThe IaC MCP Server provides nine specialized tools organized into two categories:\nRemote documentation search tools These tools connect to the AWS Knowledge MCP backend to retrieve relevant, up-to-date information:\nsearch_cdk_documentation\nSearch the AWS CDK knowledge base for APIs, concepts, and implementation guidance. search_cdk_samples_and_constructs\nDiscover pre-built AWS CDK constructs and patterns from the AWS Construct Library. search_cloudformation_documentation\nQuery CloudFormation documentation for resource types, properties, and intrinsic functions. read_iac_documentation_page\nRetrieve and read full CloudFormation and CDK documentation pages returned from searches or provided URLs. Local validation and troubleshooting tools These tools run entirely on your machine\ncdk_best_practices\nAccess a curated collection of AWS CDK best practices and design principles. validate_cloudformation_template\nPerform syntax and schema validation using cfn-lint to catch errors before deployment. check_cloudformation_template_compliance\nRun security and compliance checks against your templates using AWS Guard rules and cfn-guard. troubleshoot_cloudformation_deployment\nAnalyze CloudFormation stack deployment failures with integrated CloudTrail event analysis. This tool will use your AWS credentials to analyze your stack status. get_cloudformation_pre_deploy_validation_instructions\nReturns instructions for CloudFormationâ€™s pre-deployment validation feature, which validates templates during change set creation. Key use cases 1. Intelligent documentation assistant Instead of manually searching through documentation, ask your AI assistant natural language questions:\nâ€œHow do I create an S3 bucket with encryption enabled in CDK?â€\nThe server searches CDK best practice and samples, returning relevant code examples and explanations.\n2. Proactive template validation Before deploying infrastructure changes:\nUser: â€œValidate my CloudFormation template and check for security issuesâ€\nAI Agent: [Uses validate_cloudformation_template and check_cloudformation_template_compliance]\nâ€œFound 2 issues: Missing encryption on EBS volumes,\nand S3 bucket lacks public access block configurationâ€\n3. Rapid deployment troubleshooting When a stack deployment fails:\nUser: â€œMy stack â€˜stack_03â€™ in us-east-1 failed to deploy. What happened?â€\nAI Agent: [Uses troubleshoot_stack_deployment with CloudTrail integration]\nâ€œThe deployment failed due to insufficient IAM permissions.\nCloudTrail shows AccessDenied for ec2:CreateVpc.\nYou need to add VPC permissions to your deployment role.â€\n4. Learning and exploration New to AWS CDK? The server helps you discover constructs and patterns:\nUser: â€œShow me how to build a serverless APIâ€\nAI Agent: [Searches CDK constructs and samples]\nâ€œHere are three approaches using API Gateway + Lambdaâ€¦â€\nArchitecture and security Security design Local Execution: The MCP server runs entirely on your local machine using uv (the fast Python package manager). No code or templates are sent to external services except for documentation searches. AWS Credentials: The server uses your existing AWS credentials (from ~/.aws/credentials, environment variables, or IAM roles) to access CloudFormation and CloudTrail APIs. This follows the same security model as the AWS CLI. stdio Communication: The server communicates with AI assistants over standard input/output (stdio), with no network ports opened. Minimal Permissions: For full functionality, the server requires read-only access to CloudFormation stacks and CloudTrail eventsâ€”no write permissions needed for validation and troubleshooting workflows. Getting started Prerequisites Python 3.10 or later uv package manager AWS credentials configured locally MCP-compatible AI client (e.g., Kiro CLI, Claude Desktop) Configuration Configure the MCP server in your MCP client configuration. For this blog we will focus on Kiro CLI. Edit .kiro/settings/mcp.json):\n{ \u0026#34;mcpServers\u0026#34;: { \u0026#34;awslabs.aws-iac-mcp-server\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;uvx\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;awslabs.aws-iac-mcp-server@latest\u0026#34;], \u0026#34;env\u0026#34;: { \u0026#34;AWS_PROFILE\u0026#34;: \u0026#34;your-named-profile\u0026#34;, \u0026#34;FASTMCP_LOG_LEVEL\u0026#34;: \u0026#34;ERROR\u0026#34; }, \u0026#34;disabled\u0026#34;: false, \u0026#34;autoApprove\u0026#34;: [] } } } JSON\nSecurity considerations Privacy Notice: This MCP server executes AWS API calls using your credentials and shares the response data with your third-party AI model provider (e.g., Amazon Q, Claude Desktop, Cursor, VS Code). Users are responsible for understanding your AI providerâ€™s data handling practices and ensuring compliance with your organizationâ€™s security and privacy requirements when using this tool with AWS resources.\nIAM permissions The MCP server requires the following AWS permissions:\nFor template validation and compliance:\nNo AWS permissions required (local validation only) For deployment troubleshooting:\ncloudformation:DescribeStacks cloudformation:DescribeStackEvents cloudformation:DescribeStackResources cloudtrail:LookupEvents (for CloudTrail deep links) Example IAM policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:DescribeStacks\u0026#34;, \u0026#34;cloudformation:DescribeStackEvents\u0026#34;, \u0026#34;cloudformation:DescribeStackResources\u0026#34;, \u0026#34;cloudtrail:LookupEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } JSON\nExample use case with Kiro CLI IMPORTANT: Ensure you have satisfied all prerequisites before attempting these commands.\nWith the mcp.json file correctly set, try to run a sample prompt. In your terminal, run kiro-cli chat to start using Kiro-cli in the CLI. Scenarios:\nâ€œWhat are the CDK best practices for Lambda functions?â€ â€œSearch for CDK samples that use DynamoDB with Lambdaâ€ â€œValidate my CloudFormation template at ./template.yamlâ€ â€œCheck if my template complies with security best practicesâ€ Best practices Start with documentation search: Before writing code, search for existing constructs and patterns Validate early and often: Run validation tools before attempting deployment Check compliance: Use check_template_compliance to catch security issues during development Leverage CloudTrail: When troubleshooting, the CloudTrail integration provides detailed failure context Follow CDK best practices: Use the cdk_best_practices tool to align with AWS recommendations Whatâ€™s next? The IAC MCP Server represents a new paradigm in the AI agentic workflow infrastructure development â€“ one where AI assistants understand your tools, help you navigate complex documentation, and provide intelligent assistance throughout the development lifecycle.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. Your feedback will help the FCJ team improve based on the aspects below:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to support me whenever I face difficulties, even outside working hours. The workspace is clean and comfortable, which helps me stay focused.\n2. Support from Mentor / Team Admin\nMy mentor provides very detailed guidance, explains clearly when I donâ€™t understand, and consistently encourages me to ask questions. The admin team supports procedures and documentation and creates favorable conditions for me to work effectively. The mentor was especially helpful during key stages such as architecture design, selecting best practices, and debugging with a â€œfind the root cause yourselfâ€ approach. This helped me learn how systems operate in practice instead of just following instructions.\n3. Relevance of the Work to My Academic Major\nThe tasks I worked on aligned well with what I learned at university while also expanding into areas I had not previously explored. This allowed me to strengthen my fundamentals and gain real-world practical skills. The workshop directly relates to fields I care about, such as Cloud, DevOps, and Security/Networking: VPC, IAM/IRSA, container security (non-root, scan-on-push), logging/monitoring, and cost management. These topics helped me connect academic knowledge with real deployment scenarios.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork, and professional communication in a corporate environment. My mentor also shared valuable real-world experiences that helped me plan my career path better.\nI significantly improved my implementation and operations skills, including:\nDesigning an end-to-end MLOps lifecycle: training pipeline + model registry + deployment Packaging and deploying a FastAPI inference service on EKS Setting up basic monitoring with CloudWatch and configuring log retention Optimizing costs using Spot Instances, lifecycle policies, start/stop scheduling, and teardown scripts 5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other and works seriously while still keeping the environment enjoyable.\n6. Internship Policies / Benefits\nThe company supports internship confirmation and provides flexible working hours when needed.\nAdditional Questions What did you find most satisfying during your internship?\nI was most satisfied with being able to work in a comfortable environment with clear processes. I learned how to collaborate with my mentor and teammates, how to try solving problems independently before asking for help, and I developed a â€œdo it right â€“ do it fully â€“ clean it upâ€ mindset (after deploying, always verify, document, optimize, and clean up resourcesâ€”otherwise you end up paying unnecessary costs).\nWhat do you think the company should improve for future interns?\nI suggest organizing more networking or exchange activities between universities so interns can learn from one another and share experiences across different schools.\nIf recommending to a friend, would you suggest they intern here? Why or why not?\nYes. The environment is supportive, the mentor helps interns grow through hands-on problem solving, and the work is practical (with real deliverables). However, it is best suited for interns who are self-driven and proactive in learning.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.6-week6/",
	"title": "Week 6 - Database Service on AWS",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Deploy and manage Relational (RDS) and NoSQL (DynamoDB) databases. Implement database migration strategies. Tasks to be carried out this week: Day Task Reference 2 - RDS: Launch RDS (PostgreSQL/MySQL) in private subnets.\n- Configure Multi-AZ and Automated Backups. AWS RDS Docs 3 - Access: Connect to RDS via SSM/Bastion securely.\n- Monitor using Performance Insights. AWS Docs 4 - DynamoDB: Design tables (Partition/Sort Keys, GSIs).\n- Test CRUD operations and TTL settings. DynamoDB Guide 5 - Migration (Optional): Test AWS DMS (Database Migration Service).\n- Compare RDS vs DynamoDB performance. AWS DMS Docs 6 - Review: Validate database failover and backup recovery.\n- Weekly Report. - Week 6 Achievements: Deployed secure Multi-AZ RDS with automated backups. Designed optimized DynamoDB tables with appropriate keys and indexes. Implemented secure database access without public exposure. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/7-eks-cluster/",
	"title": "EKS Cluster Setup (Production)",
	"tags": [],
	"description": "",
	"content": "ğŸ¯ Task 7 Objectives Deploy Amazon Elastic Kubernetes Service (EKS) as the foundation to run prediction API (FastAPI) in production environment:\nEKS Control Plane: AWS-managed cluster with Kubernetes 1.30 IRSA Integration: IAM Roles for Service Accounts for secure AWS access Cost Optimization: Free Tier t2.micro instances for development VPC Integration: Use hybrid VPC and VPC Endpoints from Task 5 ECR Integration: Container image pulls from ECR repository â†’ Ensure stable, scalable system with secure IAM integration (IRSA).\nğŸ“¥ Input from Previous Tasks:\nTask 5 (Production VPC): Hybrid VPC, subnets, VPC Endpoints and security groups used for EKS networking Task 2 (IAM Roles \u0026amp; Audit): IAM roles and policies (cluster role, node role, IRSA foundations) Task 6 (ECR Registry): ECR repository for container images that EKS will pull EKS Architecture in MLOps Pipeline Tip: Use private-only endpoint access for production clusters and limit public access CIDRs to specific IP ranges. Enable all control plane logs for security audit and compliance.\nCost Optimization Strategy Component Cost Free Tier Strategy EKS Control Plane $73/month âŒ Use for demo, destroy after t2.micro Nodes (2x) $0 âœ… 750h/month 12 months FREE EBS Storage (20GB) $0 âœ… 30GB FREE Within free tier VPC Endpoints $21.6/month âŒ Shared with other services Total ~$95/month $60/month savings Free Tier Benefits:\n750 hours/month t2.micro instances â†’ Enough for 24/7 demo 30GB EBS storage â†’ Adequate for basic workloads Total savings: $60/month vs t3.medium instances Warning: EKS control plane costs $73/month regardless of usage level. For learning/testing, consider using self-managed k3s on EC2 or kind locally to avoid fixed costs. Delete cluster immediately after demo.\n1. EKS Cluster Setup via Console 1.1. Create EKS Cluster Navigate to EKS Console:\nAWS Console â†’ EKS â†’ \u0026ldquo;Create cluster\u0026rdquo; Basic Configuration:\nCluster name: mlops-retail-cluster\rKubernetes version: 1.30\rCluster service role: Use existing from Task 2 (IAM) Networking Configuration:\nVPC: mlops-retail-forecast-hybrid-vpc (from Task 5)\rSubnets: Select all 4 subnets (2 public + 2 private)\rCluster endpoint access: Public and private\rPublic access sources: Specify your IP range\rSecurity groups: mlops-hybrid-eks-control-plane-sg Control Plane Logging: âœ… API server\râœ… Audit\râœ… Authenticator\râœ… Controller manager\râœ… Scheduler Add-ons Configuration: âœ… Amazon VPC CNI: v1.18.1-eksbuild.1\râœ… CoreDNS: v1.11.1-eksbuild.4\râœ… kube-proxy: v1.30.0-eksbuild.3\râœ… Amazon EBS CSI Driver: v1.30.0-eksbuild.1 1.2. Update kubeconfig eksctl create cluster -f eksctl-cluster.yaml Option B: AWS Console (if you must) EKS â†’ Clusters â†’ Create Name: mlops-retail-cluster, Region: ap-southeast-1 Networking: select Production VPC + private subnets for nodes Enable control plane logs After cluster is ready â†’ create Managed Node Group (t2.micro for dev) 2) Add-ons \u0026amp; observability baseline Recommended EKS add-ons:\nvpc-cni coredns kube-proxy aws-ebs-csi-driver (optional) # Check core add-ons kubectl get pods -n kube-system # Verify VPC CNI kubectl get daemonset -n kube-system aws-node # Check CoreDNS kubectl get deployment -n kube-system coredns # Verify kube-proxy kubectl get daemonset -n kube-system kube-proxy # Check EBS CSI driver kubectl get deployment -n kube-system ebs-csi-controller ğŸ¯ EKS Control Plane Ready!\nControl Plane Status:\nâœ… Kubernetes 1.30 cluster ACTIVE âœ… Multi-AZ managed control plane âœ… All essential add-ons running âœ… CloudWatch logging enabled âœ… kubectl access configured Info: EKS control plane automatically runs across multiple AZs. To optimize costs, ensure worker nodes are balanced across AZs to avoid cross-AZ data transfer charges ($0.01/GB).\n2. IRSA (IAM Roles for Service Accounts) Setup 2.1. Associate OIDC Provider aws eks update-kubeconfig --name mlops-retail-cluster --region ap-southeast-1 kubectl get nodes 4) IRSA (IAM Roles for Service Accounts) 4.1 Associate OIDC provider eksctl utils associate-iam-oidc-provider --cluster mlops-retail-cluster --region ap-southeast-1 --approve 4.2 Create an IAM role for pods (example: S3 read + CloudWatch logs) Create policy irsa-s3-cw.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3ReadArtifacts\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [\u0026#34;arn:aws:s3:::\u0026lt;YOUR_BUCKET\u0026gt;\u0026#34;, \u0026#34;arn:aws:s3:::\u0026lt;YOUR_BUCKET\u0026gt;/*\u0026#34;] }, { \u0026#34;Sid\u0026#34;: \u0026#34;CWLogsWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Create policy + role (example via CLI):\naws iam create-policy --policy-name EKSIRSA-S3-CW --policy-document file://irsa-s3-cw.json # Create role trust policy is easier using eksctl serviceaccount (next section) Create K8s namespace and service account with eksctl:\nkubectl create namespace mlops-retail-forecast || true eksctl create iamserviceaccount --cluster mlops-retail-cluster --region ap-southeast-1 --namespace mlops-retail-forecast --name retail-sa --attach-policy-arn arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:policy/EKSIRSA-S3-CW --approve --override-existing-serviceaccounts 5) Verify ECR access (image pull) If your node group has the standard EKS worker role, it can pull from ECR. If you run strictly private subnets with no NAT, confirm your VPC endpoints exist:\ncom.amazonaws.ap-southeast-1.ecr.api com.amazonaws.ap-southeast-1.ecr.dkr com.amazonaws.ap-southeast-1.logs S3 Gateway endpoint 6) Deploy a sample app (smoke test) Create k8s/sample-app.yaml:\napiVersion: v1 kind: Namespace metadata: name: mlops-retail-forecast labels: name: mlops-retail-forecast --- apiVersion: v1 kind: ServiceAccount metadata: name: s3-access-sa namespace: mlops-retail-forecast annotations: eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/mlops-irsa-s3-access-role labels: app.kubernetes.io/name: s3-access-service-account app.kubernetes.io/component: rbac --- apiVersion: v1 kind: ServiceAccount metadata: name: cloudwatch-sa namespace: mlops-retail-forecast annotations: eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/mlops-irsa-cloudwatch-role labels: app.kubernetes.io/name: cloudwatch-service-account app.kubernetes.io/component: monitoring --- apiVersion: v1 kind: ServiceAccount metadata: name: retail-api-sa namespace: mlops-retail-forecast annotations: eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/mlops-irsa-s3-access-role labels: app.kubernetes.io/name: retail-api-service-account app.kubernetes.io/component: api Apply service accounts:\n# Replace ACCOUNT_ID with your AWS account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) sed \u0026#34;s/ACCOUNT_ID/$ACCOUNT_ID/g\u0026#34; k8s/service-accounts.yaml | kubectl apply -f - # Verify service accounts kubectl get serviceaccounts -n mlops-retail-forecast kubectl describe serviceaccount s3-access-sa -n mlops-retail-forecast ğŸ¯ IRSA Setup Complete!\nIRSA Components:\nâœ… OIDC identity provider associated with EKS âœ… S3 access role for ML workloads âœ… CloudWatch role for monitoring âœ… Kubernetes service accounts with proper annotations âœ… Secure AWS access without hardcoded credentials Tip: Use separate service accounts for each workload with minimal IAM policies. Avoid sharing service accounts between applications and regularly audit IRSA role permissions.\n3. Managed Node Group Setup 3.1. Create Node Group via Console Navigate to Node Groups:\nEKS Console â†’ mlops-retail-cluster â†’ Compute â†’ Node groups â†’ \u0026ldquo;Add node group\u0026rdquo; Node Group Configuration:\nName: mlops-retail-nodegroup-t2micro\rNode IAM role: mlops-hybrid-eks-nodes-role (from Task 5) Instance Configuration:\nAMI type: Amazon Linux 2 (AL2_x86_64)\rCapacity type: On-Demand\rInstance types: t2.micro\rDisk size: 20 GB Scaling Configuration:\nDesired size: 2\rMinimum size: 1\rMaximum size: 4 Network Configuration:\nSubnets: Both private workload subnets\rConfigure remote access: Enable (optional)\rSSH key pair: Your EC2 key pair (optional)\rSource security groups: mlops-hybrid-eks-nodes-sg 3.2. Alternative: eksctl Node Group (Command Line) Create file scripts/create-nodegroup.sh:\n#!/bin/bash # Configuration CLUSTER_NAME=\u0026#34;mlops-retail-cluster\u0026#34; REGION=\u0026#34;ap-southeast-1\u0026#34; NODEGROUP_NAME=\u0026#34;mlops-retail-nodegroup-t2micro\u0026#34; echo \u0026#34;ğŸ”§ Creating EKS node group with t2.micro instances...\u0026#34; # Create node group using eksctl eksctl create nodegroup \\ --cluster=$CLUSTER_NAME \\ --region=$REGION \\ --name=$NODEGROUP_NAME \\ --node-type=t2.micro \\ --nodes=2 \\ --nodes-min=1 \\ --nodes-max=4 \\ --node-volume-size=20 \\ --node-volume-type=gp3 \\ --ssh-access \\ --ssh-public-key=YOUR_KEY_NAME \\ --managed \\ --node-private-networking \\ --node-zones=ap-southeast-1a,ap-southeast-1b echo \u0026#34;âœ… Node group creation initiated!\u0026#34; echo \u0026#34;Check status: eksctl get nodegroup --cluster $CLUSTER_NAME --region $REGION\u0026#34; 3.3. Verify Node Group # Check node group status aws eks describe-nodegroup \\ --cluster-name mlops-retail-cluster \\ --nodegroup-name mlops-retail-nodegroup-t2micro \\ --region ap-southeast-1 # Verify nodes in Kubernetes kubectl get nodes kubectl get nodes -o wide # Check node labels and annotations kubectl describe nodes Expected output:\nNAME STATUS ROLES AGE VERSION\rip-10-0-101-123.ap-southeast-1.compute.internal Ready \u0026lt;none\u0026gt; 5m v1.30.0-eks-xyz\rip-10-0-102-456.ap-southeast-1.compute.internal Ready \u0026lt;none\u0026gt; 5m v1.30.0-eks-xyz ğŸ¯ Node Group Ready!\nNode Group Status:\nâœ… 2x t2.micro instances (Free Tier) âœ… Private subnet deployment âœ… Auto Scaling Group configured (1-4 instances) âœ… All nodes in Ready state âœ… EKS optimized AMI with latest patches Warning (t2.micro): Limited to 1 vCPU and 1GB RAM. Monitor CPU credits and consider burstable performance limits. For production ML workloads, use t3.medium+ instances.\n4. Cost Optimization with VPC Endpoints 4.1. Review VPC Endpoints for EKS VPC Endpoints Created in Task 5:\n# Verify VPC endpoints exist for EKS cost optimization aws ec2 describe-vpc-endpoints \\ --filters \u0026#34;Name=vpc-id,Values=vpc-xxxxx\u0026#34; \\ --query \u0026#39;VpcEndpoints[?ServiceName==`com.amazonaws.ap-southeast-1.s3`]\u0026#39; aws ec2 describe-vpc-endpoints \\ --filters \u0026#34;Name=vpc-id,Values=vpc-xxxxx\u0026#34; \\ --query \u0026#39;VpcEndpoints[?ServiceName==`com.amazonaws.ap-southeast-1.ecr.api`]\u0026#39; Expected VPC Endpoints:\nâœ… S3 Gateway Endpoint (FREE - no data charges) âœ… ECR API Endpoint ($0.01/hour per AZ) âœ… ECR Docker Endpoint ($0.01/hour per AZ) âœ… CloudWatch Logs Endpoint ($0.01/hour per AZ) 4.2. Cost Savings with VPC Endpoints Data Transfer Without VPC Endpoints With VPC Endpoints Savings S3 Access $0.09/GB (NAT Gateway) $0.00/GB (Gateway) 100% ECR Pull $0.09/GB (Internet) $0.00/GB (Private) 100% CloudWatch $0.09/GB (NAT Gateway) $0.00/GB (Private) 100% ğŸ’° Monthly Cost Breakdown with t2.micro + VPC Endpoints:\nEKS Control Plane: $73.00/month (fixed)\nt2.micro Nodes: $0.00/month (FREE tier - 750 hours)\nVPC Endpoints: ~$7.20/month (4 endpoints Ã— 2 AZ Ã— $0.01/hour)\nData Transfer: $0.00/month (all private via VPC endpoints)\nTotal Monthly Cost: ~$80.20/month\nSavings vs NAT Gateway: ~$45/month (NAT Gateway cost)\nInfo: VPC Endpoints eliminate internet routing for AWS services but cost $0.01/hour per endpoint per AZ. Monitor usage patterns and consolidate endpoints when possible.\n5. Deploy Sample Application with IRSA 5.1. Deploy FastAPI Prediction Service Create file k8s/retail-api-deployment.yaml:\n--- apiVersion: apps/v1 kind: Deployment metadata: name: sample-nginx namespace: mlops-retail-forecast spec: replicas: 1 selector: matchLabels: app: sample-nginx template: metadata: labels: app: sample-nginx spec: containers: - name: nginx image: nginx:1.25 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: sample-nginx-svc namespace: mlops-retail-forecast spec: type: ClusterIP selector: app: sample-nginx ports: - port: 80 targetPort: 80 Apply + verify:\nkubectl apply -f k8s/sample-app.yaml kubectl get pods -n mlops-retail-forecast kubectl get svc -n mlops-retail-forecast 5.3. Test IRSA S3 Access Create test pod:\n# Create test pod to verify IRSA S3 access kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: test-irsa-s3 namespace: mlops-retail-forecast spec: serviceAccountName: s3-access-sa containers: - name: test-s3 image: amazon/aws-cli:latest command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;aws s3 ls s3://mlops-retail-forecast-models \u0026amp;\u0026amp; sleep 3600\u0026#34;] env: - name: AWS_DEFAULT_REGION value: \u0026#34;ap-southeast-1\u0026#34; restartPolicy: Never EOF # Check if S3 access works via IRSA kubectl logs test-irsa-s3 -n mlops-retail-forecast Expected output:\n2024-01-15 10:30:45 model-v1.0.pkl\r2024-01-15 10:31:20 model-v1.1.pkl\rPRE training-data/ 6. Cluster Verification 6.1. Control Plane Health Check # Check cluster status kubectl get --raw=\u0026#39;/readyz?verbose\u0026#39; # Check API server health kubectl get --raw=\u0026#39;/healthz\u0026#39; # View cluster events kubectl get events --sort-by=.metadata.creationTimestamp 6.2. Add-ons Verification # Check CoreDNS kubectl get pods -n kube-system -l k8s-app=kube-dns # Check kube-proxy kubectl get daemonset -n kube-system kube-proxy # Check VPC CNI kubectl get daemonset -n kube-system aws-node # Check EBS CSI driver kubectl get deployment -n kube-system ebs-csi-controller 6.3. RBAC and Permissions # Check current user permissions kubectl auth can-i \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; # List available API resources kubectl api-resources # Check cluster roles kubectl get clusterroles | grep eks 6.4. IRSA Verification # Deploy service accounts with IRSA annotations kubectl apply -f aws/k8s/service-accounts.yaml # Deploy test pod with IRSA authentication kubectl apply -f aws/k8s/test-pod-irsa.yaml # Verify pod can access S3 via IRSA (no AWS credentials needed!) kubectl exec -it test-irsa-s3-access -- aws s3 ls # Check IRSA role annotations kubectl get serviceaccount s3-access-sa -o yaml # Verify OIDC provider aws iam list-open-id-connect-providers # List IRSA roles aws iam list-roles --query \u0026#39;Roles[?contains(RoleName, `irsa`)].{RoleName:RoleName,CreateDate:CreateDate}\u0026#39; # Test CloudWatch access kubectl exec -it test-irsa-s3-access -- aws cloudwatch list-metrics --namespace \u0026#34;MLOps/RetailForecast\u0026#34; # Cleanup test pod kubectl delete pod test-irsa-s3-access 7. Monitoring and Logging 7.1. CloudWatch Integration # Check if cluster logging is enabled aws eks describe-cluster \\ --name mlops-retail-forecast-dev-cluster \\ --query \u0026#39;cluster.logging\u0026#39; # View control plane logs in CloudWatch aws logs describe-log-groups \\ --log-group-name-prefix /aws/eks/mlops-retail-forecast-dev-cluster 7.2. Setup CloudWatch Container Insights File: aws/k8s/cloudwatch-insights.yaml\napiVersion: v1 kind: Namespace metadata: name: amazon-cloudwatch labels: name: amazon-cloudwatch --- apiVersion: v1 kind: ServiceAccount metadata: name: cloudwatch-agent namespace: amazon-cloudwatch annotations: eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/mlops-retail-forecast-dev-irsa-cloudwatch --- apiVersion: apps/v1 kind: DaemonSet metadata: name: cloudwatch-agent namespace: amazon-cloudwatch spec: selector: matchLabels: name: cloudwatch-agent template: metadata: labels: name: cloudwatch-agent spec: serviceAccountName: cloudwatch-agent containers: - name: cloudwatch-agent image: amazon/cloudwatch-agent:1.300026.2b251814 env: - name: AWS_REGION value: ap-southeast-1 - name: CLUSTER_NAME value: mlops-retail-forecast-dev-cluster volumeMounts: - name: cwagentconfig mountPath: /etc/cwagentconfig - name: rootfs mountPath: /rootfs readOnly: true - name: dockersock mountPath: /var/run/docker.sock readOnly: true - name: varlibdocker mountPath: /var/lib/docker readOnly: true volumes: - name: cwagentconfig configMap: name: cwagentconfig - name: rootfs hostPath: path: / - name: dockersock hostPath: path: /var/run/docker.sock - name: varlibdocker hostPath: path: /var/lib/docker 8. Security Hardening 8.1. Network Security # Verify security groups aws ec2 describe-security-groups \\ --group-ids $(terraform output -raw eks_control_plane_security_group_id) \\ --query \u0026#39;SecurityGroups[0].{GroupId:GroupId,IpPermissions:IpPermissions}\u0026#39; # Check VPC configuration aws eks describe-cluster \\ --name mlops-retail-forecast-dev-cluster \\ --query \u0026#39;cluster.resourcesVpcConfig\u0026#39; 8.2. RBAC Configuration File: aws/k8s/rbac.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: mlops-admin namespace: mlops-retail-forecast --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: mlops-retail-forecast name: mlops-admin-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;configmaps\u0026#34;, \u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;, \u0026#34;replicasets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: mlops-admin-binding namespace: mlops-retail-forecast subjects: - kind: ServiceAccount name: mlops-admin namespace: mlops-retail-forecast roleRef: kind: Role name: mlops-admin-role apiGroup: rbac.authorization.k8s.io 9. Clean Up Resources (AWS CLI) 9.1. Delete Node Groups # List all node groups aws eks list-nodegroups --cluster-name mlops-retail-cluster --region ap-southeast-1 # Delete node group (this will take 5-10 minutes) aws eks delete-nodegroup \\ --cluster-name mlops-retail-cluster \\ --nodegroup-name mlops-retail-nodegroup-t2micro \\ --region ap-southeast-1 # Monitor deletion status aws eks describe-nodegroup \\ --cluster-name mlops-retail-cluster \\ --nodegroup-name mlops-retail-nodegroup-t2micro \\ --region ap-southeast-1 \\ --query \u0026#39;nodegroup.status\u0026#39; 9.2. Delete IRSA Roles and OIDC Provider # Get account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # Detach and delete S3 access role aws iam detach-role-policy \\ --role-name mlops-irsa-s3-access-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/mlops-irsa-s3-policy aws iam delete-policy --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/mlops-irsa-s3-policy aws iam delete-role --role-name mlops-irsa-s3-access-role # Delete CloudWatch access role aws iam detach-role-policy \\ --role-name mlops-irsa-cloudwatch-role \\ --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy aws iam delete-role --role-name mlops-irsa-cloudwatch-role # Delete OIDC provider OIDC_URL=$(aws eks describe-cluster --name mlops-retail-cluster --region ap-southeast-1 --query \u0026#39;cluster.identity.oidc.issuer\u0026#39; --output text | sed \u0026#39;s|https://||\u0026#39;) aws iam delete-open-id-connect-provider --open-id-connect-provider-arn arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_URL} 9.3. Delete EKS Cluster # Delete the cluster (ensure node groups are deleted first) aws eks delete-cluster --name mlops-retail-cluster --region ap-southeast-1 # Monitor deletion progress aws eks describe-cluster --name mlops-retail-cluster --region ap-southeast-1 --query \u0026#39;cluster.status\u0026#39; # Verify cluster is deleted (should return error) aws eks list-clusters --region ap-southeast-1 --query \u0026#39;clusters[?contains(@, `mlops-retail-cluster`)]\u0026#39; 9.4. Clean Up Kubernetes Resources # Delete namespace (this removes all resources in the namespace) kubectl delete namespace mlops-retail-forecast # Remove cluster from kubeconfig kubectl config delete-cluster arn:aws:eks:ap-southeast-1:${ACCOUNT_ID}:cluster/mlops-retail-cluster kubectl config delete-context arn:aws:eks:ap-southeast-1:${ACCOUNT_ID}:cluster/mlops-retail-cluster # Remove user from kubeconfig kubectl config unset users.arn:aws:eks:ap-southeast-1:${ACCOUNT_ID}:cluster/mlops-retail-cluster 9.5. Script Dá»n dáº¹p EKS Tá»± Ä‘á»™ng kubectl delete namespace mlops-retail-forecast --ignore-not-found=true eksctl delete cluster --name mlops-retail-cluster --region ap-southeast-1 10. EKS Pricing Table (ap-southeast-1) 10.1. EKS Control Plane Cost Component Price (USD/cluster/hour) Price (USD/cluster/month) Notes EKS Control Plane $0.10 $73.00 Fixed cost regardless of usage Fargate $0.04048/vCPU/hour Variable Pay per pod resources EC2 Worker Nodes EC2 pricing Variable t2.micro has FREE tier 10.2. EC2 Worker Nodes Cost Instance Type vCPU Memory Price (USD/hour) Price (USD/month) Free Tier t2.micro 1 1GB $0.0116 $8.50 âœ… 750h/month t3.micro 2 1GB $0.0104 $7.61 âŒ t3.small 2 2GB $0.0208 $15.22 âŒ t3.medium 2 4GB $0.0416 $30.45 âŒ m5.large 2 8GB $0.096 $70.27 âŒ 10.3. VPC Endpoints Cost for EKS Endpoint Type Price (USD/hour/endpoint) Monthly Cost (2 AZ) Notes S3 Gateway Free $0.00 No hourly charge ECR API $0.01 $14.40 Required for image pulls ECR Docker $0.01 $14.40 Required for image pulls CloudWatch Logs $0.01 $14.40 Optional for logging EC2 $0.01 $14.40 Optional for instance metadata 10.4. Estimated Cost for Task 7 EKS Cluster Configuration:\nControl Plane: 1 cluster Worker Nodes: 2x t2.micro instances VPC Endpoints: ECR API + ECR Docker Storage: 20GB EBS per node Chi phÃ­ hÃ ng thÃ¡ng:\nComponent Configuration Monthly Cost Free Tier Discount EKS Control Plane 1 cluster $73.00 âŒ t2.micro Instances 2x instances (1,500h) $17.00 -$17.00 (FREE) EBS Storage 40GB gp3 $3.20 -$3.20 (30GB FREE) ECR VPC Endpoints 2 endpoints Ã— 2 AZ $28.80 âŒ Data Transfer Internal VPC $0.00 âœ… FREE Total $122.00 -$20.20 Actual Cost $101.80 10.5. Cost Comparison with Other Options EKS vs Self-Managed Kubernetes:\nFeature EKS Self-Managed K8s Savings Control Plane $73/month $0 (self-managed) -$73 System Management âœ… Managed âŒ Manual updates Time savings Security Patches âœ… Automatic âŒ Manual Security Multi-AZ HA âœ… Available âŒ Complex setup Reliability AWS Integration âœ… Native âŒ Manual Ease of use EKS vs ECS Fargate:\nWorkload Type EKS Cost ECS Fargate Cost Winner Small APIs (0.25 vCPU) $73 + instance $7.30/month Fargate Batch Jobs $73 + compute Pay per run Fargate Always-on Services $73 + compute $29.20/month EKS Multi-service Apps $73 + compute $N Ã— service cost EKS 10.6. Data Transfer Costs EKS Data Transfer Scenarios:\nTransfer Type Cost Use Case Pod to Pod (same AZ) Free Microservices communication Pod to Pod (different AZ) $0.01/GB Multi-AZ deployment Pod to Internet $0.12/GB API responses to users Pod to S3 (VPC Endpoint) Free Model/data access Pod to S3 (Internet) $0.12/GB No VPC endpoint 10.7. Cost Optimization Strategies Right-sizing Instance:\n# Use Kubernetes resource requests/limits resources: requests: cpu: 100m # 0.1 vCPU memory: 128Mi # 128MB limits: cpu: 500m # 0.5 vCPU memory: 512Mi # 512MB Node Scaling:\n# Cluster Autoscaler configuration desired_size = 2 min_size = 1 max_size = 10 # Scale down quickly when not needed scale_down_delay_after_add = \u0026#34;2m\u0026#34; scale_down_unneeded_time = \u0026#34;2m\u0026#34; Spot Instances:\n# Mix spot and on-demand for cost savings capacity_type = \u0026#34;SPOT\u0026#34; # 60-70% savings # or capacity_type = \u0026#34;ON_DEMAND\u0026#34; # Stable pricing 10.8. Free Tier Optimization 12-Month Free Tier Benefits:\n750 hours/month t2.micro EC2 instances 30 GB EBS General Purpose (SSD) storage 2 million Lambda requests (if used for automation) 1 GB CloudWatch Logs (first 5GB free) Always Free:\n1 million AWS Lambda requests per month 5 GB CloudWatch monitoring data S3 transfers within same region via VPC endpoints ğŸ’° Cost Summary for Task 7:\nFixed cost: $73/month (EKS control plane) Variable cost: $28.80/month (VPC endpoints) Free Tier savings: $20.20/month (instances + storage) Total: $81.60/month with free tier optimizations Production scaling: Add instance types based on workload needs Success tip: Monitor cluster costs with AWS Cost Explorer and set up billing alerts. Use eksctl or Terraform for infrastructure as code to ensure consistent and reproducible deployments.\nğŸ‘‰ Task 7 Results After Task 7, you will have a production-ready EKS Cluster, running completely in private subnets and integrated with VPC Endpoints from Task 5, saving NAT Gateway costs and increasing security.\nDeliverables Completed EKS Control Plane ACTIVE: Managed Kubernetes cluster with multi-AZ high availability IRSA Configured: OIDC provider and Service Account authentication setup Managed Node Groups: 2x t2.micro instances (FREE tier) distributed across â‰¥2 AZ VPC Endpoints Integration: Using ECR, S3 endpoints from Task 2 (70% cost savings) Core Add-ons: VPC CNI, CoreDNS, kube-proxy, metrics-server, EBS CSI driver Secure Pod Access: Pods can access S3/CloudWatch via IRSA (no hardcoded credentials) kubectl Access: Local development environment configured and tested Cost Optimization: $117.40/month saved with FREE tier + VPC Endpoints ğŸ¯ Ready for Next Tasks:\nEKS cluster foundation is ready for deployment:\nâœ… Task 5: EKS node groups scaling and optimization âœ… Task 6: ECR repository setup for container images âœ… Task 7: Build and push inference API container âœ… Task 8: S3 data storage integration âœ… Task 13: Deploy inference API to EKS cluster ğŸ” Security \u0026amp; Maintenance Notes:\nPublic Access: Limit cluster_endpoint_public_access_cidrs to actual IP ranges in production Logging: Enable all control plane logging for security audit Updates: Regularly update Kubernetes version and add-ons (quarterly) RBAC: Deploy appropriate role-based access control for team members Monitoring: Set up alerts for node health, pod failures, and resource usage Backup: Consider EKS cluster backup strategy for disaster recovery ğŸ¬ Task 7 Implementation Video Next Step: Task 08: Deploy Kubernetes\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.8-blog8/",
	"title": "AWS Clean Rooms launches privacy-enhancing synthetic dataset generation for ML model training",
	"tags": [],
	"description": "",
	"content": "Author: Micah Walter â€” 30 NOV 2025\nToday, weâ€™re announcing privacy-enhancing synthetic dataset generation for AWS Clean Rooms, a new capability that organizations and their partners can use to generate privacy-enhancing synthetic datasets from their collective data to train regression and classification machine learning (ML) models. You can use this feature to generate synthetic training datasets that preserve the statistical patterns of the original data, without the model having access to original records, opening new opportunities for model training that were previously not possible due to privacy concerns.\nWhen building ML models, data scientists and analysts typically face a fundamental tension between data utility and privacy protection. Access to high-quality, granular data is essential for training accurate models that can recognize trends, personalize experiences, and drive business outcomes. However, using granular data such as user-level event data from multiple parties raises significant privacy concerns and compliance challenges. Organizations want to answer questions like, â€œWhat characteristics indicate a high-probability customer conversion?â€, but training on the individual-level signals often conflicts with privacy policies and regulatory requirements.\nPrivacy-enhancing synthetic dataset generation for custom ML To address this challenge, weâ€™re introducing privacy-enhancing synthetic dataset generation in AWS Clean Rooms ML, which organizations can use to create synthetic versions of sensitive datasets that can be more securely used for ML model training. This capability uses advanced ML techniques to generate new datasets that maintain the statistical properties of the original data while de-identifying subjects from the original source data.\nTraditional anonymization techniques such as masking still carry the risk of re-identifying individuals in a datasetâ€”knowing attributes about a person such as zip code and date of birth can be sufficient to identify them with census data. Privacy-enhancing synthetic dataset generation addresses this risk through a fundamentally different approach. The system trains a model that learns the essential statistical patterns of the original dataset, then generates synthetic records by sampling values from the original dataset and using the model to predict the predicted value column. Rather than merely copying or perturbing the original data, the system uses a model capacity reduction technique to mitigate the risk that the model will memorize information about individuals in the training data. The resulting synthetic dataset has the same schema and statistical characteristics as the original data, making it suitable for training classification and regression models. This approach quantifiably reduces the risk of re-identification.\nOrganizations using this capability have control over the privacy parameters, including the amount of noise applied and the level of protection against membership inference attacks, where an adversary attempts to determine whether a specific individualâ€™s data was included in the training set. After generating the synthetic dataset, AWS Clean Rooms provides detailed metrics to help customers and their compliance teams understand the quality of the synthetic dataset across two critical dimensions: fidelity to the original data and privacy preservation. The fidelity score uses KL-divergence to measure how similar the synthetic data is to the original dataset, and the privacy score quantifies how likely the dataset is protected from membership inference attacks.\nWorking with synthetic data in AWS Clean Rooms Getting started with privacy-enhancing synthetic dataset generation follows the established AWS Clean Rooms ML custom models workflow, with new steps to specify privacy requirements and review quality metrics. Organizations begin by creating configured tables with analysis rules using their preferred data sources, then join or create a collaboration with their partners and associate their tables with that collaboration.\nThe new capability introduces an enhanced analysis template where data owners define not only the SQL query that creates the dataset but also specify that the resulting dataset must be synthetic. Within this template, organizations classify columns to indicate which column the ML model will predict and which columns contain categorical versus numerical values. Critically, the template also includes privacy thresholds that the generated synthetic data must meet to be made available for training. These include an epsilon value that specifies how much noise must be present in the synthetic data to protect against re-identification, and a minimum protection score against membership inference attacks. Setting these thresholds appropriately requires understanding your organizationâ€™s specific privacy and compliance requirements, and we recommend engaging with your legal and compliance teams during this process.\nAfter all data owners review and approve the analysis template, a collaboration member creates a machine learning input channel that references the template. AWS Clean Rooms then begins the synthetic dataset generation process, which typically completes within a few hours depending on the size and complexity of the dataset. If the generated synthetic dataset meets the required privacy thresholds defined in the analysis template, a synthetic machine learning input channel becomes available along with detailed quality metrics. Data scientists can review the actual protection score achieved against a simulated membership inference attack.\nOnce satisfied with the quality metrics, organizations can proceed to train their ML models using the synthetic dataset within the AWS Clean Rooms collaboration. Depending on the use case, they can export the trained model weights or continue to run inference jobs within the collaboration itself.\nLetâ€™s try it out When creating a new AWS Clean Rooms collaboration, I can now set who pays for synthetic dataset generation. After my Collaboration is configured, I can choose Require analysis template output to be synthetic when creating a new analysis template. After my synthetic analysis template is ready, I can use it when running protected queries and view all the relevant ML input channel details. Clean Rooms Synthetic Data Console\nNow available You can start using privacy-enhancing synthetic dataset generation through AWS Clean Rooms today. The feature is available in all commercial AWS Regions where AWS Clean Rooms is available. Learn more about it in the AWS Clean Rooms documentation.\nPrivacy-enhancing synthetic dataset generation is billed separately based on usage. You pay only for the compute used to generate your synthetic dataset, charged as Synthetic Data Generation Units (SDGUs). The number of SDGUs varies based on the size and complexity of your original dataset. This fee can be configured as a payer setting, meaning any collaboration member can agree to pay the costs. For more information on pricing, refer to the AWS Clean Rooms pricing page.\nThe initial release supports training classification and regression models on tabular data. The synthetic datasets work with standard ML frameworks and can be integrated into existing model development pipelines without requiring changes to your workflows.\nThis capability represents a significant advancement in privacy-enhanced machine learning. Organizations can unlock the value of sensitive user-level data for model training while mitigating the risk that sensitive information about individual users could be leaked. Whether youâ€™re optimizing advertising campaigns, personalizing insurance quotes, or enhancing fraud detection systems, privacy-enhancing synthetic dataset generation makes it possible to train more accurate models through data collaboration while respecting individual privacy.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.7-week7/",
	"title": "Week 7 - DataLake Service on AWS",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Build a Serverless Data Lake on S3. Implement ETL pipelines with Glue and analytics with Athena. Tasks to be carried out this week: Day Task Reference 2 - Data Lake: Design S3 layers (Raw, Staging, Curated).\n- Configure Partitioning strategy. Task 3 Guide 3 - Glue Catalog: setup Glue Crawlers to discover schema.\n- Build Glue Data Catalog. AWS Glue Docs 4 - ETL: Create Glue Jobs (Visual/Spark) to convert CSV to Parquet.\n- Implement data partitioning. AWS Glue Studio 5 - Analytics: Query data using Amazon Athena.\n- Visualize results with QuickSight (optional). AWS Athena Docs 6 - Review: Verify data pipeline efficiency and query performance.\n- Weekly Report. - Week 7 Achievements: Established a multi-layer Data Lake architecture. Optimized storage and query performance using Parquet format. Enabled ad-hoc SQL queries on S3 data using Athena. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/8-deploy-kubernetes/",
	"title": "API Deployment on EKS",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Má»¥c tiÃªu Task 9:\nğŸ¯ Objective of Task 9: Deploy the Retail Prediction API (FastAPI) to an EKS cluster, connect the model from S3, and expose a public endpoint via a Load Balancer (ALB). â†’ Ensure the service runs reliably, auto-scales, is secure, and can be demoed live.\nğŸ“¥ Inputs from previous tasks:\nTask 5 (Production VPC): VPC design, subnets, VPC Endpoints and ALB networking required for the cluster and load balancer Task 6 (ECR Container Registry): Container images and repository URIs to deploy Task 2 (IAM Roles \u0026amp; Audit): IRSA roles and policies for Pods to access S3 and other AWS services Task 7 (EKS Cluster): EKS cluster and node groups where manifests will be applied 1. Overview API Deployment is the step to deploy the prediction service containerized to Kubernetes (EKS). This ensures the application is deployed as a microservice, can auto-scale, and achieves high availability.\nDeployment architecture EKS Deployment Architecture:\nClient â†’ ALB â†’ EKS Service â†’ API Pods â†’ S3 Models\râ†“\rAuto-scaling (HPA) Components:\nNamespace: mlops ServiceAccount: IRSA service account for SageMaker access Deployment: API pods using the ECR Singapore image Service: LoadBalancer service HPA: Auto-scaling based on CPU 2. Kubernetes manifests Create the following 5 main files:\nnamespace.yaml - Create the mlops namespace serviceaccount.yaml - IRSA service account deployment.yaml - API deployment configured with the SageMaker registry service.yaml - LoadBalancer service hpa.yaml - Auto-scaling 2.1 Namespace configuration # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: mlops labels: app.kubernetes.io/name: retail-api --- 2.2 ServiceAccount with IRSA # serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: retail-api-sa namespace: mlops annotations: eks.amazonaws.com/role-arn: arn:aws:iam::842676018087:role/eks-sagemaker-access-role --- 2.3 Deployment configuration # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: retail-api namespace: mlops labels: app: retail-api spec: replicas: 2 selector: matchLabels: app: retail-api template: metadata: labels: app: retail-api spec: serviceAccountName: retail-api-sa containers: - name: retail-api image: 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest ports: - containerPort: 8000 env: - name: PORT value: \u0026#34;8000\u0026#34; - name: AWS_DEFAULT_REGION value: \u0026#34;ap-southeast-1\u0026#34; - name: MODEL_PACKAGE_GROUP value: \u0026#34;retail-price-sensitivity-models\u0026#34; resources: requests: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; livenessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 10 periodSeconds: 5 --- 3. Service (Load Balancer) # service.yaml apiVersion: v1 kind: Service metadata: name: retail-api-service namespace: mlops labels: app: retail-api spec: selector: app: retail-api ports: - name: http port: 80 targetPort: 8000 type: LoadBalancer 4. Auto-scaling (HPA) # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: retail-api-hpa namespace: mlops spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: retail-api minReplicas: 2 maxReplicas: 5 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 60 5. Deploy to EKS 5.1 Apply manifests # Deploy all manifests in order kubectl apply -f namespace.yaml kubectl apply -f serviceaccount.yaml kubectl apply -f deployment.yaml kubectl apply -f service.yaml kubectl apply -f hpa.yaml 5.2 Check deployment status # Check pod status kubectl get pods -n mlops # Check service and load balancer kubectl get svc -n mlops # Check horizontal pod autoscaler kubectl get hpa -n mlops # Check pod logs kubectl logs -l app=retail-api -n mlops --tail=50 5.3 Get the LoadBalancer URL and test the API # Get the LoadBalancer URL kubectl get svc retail-api-service -n mlops -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39; # Test the health check endpoint curl http://[LOAD_BALANCER_URL]/health # Test the API documentation curl http://[LOAD_BALANCER_URL]/docs # Test the prediction endpoint with a real data payload curl -X POST http://[LOAD_BALANCER_URL]/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;BASKET_SIZE\u0026#34;: \u0026#34;M\u0026#34;, \u0026#34;BASKET_TYPE\u0026#34;: \u0026#34;MIXED\u0026#34;, \u0026#34;STORE_REGION\u0026#34;: \u0026#34;LONDON\u0026#34;, \u0026#34;STORE_FORMAT\u0026#34;: \u0026#34;LS\u0026#34;, \u0026#34;SPEND\u0026#34;: 125.50, \u0026#34;QUANTITY\u0026#34;: 3, \u0026#34;PROD_CODE_20\u0026#34;: \u0026#34;FOOD\u0026#34;, \u0026#34;PROD_CODE_30\u0026#34;: \u0026#34;FRESH\u0026#34; }\u0026#39; 6. Check via the AWS Console 6.1 EKS Console - Check cluster status Open the EKS Console: AWS Console â†’ EKS â†’ Clusters â†’ mlops-retail-cluster Check the Resources tab: mlops-retail-cluster â†’ Resources â†’ All namespaces â†’ Filter: mlops 6.2 EKS workloads - Deployment details Check the Deployment: Resources â†’ Deployments â†’ retail-api Check the Pods: Click the Deployment â†’ Pods tab Pod status: Running (if Pending then there is a resource issue) Restart count: 0 (if \u0026gt; 0 there may have been crashes) 6.3 Debugging when Pods are Pending If Pods are Pending: Check the Events section for errors: Insufficient CPU/Memory: Need to scale nodes Image pull error: ECR permissions issue PodSecurityPolicy: IAM role issue If LoadBalancer times out / connection refused: Target Groups unhealthy: Pods are not passing the health check (/health endpoint) Security Groups: EKS worker nodes must allow inbound from the Load Balancer Subnets: Load Balancer requires at least 2 public subnets Check Events in the EKS Console: Resources â†’ Events â†’ Filter namespace: mlops Look for Warning/Error events related to the deployment 7. Testing and load testing 7.1 Local testing with port-forward # Port-forward the service to localhost (if the LoadBalancer is not ready) kubectl port-forward service/retail-api-service 8080:80 -n mlops # Test via port forward curl http://localhost:8080/health 7.2 Test SageMaker Model Registry integration # Check the model info endpoint curl http://[LOAD_BALANCER_URL]/model/info # Check model metrics from the SageMaker Registry curl http://[LOAD_BALANCER_URL]/model/metrics # Expected response: Accuracy 84.7%, F1-Score 83.2% from the Registry 7.3 Load testing to validate auto-scaling # Load test with the correct data format for i in {1..100}; do curl -X POST http://[LOAD_BALANCER_URL]/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;BASKET_SIZE\u0026#34;:\u0026#34;M\u0026#34;,\u0026#34;BASKET_TYPE\u0026#34;:\u0026#34;MIXED\u0026#34;,\u0026#34;STORE_REGION\u0026#34;:\u0026#34;LONDON\u0026#34;,\u0026#34;STORE_FORMAT\u0026#34;:\u0026#34;LS\u0026#34;,\u0026#34;SPEND\u0026#34;:125.50,\u0026#34;QUANTITY\u0026#34;:3,\u0026#34;PROD_CODE_20\u0026#34;:\u0026#34;FOOD\u0026#34;,\u0026#34;PROD_CODE_30\u0026#34;:\u0026#34;FRESH\u0026#34;}\u0026#39; \u0026amp; done # Monitor HPA scaling kubectl get hpa retail-api-hpa -n mlops -w # Monitor pods scaling up (from 2 â†’ up to 5) kubectl get pods -n mlops -w 8. Estimated costs Component Estimate Notes EKS Pod (2 replica on Spot) ~0.012 USD/h Compute cost ALB/NLB (public) ~0.02 USD/h Only enabled for the demo Total (1h demo) â‰ˆ 0.03â€“0.04 USD Very low if terminated after demo Cost estimates are based on Spot instances (t3.medium) and NLB in the ap-southeast-1 region. Actual costs may vary depending on the configuration and usage time.\nğŸ¯ Task 9 Complete - API Deployment on EKS\nKubernetes manifests ready EKS deployment configured with IRSA Load Balancer service for external access Auto-scaling with HPA 9. Clean up resources 9.1 Delete the deployment and resources # Delete all resources in the mlops namespace kubectl delete namespace mlops # Or delete individual resources kubectl delete deployment retail-api -n mlops kubectl delete service retail-api-service -n mlops kubectl delete hpa retail-api-hpa -n mlops kubectl delete serviceaccount retail-api-sa -n mlops # Check that the LoadBalancer has been removed aws elbv2 describe-load-balancers --query \u0026#39;LoadBalancers[?contains(LoadBalancerName, `k8s-mlops`)].LoadBalancerArn\u0026#39; 9.2 Delete ECR images (optional) # List images in the repository aws ecr describe-images --repository-name mlops/retail-api --region ap-southeast-1 # Delete a specific image tag aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --image-ids imageTag=v3 \\ --region ap-southeast-1 # Delete all images aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --image-ids \u0026#34;$(aws ecr describe-images --repository-name mlops/retail-api --region ap-southeast-1 --query \u0026#39;imageDetails[].imageDigest\u0026#39; --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | sed \u0026#39;s/.*/imageDigest=\u0026amp;/\u0026#39;)\u0026#34; \\ --region ap-southeast-1 9.3 Verify clean up # Ensure no pods remain kubectl get pods -n mlops # Ensure no services remain kubectl get svc -n mlops # Verify LoadBalancer termination aws elbv2 describe-load-balancers --query \u0026#39;LoadBalancers[?contains(LoadBalancerName, `k8s-mlops`)]\u0026#39; 10. Pricing for Kubernetes deployment (ap-southeast-1) 10.1 Pod resource costs Resource Type Request Limit Cost impact CPU 250m 500m ~25% of node CPU Memory 512Mi 1Gi ~25% of node memory Storage (EBS) - - Based on EBS pricing With t2.micro node (1 vCPU, 1GB RAM):\n1 API pod uses ~50% of the node resources Can run 2 pods with the specified resource requests Scaling is limited by node capacity 10.2 Load Balancer pricing Load Balancer Type Price (USD/hour) Price (USD/month) Data processing Classic LB $0.025 $18.25 $0.008/GB Application LB $0.0225 $16.43 $0.008/LCU-hour Network LB $0.0225 $16.43 $0.006/NLCU-hour 10.3 Service type costs Service Type AWS Resource Monthly Cost Use case ClusterIP None $0 Internal communication NodePort EC2 Security Groups $0 Development testing LoadBalancer ELB/ALB/NLB $16.43+ Production external access ExternalName None $0 External service mapping 10.4 Auto-scaling costs Horizontal Pod Autoscaler (HPA):\nHPA controller: free (part of EKS) Additional pods: EC2 instance costs Scaling triggers: CPU/Memory metrics (free) Cluster Autoscaler:\nController: free New nodes: full EC2 instance pricing Scale-down: automatic cost reduction 10.5 Task 8 cost estimate Basic deployment (2 replicas):\nComponent Quantity Resource usage Monthly cost API Pods 2 replicas 500m CPU, 1Gi RAM Included in node cost LoadBalancer Service 1 ALB Base + LCU usage $16.43 + usage HPA 1 autoscaler Controller only $0 Ingress Optional Same ALB $0 additional Total ~$16.43 + LCU With auto-scaling (2-5 replicas):\nScenario Pods Node requirements Additional cost Low load 2 pods 2x t2.micro (free) $0 Medium load 3-4 pods 1x t3.small $15.18 High load 5 pods 1x t3.medium $30.37 10.6 Data transfer costs Transfer type Cost Use case Pod-to-Pod (same AZ) Free Internal communication Pod-to-Pod (cross-AZ) $0.01/GB Multi-AZ deployment LoadBalancer to Internet $0.12/GB API responses to clients VPC Endpoints Free S3/ECR access 10.7 Storage costs for persistent volumes # Example PVC for model storage apiVersion: v1 kind: PersistentVolumeClaim metadata: name: model-storage namespace: mlops spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: gp3 Storage pricing:\n10GB gp3: $0.80/month Snapshots: $0.50/month (10GB) IOPS (if \u0026gt; 3000): $0.065/IOPS/month 10.8 Cost optimization for deployments Resource right-sizing:\nresources: requests: memory: \u0026#34;256Mi\u0026#34; # Start smaller cpu: \u0026#34;100m\u0026#34; # Minimal CPU request limits: memory: \u0026#34;512Mi\u0026#34; # Reasonable limit cpu: \u0026#34;250m\u0026#34; # Allow bursting Efficient pod scheduling:\n# Node affinity for cost optimization affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: kubernetes.io/instance-type operator: In values: [\u0026#34;t3.micro\u0026#34;, \u0026#34;t3.small\u0026#34;] # Prefer cheaper instances LoadBalancer optimization:\n# Use a single ALB for multiple services apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: shared-alb annotations: kubernetes.io/ingress.class: alb spec: rules: - http: paths: - path: /api/v1/* backend: service: name: retail-api-service port: number: 80 - path: /admin/* backend: service: name: admin-service port: number: 80 10.9 Monitoring costs # Monitor pod resource usage kubectl top pods -n mlops # Check actual vs requested resources kubectl describe pod \u0026lt;pod-name\u0026gt; -n mlops # Monitor HPA behavior kubectl get hpa -w -n mlops # Check LoadBalancer usage aws elbv2 describe-load-balancers --names \u0026lt;alb-name\u0026gt; Cost tracking commands:\n# ELB costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon Elastic Load Balancing\u0026#34;]}}\u0026#39; # EC2 costs for nodes aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=INSTANCE_TYPE ğŸ’° Cost summary for Task 8:\nPods: Included in node cost (no additional charge) LoadBalancer: $16.43/month base + usage Auto-scaling: $0-30.37/month depending on load Storage: $0.80/month per 10GB PVC Total: $17-47/month depending on scaling ğŸ¬ Video for Task 8 Next Step: Task 09: Elastic Load Balancing\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.9-blog9/",
	"title": "AWS Partner Central now available in AWS Management Console",
	"tags": [],
	"description": "",
	"content": "Author: SÃ©bastien Stormacq â€” 30 NOV 2025\nToday, weâ€™re announcing that AWS Partner Central is now available directly in the AWS Management Console, creating a unified experience that transforms how you engage with AWS as both customers and AWS Partners.\nAs someone who has worked with countless AWS customers over the years, Iâ€™ve observed how organizations evolve in their AWS journey. Many of our most successful Partners began as AWS customersâ€”first using our services to build their own infrastructure and solutions, then expanding to create offerings for others. Seeing this natural progression from customer to Partner, we recognized an opportunity to streamline these traditionally separate experiences into one unified journey.\nAs AWS evolved, so did the needs of our Partner community. Organizations today operate in multiple capacities: using AWS services for their own infrastructure while simultaneously building and delivering solutions for their customers. Modern businesses need streamlined workflows that support their growth from AWS customer to Partner to AWS Marketplace Seller, with enterprise-grade security features that match how they actually work with AWS today.\nA new unified console experience The integration of AWS Partner Central into the Console represents a fundamental shift in partnership accessibility. For existing AWS customers, such as you, becoming an AWS Partner is now as clear as accessing any other AWS service. The familiar console interface provides direct access to partnership opportunities, program benefits, and AWS Marketplace capabilities without needing separate logins or navigation between different systems.\nGetting started as an AWS Partner now takes only a few clicks within your existing console environment. You can discover partnership opportunities, understand program requirements, and begin your Partner journey without leaving the AWS interface you already know and trust.\nThe console integration creates an intuitive pathway for existing customers to transition into AWS Marketplace Sellers. You can now access AWS Marketplace Seller capabilities alongside your existing AWS services, managing both your infrastructure and AWS Marketplace business from a single interface. Private offer requests and negotiations can be managed directly within AWS Partner Central, and you can manage your AWS Marketplace listings alongside your other AWS activities through streamlined workflows.\nBecoming an AWS Partner The unified console experience provides access to comprehensive partnership benefits designed to accelerate your business growth.\nJoin the AWS Partner Network (APN) and complete your Partner and AWS Marketplace Seller requirements seamlessly within the same interface. Enroll in Partner Paths that align with your customer solutions to build, market, list, and sell in AWS Marketplace while growing alongside AWS. When you are established, use the Partner programs to differentiate your solution, list in AWS Marketplace to improve your go-to-market discoverability, and build AWS expertise through certifications to drive profitability by capturing new revenue streams. Scale your business by selling or reselling software and professional services in AWS Marketplace, helping you accelerate deals, boost revenue, and expand your customer reach to new geographies, industries, and segments.\nThroughout your journey, you can continue using Amazon Q in the console, which provides personalized guidance through AWS Partner Assistant.\nLetâ€™s see the new Partner Central console The new AWS Partner Central is accessible like any other AWS service from the console. Among many new capabilities, it provides four key sections that support Partner operations and business growth within the AWS Partner Network:\n1. It helps you sell your solutions AWS Partner Central - Solutions You can create and publish solutions that address specific customer needs through AWS Marketplace. Solutions are made up of products such as software as a service (SaaS), Amazon Machine Images (AMI), containers, professional services, AI agents and tools, and more. The solutions management capability guides you through building offerings that include both products you own and those you are authorized to resell. You can craft compelling value propositions and descriptions that clearly communicate your solution benefits to potential buyers browsing AWS Marketplace.\nI choose Create solution to start listing a new solution in the AWS Marketplace, as shown in the following figure.\nAWS Partner Central - Create solution 2. It helps you update and manage your Partner profile AWS Partner Central - Manage profile Your Partner profile showcases your organizationâ€™s expertise and capabilities to the AWS community. You control how your business appears to potential customers and Partners by highlighting the industry segments you serve and describing your primary products or services. Profile visibility settings provide you with the option to choose whether your information is public or private.\n3. It helps you track opportunities AWS Partner Central - Track Opportunities You can manage your pipeline of AWS customers, supporting joint collaborations with AWS on customer engagements. You monitor these prospects using clear status indicators: approved, rejected, draft, and pending approval. The opportunity dashboard shows stages, estimated AWS Monthly Recurring Revenue, and other key metrics that help you understand your pipeline. You can create more opportunities directly within the console and export data for your own reporting and analysis.\n4. It provides you with the ability to discover and connect with other Partners After becoming an AWS Partner, you get access to the AWS Partners network, where you can search for other Partners. You can connect with them to collaborate on sales opportunities and expand your customer outreach.\nAWS Partner Central - Discover and Search for partners You search through available Partners using filters for industry, location, Partner program type, and specialization. The centralized dashboard shows your active connections, pending requests, and connection history, so that you can manage business relationships and identify collaboration opportunities that can expand your reach. Like all other AWS services, these Partner connection capabilities are now available as APIs, which provide automation and integration into your existing workflows.\nAWS Partner Central - Manage contact requests These capabilities work together within the new AWS Partner Central console, accessible directly from the console, helping you transition from AWS customer to successful Partner with enterprise-grade security and streamlined workflows.\nThe technical foundation: Migrating the identity system This unified console experience is made possible by our migration to a modern identity system built on AWS Identity and Access Management (IAM). Weâ€™ve transitioned from legacy identity infrastructure to IAM Identity Center, providing enterprise-grade security capabilities including single sign-on capabilities and multi-factor authentication. With security as job zero, this migration provides new and existing Partners with the possibility to connect their own identity providers to AWS Partner Central. It provides seamless integration with existing enterprise authentication systems while removing the complexity of managing separate credentials across different services.\nOne more thing APIs are the core of what we do at AWS, and AWS Partner Central is no different. You can automate and streamline your co-sell workflows by connecting your business tools to AWS Partner Central. The APIs offered by AWS Partner Central help you accelerate APN benefitsâ€”from Account Management (Account API) and Solution Management (Solution API) to co-selling with Opportunity and Leads APIs, and Benefits APIs for faster benefit activation.\nYou can use these APIs to engage with AWS and grow your Partner business from your own CRM tools.\nGet started today This integration between the console and AWS Partner Central reflects our commitment to reducing complexity and improving the Partner experience. Weâ€™re bringing AWS Partner Central into the console to create a more intuitive path for organizations to grow with AWS from initial customer adoption through to full partnership engagement and AWS Marketplace success.\nYour journey from AWS customer to successful AWS Partner and AWS Marketplace Seller starts with a few clicks in your console. I encourage you to explore the new unified experience today and discover how AWS Partner Central in the console can accelerate your organizationâ€™s growth and success within the AWS community.\nReady to get started? Visit AWS Partner Central in your console to learn more about the AWS Partner Network and discover the partnership path thatâ€™s right for your organization.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.8-week8/",
	"title": "Week 8 - Serverless Compute",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Build event-driven applications using Lambda and API Gateway. Implement asynchronous processing with SQS/SNS. Tasks to be carried out this week: Day Task Reference 2 - Lambda: Develop functions (Python/Node.js) with environment variables.\n- Configure memory and timeouts. AWS Lambda Docs 3 - API Gateway: Create HTTP/REST APIs integrated with Lambda.\n- Setup CORS and Stages. AWS APIGW Docs 4 - Event-Driven: Configure S3 Event Notifications to trigger Lambda.\n- Setup EventBridge rules. AWS EventBridge 5 - Messaging: Implement SQS for decoupling and DLQ for error handling.\n- Use SNS for fan-out messaging. AWS SQS/SNS 6 - Review: Test end-to-end serverless flows.\n- Weekly Report. - Week 8 Achievements: Developed a serverless API using API Gateway and Lambda. Implemented robust error handling with SQS Dead Letter Queues. Reduced operational overhead by leveraging managed services. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/9-elastic-load-balancing/",
	"title": "Load Balancing",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Task 11 Objective:\nSet up load balancing for the Retail Prediction API, ensuring:\nA public endpoint to demo the API (/predict and /docs) Automatic traffic distribution across Pods when scaling High availability and security of the service ğŸ“¥ Input from previous Tasks:\nTask 5 (Production VPC): VPC subnets, security groups and VPC Endpoints required for ALB and EKS Task 7 (EKS Cluster): EKS cluster and Service/Ingress targets for ALB to forward traffic Task 6 (ECR Container Registry): Container images (API) deployed to EKS and exposed via ALB 1. Overview â€” Load Balancing for Retail Prediction API Load balancing is essential in a microservices architecture on AWS EKS, especially for the Retail Prediction API which may handle high and spiky request volumes. Load balancing provides:\nScalability: evenly distribute requests across Pods so the API remains responsive under high load High Availability: automatically detect and isolate unhealthy Pods to keep the service running Auto-scaling integration: work with HPA to adjust Pod counts based on real traffic Consistent endpoint: provide a single endpoint for clients (web/mobile) to access the prediction service Observability: collect metrics for traffic, latency and errors to monitor API performance 2. Setting up an Application Load Balancer (ALB) AWS Application Load Balancer (ALB) is a good choice for the Retail Prediction API because:\nOperates at Layer 7 (Application Layer) Supports routing requests based on path or host header Integrates with AWS WAF for API protection Supports SSL/TLS termination 2.1 Service Type LoadBalancer with ALB The simplest way to create an ALB is to use a Kubernetes Service of type LoadBalancer:\n# service-alb.yaml apiVersion: v1 kind: Service metadata: name: retail-api-service namespace: retail-prediction labels: app: retail-api annotations: # Specify the use of Application Load Balancer service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;application\u0026#34; service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \u0026#34;http\u0026#34; # Health check configuration service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: \u0026#34;/health\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: \u0026#34;30\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: \u0026#34;5\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: \u0026#34;2\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: \u0026#34;3\u0026#34; spec: type: LoadBalancer ports: - port: 80 targetPort: 8000 protocol: TCP name: http selector: app: retail-api --- When using the annotation service.beta.kubernetes.io/aws-load-balancer-type: \u0026quot;application\u0026quot;, EKS will automatically create an ALB instead of an NLB or Classic Load Balancer.\n2.2 Health Checks and Routing ALB continuously performs health checks against the /health endpoint to ensure only healthy Pods receive traffic:\nsequenceDiagram\rparticipant ALB as Application Load Balancer\rparticipant Pod1 as API Pod 1 (Healthy)\rparticipant Pod2 as API Pod 2 (Unhealthy)\rparticipant Pod3 as API Pod 3 (Healthy)\rALB-\u003e\u003ePod1: Health Check: GET /health\rPod1-\u003e\u003eALB: 200 OK: {\"status\": \"healthy\"}\rALB-\u003e\u003ePod2: Health Check: GET /health\rPod2-\u003e\u003eALB: 500 Error: Service Unavailable\rALB-\u003e\u003ePod3: Health Check: GET /health\rPod3-\u003e\u003eALB: 200 OK: {\"status\": \"healthy\"}\rNote over ALB,Pod2: Pod2 Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u unhealthy\rClient-\u003e\u003eALB: Request: POST /predict\rALB-\u003e\u003ePod1: Forward request\rPod1-\u003e\u003eALB: Response\rALB-\u003e\u003eClient: Return response\rClient-\u003e\u003eALB: Request: GET /docs\rALB-\u003e\u003ePod3: Forward request\rPod3-\u003e\u003eALB: Response\rALB-\u003e\u003eClient: Return response\rFlow explanation:\nALB continuously checks the health of all Pods via the /health endpoint Any Pod returning a non-200 status is marked unhealthy Client requests are only forwarded to healthy Pods When an unhealthy Pod becomes healthy again, it is automatically brought back into rotation This mechanism keeps the API highly available even when some Pods fail.\n2.3 Advanced ALB Configuration # service-alb-advanced.yaml apiVersion: v1 kind: Service metadata: name: retail-api-service namespace: retail-prediction labels: app: retail-api annotations: # Application Load Balancer configuration service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;application\u0026#34; service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \u0026#34;http\u0026#34; # Health check configuration service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: \u0026#34;/health\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: \u0026#34;8000\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: \u0026#34;HTTP\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: \u0026#34;20\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: \u0026#34;5\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: \u0026#34;2\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: \u0026#34;2\u0026#34; # Access logging (optional) service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: \u0026#34;true\u0026#34; service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: \u0026#34;retail-prediction-alb-logs\u0026#34; service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: \u0026#34;api-access-logs\u0026#34; # Target group configuration service.beta.kubernetes.io/aws-load-balancer-attributes: \u0026#34;idle_timeout.timeout_seconds=60\u0026#34; # Target type (IP mode preferred for pods) service.beta.kubernetes.io/aws-load-balancer-target-type: \u0026#34;ip\u0026#34; spec: type: LoadBalancer ports: - port: 80 targetPort: 8000 protocol: TCP name: http selector: app: retail-forecast-api externalTrafficPolicy: Local # Preserve source IP --- 3. AWS Load Balancer Controller for Ingress The AWS Load Balancer Controller is a Kubernetes controller that manages ALB (and NLB) for services in EKS. It enables more advanced ALB configuration via Ingress and IngressClass, rather than only using Service annotations.\n3.1 CÃ i Ä‘áº·t AWS Load Balancer Controller # Add eks-charts repository helm repo add eks https://aws.github.io/eks-charts helm repo update # Create IAM policy for the ALB controller curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json # Create IAM policy aws iam create-policy \\ --policy-name AWSLoadBalancerControllerIAMPolicy \\ --policy-document file://iam_policy.json # Create IAM role and service account using IRSA eksctl create iamserviceaccount \\ --cluster=retail-prediction-cluster \\ --namespace=kube-system \\ --name=aws-load-balancer-controller \\ --role-name AmazonEKSLoadBalancerControllerRole \\ --attach-policy-arn=arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:policy/AWSLoadBalancerControllerIAMPolicy \\ --approve # Install AWS Load Balancer Controller helm install aws-load-balancer-controller eks/aws-load-balancer-controller \\ -n kube-system \\ --set clusterName=retail-prediction-cluster \\ --set serviceAccount.create=false \\ --set serviceAccount.name=aws-load-balancer-controller The controller watches Service and Ingress resources and automatically creates and manages the corresponding ALBs on AWS. Confirm controller status with kubectl get pods -n kube-system | grep aws-load-balancer-controller.\n3.2 ALB Ingress Configuration for Retail Prediction API The Ingress configuration allows finer control over how the ALB handles traffic, including path-based routing, SSL termination and advanced features:\n# ingress-retail-prediction.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: retail-api-ingress namespace: retail-prediction annotations: # ALB Controller configuration kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/load-balancer-name: retail-prediction-alb # Health check configuration alb.ingress.kubernetes.io/healthcheck-path: /health alb.ingress.kubernetes.io/healthcheck-interval-seconds: \u0026#34;20\u0026#34; alb.ingress.kubernetes.io/healthcheck-timeout-seconds: \u0026#34;5\u0026#34; alb.ingress.kubernetes.io/healthy-threshold-count: \u0026#34;2\u0026#34; alb.ingress.kubernetes.io/unhealthy-threshold-count: \u0026#34;2\u0026#34; # SSL configuration alb.ingress.kubernetes.io/ssl-redirect: \u0026#34;443\u0026#34; alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}, {\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:\u0026lt;ACCOUNT_ID\u0026gt;:certificate/\u0026lt;CERT_ID\u0026gt; # Access logging alb.ingress.kubernetes.io/load-balancer-attributes: | access_logs.s3.enabled=true, access_logs.s3.bucket=retail-prediction-alb-logs, access_logs.s3.prefix=api-access-logs, idle_timeout.timeout_seconds=60 # Performance and deregistration configuration alb.ingress.kubernetes.io/target-group-attributes: | deregistration_delay.timeout_seconds=30, slow_start.duration_seconds=30, load_balancing.algorithm.type=least_outstanding_requests # WAF integration for API protection alb.ingress.kubernetes.io/wafv2-acl-arn: arn:aws:wafv2:us-east-1:\u0026lt;ACCOUNT_ID\u0026gt;:regional/webacl/retail-prediction-waf/\u0026lt;WAF_ID\u0026gt; # Tags for cost allocation alb.ingress.kubernetes.io/tags: Environment=Production,Project=RetailPrediction,Service=API spec: rules: - http: # Default rule for all incoming requests paths: # API Documentation - path: /docs pathType: Prefix backend: service: name: retail-api-service port: number: 80 # Swagger JSON endpoint - path: /openapi.json pathType: Exact backend: service: name: retail-api-service port: number: 80 # Health check and readiness endpoints - path: /health pathType: Exact backend: service: name: retail-api-service port: number: 80 # Prediction API endpoints - path: /predict pathType: Exact backend: service: name: retail-api-service port: number: 80 # Default catch-all for all other paths - path: / pathType: Prefix backend: service: name: retail-api-service port: number: 80 3.3 Advanced ALB with Multiple Paths # ingress-alb-advanced.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: retail-forecast-alb-advanced namespace: mlops annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip # Listen ports for HTTP and HTTPS alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}, {\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; # Actions for different paths alb.ingress.kubernetes.io/actions.weighted-routing: | { \u0026#34;type\u0026#34;: \u0026#34;forward\u0026#34;, \u0026#34;forwardConfig\u0026#34;: { \u0026#34;targetGroups\u0026#34;: [ { \u0026#34;serviceName\u0026#34;: \u0026#34;retail-forecast-service\u0026#34;, \u0026#34;servicePort\u0026#34;: 80, \u0026#34;weight\u0026#34;: 100 } ] } } # Rate limiting alb.ingress.kubernetes.io/target-group-attributes: | stickiness.enabled=false, load_balancing.algorithm.type=round_robin, slow_start.duration_seconds=30 spec: rules: - host: api.retail-forecast.com http: paths: # Health check endpoint - path: /healthz pathType: Exact backend: service: name: retail-forecast-service port: number: 80 # Prediction endpoints - path: /predict pathType: Exact backend: service: name: retail-forecast-service port: number: 80 # Batch prediction - path: /batch-predict pathType: Exact backend: service: name: retail-forecast-service port: number: 80 # Model information - path: /model-info pathType: Exact backend: service: name: retail-forecast-service port: number: 80 # Metrics endpoint - path: /metrics pathType: Exact backend: service: name: retail-forecast-service port: number: 80 # Default catch-all - path: / pathType: Prefix backend: service: name: retail-forecast-service port: number: 80 --- 4. HTTPS and SSL/TLS Configuration Securing the API with HTTPS is a critical requirement for any production service, especially when the prediction service may contain sensitive business data.\n4.1 Request an SSL Certificate with AWS Certificate Manager (ACM) # Request SSL certificate aws acm request-certificate \\ --domain-name api.retail-prediction.example.com \\ --validation-method DNS \\ --region us-east-1 # Get certificate ARN CERT_ARN=$(aws acm list-certificates --region us-east-1 --query \u0026#34;CertificateSummaryList[?DomainName==\u0026#39;api.retail-prediction.example.com\u0026#39;].CertificateArn\u0026#34; --output text) echo $CERT_ARN 4.2 Configure HTTPS for the ALB # service-alb-https.yaml apiVersion: v1 kind: Service metadata: name: retail-api-service namespace: retail-prediction annotations: # Basic ALB configuration service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;application\u0026#34; service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34; # SSL configuration service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \u0026#34;arn:aws:acm:us-east-1:123456789012:certificate/abcdef12-3456-7890-abcd-ef1234567890\u0026#34; service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \u0026#34;443\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \u0026#34;http\u0026#34; service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: \u0026#34;ELBSecurityPolicy-TLS-1-2-2017-01\u0026#34; # Redirect HTTP to HTTPS service.beta.kubernetes.io/aws-load-balancer-ssl-redirect: \u0026#34;443\u0026#34; # Other configurations... spec: type: LoadBalancer ports: - port: 443 targetPort: 8000 protocol: TCP name: https - port: 80 targetPort: 8000 protocol: TCP name: http selector: app: retail-api sequenceDiagram\rparticipant Client\rparticipant ALB as Application Load Balancer\rparticipant API as API Pod\rClient-\u003e\u003eALB: HTTPS Request (port 443)\rNote over ALB: SSL Termination\rALB-\u003e\u003eAPI: HTTP Request (port 8080)\rAPI-\u003e\u003eALB: HTTP Response\rALB-\u003e\u003eClient: HTTPS Response\rClient-\u003e\u003eALB: HTTP Request (port 80)\rNote over ALB: HTTP to HTTPS Redirect\rALB-\u003e\u003eClient: 301 Redirect to HTTPS\rClient-\u003e\u003eALB: HTTPS Request (port 443)\rNote over ALB: SSL Termination\rALB-\u003e\u003eAPI: HTTP Request (port 8080)\rAPI-\u003e\u003eALB: HTTP Response\rALB-\u003e\u003eClient: HTTPS Response\r4.3 Route 53 DNS Setup # Get the ALB DNS name ALB_DNS=$(kubectl get service retail-api-service -n retail-prediction -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) # Create Route 53 record aws route53 change-resource-record-sets \\ --hosted-zone-id YOUR_HOSTED_ZONE_ID \\ --change-batch \u0026#39;{ \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;CREATE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;api.retail-prediction.example.com\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;CNAME\u0026#34;, \u0026#34;TTL\u0026#34;: 300, \u0026#34;ResourceRecords\u0026#34;: [{\u0026#34;Value\u0026#34;: \u0026#34;\u0026#39;$ALB_DNS\u0026#39;\u0026#34;}] } }] }\u0026#39; 4.4 Verify HTTPS is working # Test HTTPS endpoint curl -k https://api.retail-prediction.example.com/health # Verify with proper SSL validation curl https://api.retail-prediction.example.com/health # Test HTTP to HTTPS redirect curl -v http://api.retail-prediction.example.com/health # Expected: HTTP/1.1 301 Moved Permanently 5. Security for the Retail Prediction API Securing the prediction API is important because it may contain sensitive sales and business strategy information. We\u0026rsquo;ll implement multiple security layers:\n5.1 AWS WAF (Web Application Firewall) WAF protects the API from common attacks such as SQL Injection, XSS and DDoS:\n# Create WAF Web ACL aws wafv2 create-web-acl \\ --name retail-prediction-waf \\ --scope REGIONAL \\ --region us-east-1 \\ --default-action Allow={} \\ --visibility-config SampledRequestsEnabled=true,CloudWatchMetricsEnabled=true,MetricName=RetailAPI \\ --rules file://waf-rules.json Contents of waf-rules.json: [ { \u0026#34;Name\u0026#34;: \u0026#34;API-RateLimit\u0026#34;, \u0026#34;Priority\u0026#34;: 0, \u0026#34;Statement\u0026#34;: { \u0026#34;RateBasedStatement\u0026#34;: { \u0026#34;Limit\u0026#34;: 3000, \u0026#34;AggregateKeyType\u0026#34;: \u0026#34;IP\u0026#34; } }, \u0026#34;Action\u0026#34;: { \u0026#34;Block\u0026#34;: {} }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;SampledRequestsEnabled\u0026#34;: true, \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;API-RateLimit\u0026#34; } }, { \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesSQLiRuleSet\u0026#34;, \u0026#34;Priority\u0026#34;: 1, \u0026#34;Statement\u0026#34;: { \u0026#34;ManagedRuleGroupStatement\u0026#34;: { \u0026#34;VendorName\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesSQLiRuleSet\u0026#34; } }, \u0026#34;OverrideAction\u0026#34;: { \u0026#34;None\u0026#34;: {} }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;SampledRequestsEnabled\u0026#34;: true, \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;SQLiRuleSet\u0026#34; } }, { \u0026#34;Name\u0026#34;: \u0026#34;BlockHighRiskRequests\u0026#34;, \u0026#34;Priority\u0026#34;: 2, \u0026#34;Statement\u0026#34;: { \u0026#34;ManagedRuleGroupStatement\u0026#34;: { \u0026#34;VendorName\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesKnownBadInputsRuleSet\u0026#34; } }, \u0026#34;OverrideAction\u0026#34;: { \u0026#34;None\u0026#34;: {} }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;SampledRequestsEnabled\u0026#34;: true, \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;BadInputs\u0026#34; } } ] 5.2 Security Group Configuration # Create a security group for the ALB aws ec2 create-security-group \\ --group-name retail-prediction-alb-sg \\ --description \u0026#34;Security group for Retail Prediction API ALB\u0026#34; \\ --vpc-id vpc-0123456789abcdef0 # Allow HTTPS traffic (port 443) aws ec2 authorize-security-group-ingress \\ --group-id sg-0123456789abcdef0 \\ --protocol tcp \\ --port 443 \\ --cidr 0.0.0.0/0 \\ --description \u0026#34;HTTPS from anywhere\u0026#34; # Allow HTTP traffic (port 80) - only for redirect to HTTPS aws ec2 authorize-security-group-ingress \\ --group-id sg-0123456789abcdef0 \\ --protocol tcp \\ --port 80 \\ --cidr 0.0.0.0/0 \\ --description \u0026#34;HTTP from anywhere (for redirect only)\u0026#34; 5.3 API Authorization with JWT Authentication To protect the API from unauthorized access implement JWT authentication:\n# Add the following code to your FastAPI `main.py` from fastapi import Depends, FastAPI, HTTPException, status from fastapi.security import OAuth2PasswordBearer import jwt from jwt.exceptions import PyJWTError # Setup OAuth2 oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\u0026#34;token\u0026#34;) # JWT Configuration JWT_SECRET_KEY = os.getenv(\u0026#34;JWT_SECRET_KEY\u0026#34;, \u0026#34;your-secret-key\u0026#34;) # NÃªn lÆ°u trong AWS Secrets Manager JWT_ALGORITHM = \u0026#34;HS256\u0026#34; # Verify JWT token def get_current_user(token: str = Depends(oauth2_scheme)): credentials_exception = HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=\u0026#34;Could not validate credentials\u0026#34;, headers={\u0026#34;WWW-Authenticate\u0026#34;: \u0026#34;Bearer\u0026#34;}, ) try: payload = jwt.decode(token, JWT_SECRET_KEY, algorithms=[JWT_ALGORITHM]) username: str = payload.get(\u0026#34;sub\u0026#34;) if username is None: raise credentials_exception return {\u0026#34;username\u0026#34;: username} except PyJWTError: raise credentials_exception # Protected API endpoint @app.post(\u0026#34;/predict\u0026#34;, response_model=PredictionResponse) async def predict( request: PredictionRequest, current_user: dict = Depends(get_current_user) ): # Log the user who made the prediction logger.info(f\u0026#34;Prediction requested by user: {current_user[\u0026#39;username\u0026#39;]}\u0026#34;) # Process prediction as normal prediction = model.predict(request.features) return {\u0026#34;prediction\u0026#34;: float(prediction), \u0026#34;model_version\u0026#34;: model.version} sequenceDiagram\rparticipant Client\rparticipant ALB as ALB + WAF\rparticipant API as Retail API\rparticipant Auth as Authentication Service\rClient-\u003e\u003eAuth: Request JWT token (username/password)\rAuth-\u003e\u003eAuth: Verify credentials\rAuth-\u003e\u003eClient: Return JWT token\rClient-\u003e\u003eALB: POST /predict with Bearer token\rALB-\u003e\u003eALB: WAF rules check\rALB-\u003e\u003eAPI: Forward request\rAPI-\u003e\u003eAPI: Validate JWT token\rAPI-\u003e\u003eAPI: Perform prediction\rAPI-\u003e\u003eALB: Return prediction result\rALB-\u003e\u003eClient: Return response\rClient-\u003e\u003eALB: POST /predict (no token)\rALB-\u003e\u003eALB: WAF rules check\rALB-\u003e\u003eAPI: Forward request\rAPI-\u003e\u003eALB: 401 Unauthorized\rALB-\u003e\u003eClient: 401 Unauthorized\r6. Deployment and Verification 6.1 Deploy Load Balancer Services # Create mlops namespace if not exists kubectl create namespace mlops # Deploy LoadBalancer service kubectl apply -f service-nlb-advanced.yaml # Or deploy ALB Ingress kubectl apply -f ingress-alb.yaml # Check service status kubectl get services -n mlops # Check ingress status kubectl get ingress -n mlops 6.2 Verify External Access # Get external IP/hostname EXTERNAL_IP=$(kubectl get service retail-forecast-nlb -n mlops -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) echo \u0026#34;External endpoint: $EXTERNAL_IP\u0026#34; # Test health check curl http://$EXTERNAL_IP/healthz # Test prediction endpoint curl -X POST http://$EXTERNAL_IP/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;features\u0026#34;: { \u0026#34;store_id\u0026#34;: 1, \u0026#34;product_id\u0026#34;: 123, \u0026#34;date\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;price\u0026#34;: 29.99, \u0026#34;promotion\u0026#34;: 0 } }\u0026#39; 7. Load Testing and Performance 7.1 Load Testing with Apache Bench # Install Apache Bench # For Ubuntu/Debian: sudo apt-get install apache2-utils # For CentOS/RHEL: sudo yum install httpd-tools # Basic load test ab -n 1000 -c 10 http://$EXTERNAL_IP/healthz # POST request load test ab -n 100 -c 5 -p data.json -T application/json http://$EXTERNAL_IP/predict 7.2 Load Testing with Artillery # Install Artillery npm install -g artillery # Create load test configuration cat \u0026gt; load-test.yml \u0026lt;\u0026lt; EOF config: target: \u0026#39;http://$EXTERNAL_IP\u0026#39; phases: - duration: 60 arrivalRate: 10 - duration: 120 arrivalRate: 20 - duration: 60 arrivalRate: 5 scenarios: - name: \u0026#34;Health Check\u0026#34; weight: 30 flow: - get: url: \u0026#34;/healthz\u0026#34; - name: \u0026#34;Prediction\u0026#34; weight: 70 flow: - post: url: \u0026#34;/predict\u0026#34; json: features: store_id: 1 product_id: 123 date: \u0026#34;2024-01-01\u0026#34; price: 29.99 promotion: 0 EOF # Run load test artillery run load-test.yml 8. Monitoring and Observability 8.1 CloudWatch Metrics # Create CloudWatch dashboard for ALB metrics aws cloudwatch put-dashboard \\ --dashboard-name \u0026#34;RetailForecastALB\u0026#34; \\ --dashboard-body \u0026#39;{ \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/ApplicationELB\u0026#34;, \u0026#34;RequestCount\u0026#34;, \u0026#34;LoadBalancer\u0026#34;, \u0026#34;retail-forecast-alb\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;TargetResponseTime\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;HTTPCode_Target_2XX_Count\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;HTTPCode_Target_4XX_Count\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;HTTPCode_Target_5XX_Count\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;ALB Metrics\u0026#34; } } ] }\u0026#39; 8.2 Prometheus Monitoring # servicemonitor-loadbalancer.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: retail-forecast-lb-monitor namespace: mlops labels: app: retail-forecast-api spec: selector: matchLabels: app: retail-forecast-api endpoints: - port: http path: /metrics interval: 30s scrapeTimeout: 10s --- 9. Troubleshooting 9.1 Common Load Balancer Issues # Check service events kubectl describe service retail-forecast-nlb -n mlops # Check ingress events kubectl describe ingress retail-forecast-alb-ingress -n mlops # Check ALB controller logs kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller # Check target group health aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/k8s-mlops-retailfo-1234567890 9.2 Debug Network Connectivity # Test from inside cluster kubectl run debug-pod --image=nicolaka/netshoot -n mlops --rm -it -- /bin/bash # Inside the debug pod: # Test service connectivity curl http://retail-forecast-service.mlops.svc.cluster.local/healthz # Test DNS resolution nslookup retail-forecast-service.mlops.svc.cluster.local # Test external connectivity curl http://api.retail-forecast.com/healthz 10. Cost Optimization 10.1 ALB vs NLB Cost Comparison # ALB pricing (approximate) # - $0.0225 per ALB-hour # - $0.008 per LCU-hour (Load Balancer Capacity Unit) # NLB pricing (approximate) # - $0.0225 per NLB-hour # - $0.006 per NLCU-hour (Network Load Balancer Capacity Unit) # Monitor usage with AWS Cost Explorer aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE 10.2 Resource Optimization # Use target type \u0026#34;ip\u0026#34; for better performance apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/aws-load-balancer-target-type: \u0026#34;ip\u0026#34; # Enable cross-zone load balancing only if needed service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \u0026#34;true\u0026#34; spec: # Use Local traffic policy to reduce network hops externalTrafficPolicy: Local --- Expected Results âœ… Completion Checklist Application Load Balancer: Service type LoadBalancer with annotation aws-load-balancer-type: \u0026quot;application\u0026quot; deployed successfully External Endpoint: ALB DNS assigned and accessible Health Checks: ALB health checks against /health are functioning correctly API Access: Endpoints /predict and /docs are reachable from the internet Logging: Access logs are written to an S3 bucket SSL/TLS: HTTPS endpoint configured (optional) Domain Name: Custom domain mapped via Route 53 (optional) Security: WAF rules and security groups configured Monitoring: CloudWatch ALB metrics enabled Load Testing: Performance testing meets throughput and latency targets ğŸ“Š Verification Steps Service trong namespace retail-prediction cÃ³ EXTERNAL-IP hiá»ƒn thá»‹\nkubectl get services -n retail-prediction # Expected output: # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # retail-api-service LoadBalancer 10.100.123.45 retail-alb-123456789.us-east-1.elb.amazonaws.com 80:31234/TCP 5m CÃ³ thá»ƒ gá»­i request tá»« bÃªn ngoÃ i vÃ  nháº­n káº¿t quáº£ inference\n# Health check curl http://EXTERNAL-IP/health # Expected: {\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-01T12:00:00Z\u0026#34;} # API Documentation curl http://EXTERNAL-IP/docs # Expected: FastAPI Swagger UI loads # Prediction request curl -X POST http://EXTERNAL-IP/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;features\u0026#34;: {\u0026#34;store_id\u0026#34;: 1, \u0026#34;product_id\u0026#34;: 123, \u0026#34;date\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;price\u0026#34;: 29.99, \u0026#34;promotion\u0026#34;: 0}}\u0026#39; # Expected: {\u0026#34;prediction\u0026#34;: 150.75, \u0026#34;model_version\u0026#34;: \u0026#34;v1.0\u0026#34;} Load Balancer hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh vÃ  phÃ¢n phá»‘i traffic\n# Check target health aws elbv2 describe-target-health --target-group-arn \u0026lt;target-group-arn\u0026gt; # Expected: All targets should be \u0026#34;healthy\u0026#34; # Monitor load distribution kubectl logs -f deployment/retail-api -n retail-prediction # Should see requests distributed across different pods # Check ALB metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name RequestCount \\ --dimensions Name=LoadBalancer,Value=app/retail-alb/1234567890123456 \\ --start-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34; -d \u0026#34;1 hour ago\u0026#34;) \\ --end-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) \\ --period 300 \\ --statistics Sum ğŸ” Monitoring Commands # Check service status kubectl get svc -n retail-prediction -w # Monitor ALB controller status kubectl get deployment -n kube-system aws-load-balancer-controller # Check load balancer controller logs kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -f # Monitor target group health watch \u0026#34;aws elbv2 describe-target-health --target-group-arn \u0026lt;your-target-group-arn\u0026gt;\u0026#34; # Get ALB details aws elbv2 describe-load-balancers --names retail-api-alb # Check ALB metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name RequestCount \\ --dimensions Name=LoadBalancer,Value=app/retail-api-alb/1234567890123456 \\ --start-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34; -d \u0026#34;1 hour ago\u0026#34;) \\ --end-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) \\ --period 300 \\ --statistics Sum # Check target response time metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name TargetResponseTime \\ --dimensions Name=LoadBalancer,Value=app/retail-api-alb/1234567890123456 \\ --start-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34; -d \u0026#34;1 hour ago\u0026#34;) \\ --end-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) \\ --period 300 \\ --statistics Average 11. Clean Up Resources (AWS CLI) 11.1. XÃ³a Load Balancers # Liá»‡t kÃª ALBs aws elbv2 describe-load-balancers --query \u0026#39;LoadBalancers[?contains(LoadBalancerName, `retail`)].{Name:LoadBalancerName,ARN:LoadBalancerArn}\u0026#39; --output table # XÃ³a ALB (tá»± Ä‘á»™ng xÃ³a listeners) aws elbv2 delete-load-balancer --load-balancer-arn \u0026lt;alb-arn\u0026gt; # Liá»‡t kÃª Target Groups aws elbv2 describe-target-groups --query \u0026#39;TargetGroups[?contains(TargetGroupName, `retail`)].{Name:TargetGroupName,ARN:TargetGroupArn}\u0026#39; --output table # XÃ³a Target Groups aws elbv2 delete-target-group --target-group-arn \u0026lt;target-group-arn\u0026gt; 11.2. XÃ³a AWS Load Balancer Controller # Uninstall AWS Load Balancer Controller helm uninstall aws-load-balancer-controller -n kube-system # XÃ³a IRSA role cho load balancer controller aws iam detach-role-policy \\ --role-name AmazonEKSLoadBalancerControllerRole \\ --policy-arn arn:aws:iam::\u0026lt;account-id\u0026gt;:policy/AWSLoadBalancerControllerIAMPolicy aws iam delete-role --role-name AmazonEKSLoadBalancerControllerRole # XÃ³a IAM policy aws iam delete-policy --policy-arn arn:aws:iam::\u0026lt;account-id\u0026gt;:policy/AWSLoadBalancerControllerIAMPolicy 11.3. XÃ³a Ingress Resources # XÃ³a Ingress resources kubectl delete ingress --all -n mlops # XÃ³a IngressClass kubectl delete ingressclass alb # Verify cleanup kubectl get ingress -A kubectl get ingressclass 11.4. XÃ³a WAF vÃ  Security Configurations # Liá»‡t kÃª WAF Web ACLs aws wafv2 list-web-acls --scope REGIONAL --region ap-southeast-1 # XÃ³a WAF rules trÆ°á»›c aws wafv2 update-web-acl \\ --scope REGIONAL \\ --id \u0026lt;web-acl-id\u0026gt; \\ --lock-token \u0026lt;lock-token\u0026gt; \\ --rules \u0026#39;[]\u0026#39; \\ --default-action Allow={} # XÃ³a WAF Web ACL aws wafv2 delete-web-acl \\ --scope REGIONAL \\ --id \u0026lt;web-acl-id\u0026gt; \\ --lock-token \u0026lt;lock-token\u0026gt; 11.5. Clean Up SSL Certificates # Liá»‡t kÃª ACM certificates aws acm list-certificates --region ap-southeast-1 --query \u0026#39;CertificateSummaryList[*].{Domain:DomainName,ARN:CertificateArn}\u0026#39; # XÃ³a certificate (chá»‰ khi khÃ´ng cÃ²n sá»­ dá»¥ng) aws acm delete-certificate --certificate-arn \u0026lt;certificate-arn\u0026gt; --region ap-southeast-1 # XÃ³a Route53 records aws route53 change-resource-record-sets \\ --hosted-zone-id \u0026lt;zone-id\u0026gt; \\ --change-batch \u0026#39;{ \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;DELETE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;api.retail-prediction.example.com\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;DNSName\u0026#34;: \u0026#34;\u0026lt;alb-dns-name\u0026gt;\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false, \u0026#34;HostedZoneId\u0026#34;: \u0026#34;\u0026lt;alb-zone-id\u0026gt;\u0026#34; } } }] }\u0026#39; 11.6. Load Balancing Cleanup Script #!/bin/bash # loadbalancer-cleanup.sh NAMESPACE=\u0026#34;mlops\u0026#34; CLUSTER_NAME=\u0026#34;retail-prediction-cluster\u0026#34; REGION=\u0026#34;ap-southeast-1\u0026#34; echo \u0026#34;ğŸ§¹ Cleaning up Load Balancer resources...\u0026#34; # 1. Delete Kubernetes resources echo \u0026#34;Deleting Ingress resources...\u0026#34; kubectl delete ingress --all -n $NAMESPACE kubectl delete service --selector=app=retail-api -n $NAMESPACE # 2. Wait for ALB deletion echo \u0026#34;Waiting for ALB deletion...\u0026#34; sleep 60 # 3. Clean up AWS resources echo \u0026#34;Cleaning up AWS Load Balancer Controller...\u0026#34; helm uninstall aws-load-balancer-controller -n kube-system 2\u0026gt;/dev/null || true # 4. Delete remaining target groups echo \u0026#34;Cleaning up target groups...\u0026#34; TARGET_GROUPS=$(aws elbv2 describe-target-groups --query \u0026#39;TargetGroups[?contains(TargetGroupName, `k8s-`)].TargetGroupArn\u0026#39; --output text) for tg in $TARGET_GROUPS; do aws elbv2 delete-target-group --target-group-arn $tg 2\u0026gt;/dev/null || true done echo \u0026#34;âœ… Load Balancer cleanup completed\u0026#34; 12. Báº£ng giÃ¡ Load Balancing (ap-southeast-1) 12.1. Chi phÃ­ Application Load Balancer (ALB) Component GiÃ¡ (USD/hour) GiÃ¡ (USD/month) Ghi chÃº ALB Base Cost $0.0225 $16.43 Per ALB instance LCU (Load Balancer Capacity Unit) $0.008 $5.84 Per LCU-hour New connections Included Included Up to 25/second per LCU Active connections Included Included Up to 3,000 per LCU Data processed Included Included Up to 1GB per LCU Rule evaluations Included Included Up to 1,000 per LCU 12.2. Chi phÃ­ Network Load Balancer (NLB) Component GiÃ¡ (USD/hour) GiÃ¡ (USD/month) Ghi chÃº NLB Base Cost $0.0225 $16.43 Per NLB instance NLCU (Network LCU) $0.006 $4.38 Per NLCU-hour New connections/flows Included Included Up to 800/second per NLCU Active connections/flows Included Included Up to 100,000 per NLCU Data processed Included Included Up to 1GB per NLCU 12.3. Chi phÃ­ Classic Load Balancer (CLB) Component GiÃ¡ (USD/hour) GiÃ¡ (USD/month) Ghi chÃº CLB Base Cost $0.025 $18.25 Per CLB instance Data Transfer $0.008/GB Variable Data processed 12.4. So sÃ¡nh cÃ¡c loáº¡i Load Balancer Feature ALB NLB CLB Best For Layer Layer 7 (HTTP/HTTPS) Layer 4 (TCP/UDP) Layer 4/7 ALB: Web apps, NLB: High performance Base Cost $16.43/month $16.43/month $18.25/month ALB/NLB cheaper Capacity Units LCU ($5.84) NLCU ($4.38) Fixed NLB most cost-effective SSL Termination âœ… âœ… âœ… All support Path-based Routing âœ… âŒ âŒ ALB only WebSocket âœ… âœ… âŒ ALB/NLB Static IP âŒ âœ… âŒ NLB only 12.5. AWS Load Balancer Controller Costs Component Cost Ghi chÃº Controller Pods Free Runs on existing EKS nodes IRSA Role Free IAM integration Webhook Certificate Free TLS for admission controller Target Group Binding Free CRD for pod registration Ingress Management Free Kubernetes native 12.6. SSL/TLS Certificate Costs Service Cost Features AWS Certificate Manager (ACM) Free Public SSL certificates Route 53 DNS $0.50/hosted zone Domain validation Third-party Certificates $10-100/year Extended validation options 12.7. WAF (Web Application Firewall) Costs Component GiÃ¡ (USD/month) Ghi chÃº WAF Web ACL $1 Per Web ACL WAF Rules $0.60 Per rule per month WAF Requests $0.60 Per million requests Bot Control $10 Advanced bot protection Rate Limiting $2 Per rate-based rule 12.8. Æ¯á»›c tÃ­nh chi phÃ­ Task 9 Basic ALB Setup:\nComponent Usage Monthly Cost ALB Base 1 ALB $16.43 LCU Usage 1 LCU average $5.84 ACM Certificate 1 domain $0 Route 53 1 hosted zone $0.50 WAF (optional) Basic rules $3.20 Total $26.00 Production Setup vá»›i High Availability:\nComponent Usage Monthly Cost ALB Base 1 ALB (Multi-AZ) $16.43 LCU Usage 3 LCU average $17.52 ACM Certificate 2 domains $0 Route 53 2 hosted zones $1.00 WAF Advanced rules $15.20 CloudFront CDN integration $8.50 Total $58.65 12.9. Cost Optimization Strategies Single ALB for Multiple Services:\n# Cost-effective: 1 ALB serves multiple applications apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: shared-alb-ingress annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/group.name: shared-alb spec: rules: - host: api.retail.com http: paths: - path: /predict backend: service: name: retail-api port: number: 80 - host: admin.retail.com http: paths: - path: / backend: service: name: admin-dashboard port: number: 80 Right-size Load Balancer:\nMonitor LCU/NLCU usage vá»›i CloudWatch Optimize connection pooling Use appropriate health check intervals Configure proper idle timeouts 12.10. Monitoring vÃ  Cost Control # Monitor ALB metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name ConsumedLCUs \\ --dimensions Name=LoadBalancer,Value=\u0026lt;alb-full-name\u0026gt; \\ --start-time 2024-01-01T00:00:00Z \\ --end-time 2024-01-31T23:59:59Z \\ --period 3600 \\ --statistics Average,Maximum # Check ALB costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon Elastic Load Balancing\u0026#34;]}}\u0026#39; # Monitor target health aws elbv2 describe-target-health \\ --target-group-arn \u0026lt;target-group-arn\u0026gt; \\ --query \u0026#39;TargetHealthDescriptions[*].{Target:Target.Id,Health:TargetHealth.State}\u0026#39; Cost alerts:\n# Create cost alarm for Load Balancing aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;LoadBalancer-Cost-Alert\u0026#34; \\ --alarm-description \u0026#34;Alert when Load Balancer cost \u0026gt; $50/month\u0026#34; \\ --metric-name EstimatedCharges \\ --namespace AWS/Billing \\ --statistic Maximum \\ --period 86400 \\ --threshold 50 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=Currency,Value=USD Name=ServiceName,Value=AmazonELB ğŸ’° Cost Summary cho Task 9:\nBasic ALB: $26/month (single service) Shared ALB: $22/month (multiple services sharing 1 ALB) Production: $58.65/month (vá»›i WAF, CloudFront) Optimization: 15-30% savings vá»›i proper resource sharing ğŸ¬ Video thá»±c hiá»‡n Task 9 Next Step: Task 10: CloudWatch Monitoring \u0026amp; Alerting\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.10-blog10/",
	"title": "Announcing AWS Partner Central in the AWS Management Console",
	"tags": [],
	"description": "",
	"content": "Author: Raj Kandaswamy, Nicole Schreiber, and Christin Voytko â€” 30 NOV 2025\nBy Raj Kandaswamy, Principal PMT-ES â€“ AWS\nBy Nicole Schreiber, Partner Experience Lead â€“ AWS\nBy Christin Voytko, Partner Experience Lead â€“ AWS\nWeâ€™re excited to announce that AWS Partner Central is now available in the AWS Management Console, simplifying access to Partner Central and the AWS Marketplace Management Portal. This launch introduces powerful APIs that offer process automation and integration capabilities, helping Partners connect their business systems directly to Partner Central, and streamlining how they grow and scale with Amazon Web Services (AWS).\nBuilt on AWS Identity and Access Management (IAM), this enhanced experience offers enterprise-grade security features and flexible user management and single sign-on (SSO) capabilitiesâ€”making it easier to give your teams access to the tools they need for running your AWS business.\nWhatâ€™s new in the enhanced Partner Central experience The new Partner Central experience is built to support teams as they scale with AWS. By unifying access and systems, teams can more quickly navigate to the tools they need to efficiently co-sell with AWS and other Partners. This enhanced experience enables you to:\nTransform operations with powerful integration and automation Connect your existing business systems directly to Partner Central through expanded APIs, enabling seamless integration and workflow automation. Eliminate duplicate data entry between co-selling and AWS Marketplace transactions, and streamline operations across solution management, benefit applications, and funding requests. Partner Central APIs include:\nAccount and Connections APIs for programmatic Partner registration, profile management, and multi-Partner collaboration through joint opportunities. Solution APIs for creating and managing solution listings in AWS Marketplace. Benefits APIs for centralized benefit discovery and application of benefits across Partner programs. Selling and Leads APIs now support the complete co-sell lifecycleâ€”from lead creation and qualification to opportunity conversionâ€”with real-time deal progression tracking. Partners are already leveraging these APIs to streamline operations. â€œLeveraging the Benefits API, weâ€™ve streamlined the application process for Marketplace Private Offer Promotion Program (MPOPP) applications,â€ says Adam Boyle, VP of Product \u0026amp; Engineering at Tackle.io. â€œWith this integration, users can now access pre-populated data, use AI-assisted justifications, and submit applications with a single click. These API capabilities allow sellers to manage fund requests from within the systems they are already working in on a day-to-day basis.â€\nAccess the API Reference to start building custom integrations yourself or connect with a provider who can help you implement automations for Partner Central and AWS Marketplace.\nGet personalized guidance using Amazon Q chat The enhanced Partner Assistant, now integrated with Amazon Q, provides tailored responses to help you navigate your AWS Partner journey. Get personalized recommendations and insights based on your Partner Path, tier, solution, opportunity pipeline, and product listings. Ask questions in your preferred local language about anything from maximizing your use of AWS Partner benefits to checking the status of a customer opportunity.\nScale operations with flexible access controls Through AWS IAM, you can manage permissions with precision and use your existing Workforce Identity Provider for SSO. IAM gives you precise control over user access settings without traditional limitationsâ€”like being restricted to one Alliance Lead. Implement granular access controls to match your organizationâ€™s needs, such as restricting specific opportunities to designated teams.\nWith SSO, your team members can access Partner tools and resources with a single set of credentials. Access the AWS IAM Identity Center guide for step-by-step guidance for connecting with identity providers. Partners using Okta can leverage Oktaâ€™s AWS Identity Center integration to streamline user onboarding and enable SSO across their organization.\nPartners are already experiencing these benefits. â€œArctic Wolfâ€™s onboarding to AWS Partner Central in the AWS Console was incredibly smooth using AWS Identity Center with our existing identity providerâ€”our IT department was able to provision users in minutes,â€ says Sean Phillips, VP of Strategic Alliances at Arctic Wolf. â€œIâ€™m excited to no longer manage individual user permissions myself, knowing our IT team has the security controls and streamlined administration they need through IAM. Weâ€™re looking forward to leveraging the new API capabilities and automated workflows to transform how we manage our AWS partnership.â€\nExpand your reach with solution discovery in AWS Marketplace In Partner Central, you can now publish multi-product, multi-vendor solutions for public discovery in AWS Marketplace. Beyond using Partner Connections to find complementary Partners for co-selling, you can now create multi-product solutions that include multiple software and service components from multiple vendors, and list them publicly in AWS Marketplace. Customers can discover your solutions in AWS Marketplace by searching for business problems (such as â€œdata migrationâ€) rather than specific products, making it easier for them to find the right solutions.\nLooking ahead as we continue to enhance the experience for Partners, weâ€™re investing in AI-powered capabilities to accelerate technical reviews, approval workflows, and co-selling motionsâ€”helping you improve operational efficiency and scale your business faster with AWS.\nGet started Visit the AWS Partner Central webpage to learn more about the new experience. New Partners can get started by registering as an AWS Partner in the AWS Console.\nExisting Partners need to take action to migrate to the new Partner Central experience:\nCollaborate with your IT department to link your APN account to an AWS account and plan your user onboarding strategy. To onboard users, connect with your existing company Identity Provider, use AWS IAM Identity Center, or manage permissions directly in the IAM console. Schedule your migration in Partner Centralridge, Cloudsoft, CloudZone, Innovative Solutions, Labra, Rackspace, Spektra Systems, Suger, Tackle.io, and WorkSpan. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.9-week9/",
	"title": "Week 9 - Container Compute",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Master Containerization (Docker) and Orchestration (ECS/Fargate). Manage container images with ECR. Tasks to be carried out this week: Day Task Reference 2 - Docker: Create Multi-stage Dockerfiles for application optimization.\n- Build and test images locally. Docker Docs 3 - ECR: Setup Private ECR Repositories with Lifecycle policies.\n- Push images to ECR. AWS ECR Docs 4 - ECS: Create ECS Cluster and Task Definitions (Fargate).\n- Define Task/Execution Roles. AWS ECS Docs 5 - Service: Deploy ECS Service with ALB integration.\n- Configure Auto Scaling. AWS ELB/ASG 6 - Review: Verify container deployment and scaling behaviors.\n- Weekly Report. - Week 9 Achievements: Optimized container images using multi-stage builds. Deployed scalable containerized applications on ECS Fargate. Secured container communication using Security Groups and IAM Roles. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/10-cloudwatch/",
	"title": "CloudWatch Monitoring",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Task 10 Objective:\nSet up basic CloudWatch monitoring for the EKS cluster and the retail API.\n1. Container Insights # Enable Container Insights for the EKS cluster aws eks update-addon \\ --cluster-name mlops-retail-cluster \\ --addon-name amazon-cloudwatch-observability \\ --addon-version v1.0.0-eksbuild.1 # Create Log Groups aws logs create-log-group \\ --log-group-name /aws/containerinsights/mlops-retail-cluster/application \\ --retention-in-days 7 Tip: Choose an appropriate retention period (e.g., 7â€“30 days) for log groups based on debugging and compliance needs to balance cost and traceability.\n2. Basic Alarms # CPU alarm aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;EKS-HighCPU\u0026#34; \\ --alarm-description \u0026#34;CPU \u0026gt; 80%\u0026#34; \\ --metric-name node_cpu_utilization \\ --namespace ContainerInsights \\ --statistic Average \\ --period 300 \\ --threshold 80 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=ClusterName,Value=mlops-retail-cluster # Memory alarm aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;EKS-HighMemory\u0026#34; \\ --alarm-description \u0026#34;Memory \u0026gt; 85%\u0026#34; \\ --metric-name node_memory_utilization \\ --namespace ContainerInsights \\ --statistic Average \\ --period 300 \\ --threshold 85 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=ClusterName,Value=mlops-retail-cluster Info: Configure alarm actions (for example, an SNS topic or a webhook to PagerDuty) to notify the team automatically when an alarm triggers. Double-check metric namespaces and dimensions before enabling alarms.\n3. SageMaker Training Logs # Create SageMaker Training Job with logging configuration aws sagemaker create-training-job \\ --training-job-name retail-prediction-training \\ --algorithm-specification TrainingImage=842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:v3 \\ --role-arn arn:aws:iam::842676018087:role/SageMakerExecutionRole \\ --input-data-config \u0026#39;[{ \u0026#34;ChannelName\u0026#34;: \u0026#34;training\u0026#34;, \u0026#34;DataSource\u0026#34;: { \u0026#34;S3DataSource\u0026#34;: { \u0026#34;S3DataType\u0026#34;: \u0026#34;S3Prefix\u0026#34;, \u0026#34;S3Uri\u0026#34;: \u0026#34;s3://retail-prediction-bucket/data/\u0026#34;, \u0026#34;S3DataDistributionType\u0026#34;: \u0026#34;FullyReplicated\u0026#34; } }, \u0026#34;ContentType\u0026#34;: \u0026#34;text/csv\u0026#34;, \u0026#34;CompressionType\u0026#34;: \u0026#34;None\u0026#34; }]\u0026#39; \\ --output-data-config S3OutputPath=s3://retail-prediction-bucket/model-artifacts/ \\ --resource-config \u0026#39;{ \u0026#34;InstanceType\u0026#34;: \u0026#34;ml.m5.xlarge\u0026#34;, \u0026#34;InstanceCount\u0026#34;: 1, \u0026#34;VolumeSizeInGB\u0026#34;: 50 }\u0026#39; \\ --stopping-condition MaxRuntimeInSeconds=3600 # Log analysis with CloudWatch Insights aws logs start-query \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training\u0026#34; \\ --start-time $(date -d \u0026#39;24 hour ago\u0026#39; +%s) \\ --end-time $(date +%s) \\ --query-string \u0026#39; filter @message like \u0026#34;loss\u0026#34; | parse @message \u0026#34;loss: *,\u0026#34; as loss_value | parse @message \u0026#34;epoch * \u0026#34; as epoch_num | stats avg(loss_value) as avg_loss by bin(1h)\u0026#39; # Create a metric filter for training loss aws logs put-metric-filter \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training\u0026#34; \\ --filter-name \u0026#34;TrainingLoss\u0026#34; \\ --filter-pattern \u0026#39;\u0026#34;loss: \u0026#34;\u0026#39; \\ --metric-transformations \\ metricName=TrainingLoss,metricNamespace=RetailMLOps,metricValue=1,unit=None Warning: SageMaker training logs can produce a large volume of data (especially when printing many lines or using verbose debug). Limit logging levels in training scripts and consider metric extraction/filters to avoid spikes in ingestion/storage costs.\n4. Verification # Check Container Insights kubectl get pods -n amazon-cloudwatch # Check alarms aws cloudwatch describe-alarms --alarm-names \u0026#34;EKS-HighCPU\u0026#34; \u0026#34;EKS-HighMemory\u0026#34; # Check logs aws logs describe-log-groups --log-group-name-prefix \u0026#34;/aws/containerinsights/mlops-retail-cluster\u0026#34; # Check SageMaker training logs aws logs describe-log-streams \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training\u0026#34; ğŸ¯ Task 10 Complete - CloudWatch Monitoring\nContainer Insights enabled for the EKS cluster CloudWatch Alarms configured for CPU/Memory SageMaker training logs with custom metrics Log Groups with retention policies Note: Before deleting log groups or metric filters, export or snapshot important dashboards (for example Grafana/CSV) if you need to retain them for audit or later comparison.\n5. Clean Up Resources 5.1 XÃ³a CloudWatch Alarms # List all alarms related to the project aws cloudwatch describe-alarms \\ --alarm-name-prefix \u0026#34;EKS-\u0026#34; \\ --query \u0026#39;MetricAlarms[].AlarmName\u0026#39; \\ --output table # Delete specific alarms aws cloudwatch delete-alarms \\ --alarm-names \u0026#34;EKS-HighCPU\u0026#34; \u0026#34;EKS-HighMemory\u0026#34; # Delete alarms related to SageMaker aws cloudwatch describe-alarms \\ --alarm-name-prefix \u0026#34;SageMaker-\u0026#34; \\ --query \u0026#39;MetricAlarms[].AlarmName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | xargs -I {} aws cloudwatch delete-alarms --alarm-names {} 5.2 XÃ³a Log Groups vÃ  Metric Filters # List all related log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/containerinsights/mlops-retail-cluster\u0026#34; \\ --query \u0026#39;logGroups[].logGroupName\u0026#39; \\ --output table # Delete Container Insights log groups aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/containerinsights/mlops-retail-cluster/application\u0026#34; aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/containerinsights/mlops-retail-cluster/dataplane\u0026#34; aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/containerinsights/mlops-retail-cluster/host\u0026#34; # Delete SageMaker training log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/sagemaker/TrainingJobs\u0026#34; \\ --query \u0026#39;logGroups[].logGroupName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read log_group; do echo \u0026#34;Deleting log group: $log_group\u0026#34; aws logs delete-log-group --log-group-name \u0026#34;$log_group\u0026#34; done # Delete EKS cluster log groups aws logs delete-log-group --log-group-name \u0026#34;/aws/eks/mlops-retail-cluster/cluster\u0026#34; || true 5.3 Disable Container Insights # Disable Container Insights addon aws eks delete-addon \\ --cluster-name mlops-retail-cluster \\ --addon-name amazon-cloudwatch-observability # Remove CloudWatch agent from the EKS cluster kubectl delete namespace amazon-cloudwatch || true # Remove CloudWatch agent DaemonSet if present kubectl delete daemonset cloudwatch-agent -n amazon-cloudwatch || true kubectl delete configmap cwagentconfig -n amazon-cloudwatch || true kubectl delete serviceaccount cloudwatch-agent -n amazon-cloudwatch || true 5.4 XÃ³a Custom Metrics vÃ  Dashboards # List custom metrics in the RetailMLOps namespace aws cloudwatch list-metrics \\ --namespace \u0026#34;RetailMLOps\u0026#34; \\ --query \u0026#39;Metrics[].MetricName\u0026#39; \\ --output table # Delete custom metric filters aws logs describe-metric-filters \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training\u0026#34; \\ --query \u0026#39;metricFilters[].filterName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read filter; do echo \u0026#34;Deleting metric filter: $filter\u0026#34; aws logs delete-metric-filter \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training\u0026#34; \\ --filter-name \u0026#34;$filter\u0026#34; done # List and delete CloudWatch Dashboards aws cloudwatch list-dashboards \\ --dashboard-name-prefix \u0026#34;RetailMLOps\u0026#34; \\ --query \u0026#39;DashboardEntries[].DashboardName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read dashboard; do echo \u0026#34;Deleting dashboard: $dashboard\u0026#34; aws cloudwatch delete-dashboards --dashboard-names \u0026#34;$dashboard\u0026#34; done 5.5 Verification Clean Up # Verify alarms have been deleted aws cloudwatch describe-alarms \\ --alarm-name-prefix \u0026#34;EKS-\u0026#34; \\ --query \u0026#39;MetricAlarms[].AlarmName\u0026#39; # Verify log groups have been deleted aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/containerinsights/mlops-retail-cluster\u0026#34; \\ --query \u0026#39;logGroups[].logGroupName\u0026#39; # Verify Container Insights addon has been removed aws eks describe-addon \\ --cluster-name mlops-retail-cluster \\ --addon-name amazon-cloudwatch-observability || echo \u0026#34;Addon removed successfully\u0026#34; # Check namespaces in EKS kubectl get namespaces | grep cloudwatch 6. CloudWatch Monitoring Pricing (ap-southeast-1) 6.1. CloudWatch Logs Pricing Service Ingestion Storage Analysis Log Ingestion $0.67/GB - - Log Storage - $0.033/GB/month - Log Insights - - $0.0067/GB scanned 6.2. Container Insights Pricing Component Volume Monthly Cost Description Performance Logs ~2GB/month $1.34 Node and Pod metrics Application Logs ~1GB/month $0.67 Container stdout/stderr Storage (30 days) 3GB total $0.099 Log retention Insights Queries ~0.5GB scanned $0.0034 Monthly analysis Total Container Insights $2.11/month Per cluster 6.3. CloudWatch Alarms and Metrics Feature Quantity Unit Cost Monthly Cost Standard Metrics Free $0 $0 Custom Metrics 10 metrics $0.30/metric $3.00 Alarms 5 alarms $0.10/alarm $0.50 API Calls 1M calls $0.01/1000 calls $10.00 Total Monitoring $13.50/month 6.4. SageMaker Training Logs # Estimate SageMaker training log volume # Typical training job generates ~100MB logs # With 4 training jobs per month Training Scenario Log Volume Ingestion Cost Storage Cost Total Cost Single Training 100MB $0.067 $0.0033 $0.070 4 Jobs/month 400MB $0.268 $0.013 $0.281 Daily Training 3GB/month $2.01 $0.099 $2.11 6.5. CloudWatch Dashboards Dashboard Type Widgets Monthly Cost Use Case Basic Dashboard 3 widgets $3.00 EKS cluster overview Detailed Dashboard 10 widgets $3.00 Full MLOps monitoring Custom Dashboard 20 widgets $3.00 Multi-service view Note: CloudWatch Dashboard pricing is $3.00/month per dashboard, regardless of widget count\n6.6. Log Insights Query Costs # Example query costs for different scenarios Query Type Data Scanned Cost per Query Monthly Queries Monthly Cost Error Analysis 100MB $0.00067 50 $0.034 Performance Review 500MB $0.0034 20 $0.068 Full Log Search 2GB $0.013 10 $0.13 Total Query Cost $0.23/month 6.7. Data Transfer Costs Transfer Type Volume Cost Monthly Estimate CloudWatch API 1GB/month $0.12/GB $0.12 Log Streaming 500MB/month $0.12/GB $0.06 Cross-AZ Logs 200MB/month $0.01/GB $0.002 Total Transfer $0.18/month 6.8. Retention Cost Analysis Retention Period Storage Multiplier Cost Impact Use Case 1 day 1x Baseline Development 7 days 7x 7x storage cost Testing 30 days 30x 30x storage cost Production 1 year 365x 365x storage cost Compliance # Example: 1GB/day logs with different retention # 1 day: $0.033 storage cost # 30 days: $0.99 storage cost # 1 year: $12.05 storage cost 6.9. Cost Optimization Strategies Log Filtering:\n# Chá»‰ log ERROR vÃ  WARNING levels aws logs put-metric-filter \\ --log-group-name \u0026#34;/aws/containerinsights/mlops-retail-cluster/application\u0026#34; \\ --filter-name \u0026#34;ErrorsOnly\u0026#34; \\ --filter-pattern \u0026#34;ERROR WARN\u0026#34; \\ --metric-transformations metricName=ErrorCount,metricNamespace=RetailMLOps,metricValue=1 Intelligent Log Routing:\n# Kubernetes fluent-bit configuration apiVersion: v1 kind: ConfigMap metadata: name: fluent-bit-config data: output.conf: | [OUTPUT] Name cloudwatch_logs Match kube.var.log.containers.*error* log_group_name /aws/containerinsights/cluster/errors [OUTPUT] Name cloudwatch_logs Match kube.var.log.containers.*info* log_group_name /aws/containerinsights/cluster/info retention_in_days 7 Metric Sampling:\n# Sample metrics every 5 minutes instead of 1 minute aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;EKS-HighCPU-Sampled\u0026#34; \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 80 6.10. Tá»•ng káº¿t chi phÃ­ Task 10 6.10. Cost Summary for Task 10 Scenario 1: Basic Monitoring (Development)\nComponent Monthly Cost Container Insights (basic) $2.11 3 CloudWatch Alarms $0.30 1 Dashboard $3.00 Log storage (7 days) $0.23 Total Development $5.64/month Scenario 2: Production Monitoring\nComponent Monthly Cost Container Insights (full) $6.00 10 CloudWatch Alarms $1.00 2 Dashboards $6.00 SageMaker training logs $0.28 Log storage (30 days) $0.99 Custom metrics $3.00 Log Insights queries $0.23 Total Production $17.50/month Scenario 3: Enterprise Monitoring\nComponent Monthly Cost Container Insights (multi-cluster) $12.00 25 CloudWatch Alarms $2.50 5 Dashboards $15.00 Daily SageMaker training $2.11 Extended log retention $5.00 Heavy custom metrics $10.00 Frequent queries $2.00 Total Enterprise $48.61/month 6.11. Monitoring Cost Commands # Track CloudWatch costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon CloudWatch\u0026#34;]}}\u0026#39; # Monitor log ingestion volume aws logs describe-log-groups \\ --query \u0026#39;logGroups[?storedBytes \u0026gt; `1000000`].[logGroupName,storedBytes]\u0026#39; \\ --output table # Check metric usage aws cloudwatch list-metrics \\ --namespace \u0026#34;RetailMLOps\u0026#34; \\ --query \u0026#39;length(Metrics)\u0026#39; ğŸ’° Cost Summary for Task 10:\nDevelopment: $5.64/month (basic monitoring) Production: $17.50/month (full monitoring) Enterprise: $48.61/month (extensive monitoring) Optimization potential: 30-50% cost reduction with log filtering and retention tuning ğŸ¬ Video for Task 10 Next Step: Task 11: CI/CD Pipeline\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.11-blog11/",
	"title": "Announcing Amazon EKS Capabilities for workload orchestration and cloud resource management",
	"tags": [],
	"description": "",
	"content": "Author: Channy Yun (ìœ¤ì„ì°¬) â€” 30 NOV 2025\nToday, weâ€™re announcing Amazon Elastic Kubernetes Service (Amazon EKS) Capabilities, an extensible set of Kubernetes-native solutions that streamline workload orchestration, Amazon Web Services (AWS) cloud resource management, and Kubernetes resource composition and orchestration. These fully managed, integrated platform capabilities include open source Kubernetes solutions that many customers are using today, such as Argo CD, AWS Controllers for Kubernetes, and Kube Resource Orchestrator.\nWith EKS Capabilities, you can build and scale Kubernetes applications without managing complex solution infrastructure. Unlike typical in-cluster installations, these capabilities actually run in EKS service-owned accounts that are fully abstracted from customers.\nWith AWS managing infrastructure scaling, patching, and updates of these cluster capabilities, you can use the enterprise reliability and security without needing to maintain and manage the underlying components.\nHere are the capabilities available at launch:\nArgo CD â€“ This is a declarative GitOps tool for Kubernetes that provides continuous continuous deployment (CD) capabilities for Kubernetes. Itâ€™s broadly adopted, with more than 45% of Kubernetes end-users reporting production or planned production use in the 2024 Cloud Native Computing Foundation (CNCF) Survey. AWS Controllers for Kubernetes (ACK) â€“ ACK is highly popular with enterprise platform teams in production environments. ACK provides custom resources for Kubernetes that enable the management of AWS Cloud resources directly from within your clusters. Kube Resource Orchestrator (KRO) â€“ KRO provides a streamlined way to create and manage custom resources in Kubernetes. With KRO, platform teams can create reusable resource bundles that abstract away complexity while remaining natively to the Kubernetes ecosystem. With these features, you can accelerate and scale your Kubernetes use with fully managed capabilities, using its opinionated but flexible features to build for scale right from the start. It is designed to offer a set of foundational cluster capabilities that layer seamlessly with each other, providing integrated features for continuous deployment, resource orchestration, and composition. You can focus on managing and shipping software without needing to spend time and resources building and managing these foundational platform components.\nHow it works Platform engineers and cluster administrators can set up EKS Capabilities to offload building and managing custom solutions to provide common foundational services, meaning they can focus on more differentiated features that matter to your business.\nYour application developers primarily work with EKS Capabilities as they do other Kubernetes features. They do this by applying declarative configuration to create Kubernetes resources using familiar tools, such as kubectl or through automation from git commit to running code. Get started with EKS Capabilities To enable EKS Capabilities, you can use the EKS console, AWS Command Line Interface (AWS CLI), eksctl, or other preferred tools. In the EKS console, choose Create capabilities in the Capabilities tab on your existing EKS cluster. EKS Capabilities are AWS resources, and they can be tagged, managed, and deleted. You can select one or more capabilities to work together. I checked all three capabilities: ArgoCD, ACK, and KRO. However, these capabilities are completely independent and you can pick and choose which capabilities you want enabled on your clusters. Now you can configure selected capabilities. You should create AWS Identity and Access Management (AWS IAM) roles to enable EKS to operate these capabilities within your cluster. Please note you cannot modify the capability name, namespace, authentication region, or AWS IAM Identity Center instance after creating the capability. Choose Next and review the settings and enable capabilities. Now you can see and manage created capabilities. Select ArgoCD to update configuration of the capability. You can see details of ArgoCD capability. Choose Edit to change configuration settings or Monitor ArgoCD to show the health status of the capability for the current EKS cluster. Choose Go to Argo UI to visualize and monitor deployment status and application health. To learn more about how to set up and use each capability in detail, visit Getting started with EKS Capabilities in the Amazon EKS User Guide.\nThings to know Here are key considerations to know about this feature:\nPermissions â€“ EKS Capabilities are cluster-scoped administrator resources, and resource permissions are configured through AWS IAM. For some capabilities, there is additional configuration for single sign-on. For example, Argo CD single sign-on configuration is enabled directly in EKS with a direct integration with IAM Identity Center. Upgrades â€“ EKS automatically updates cluster capabilities you enable and their related dependencies. It automatically analyzes for breaking changes, patches and updates components as needed, and informs you of conflicts or issues through the EKS cluster insights. Adoptions â€“ ACK provides resource adoption features that enable migration of existing AWS resources into ACK management. ACK also provides read-only resources which can help facilitate a step-wise migration from provisioned resources with Terraform, AWS CloudFormation into EKS Capabilities. Now available Amazon EKS Capabilities are now available in commercial AWS Regions. For Regional availability and future roadmap, visit the AWS Capabilities by Region. There are no upfront commitments or minimum fees, and you only pay for the EKS Capabilities and resources that you use. To learn more, visit the EKS pricing page.\nGive it a try in the Amazon EKS console and send feedback to AWS re:Post for EKS or through your usual AWS Support contacts.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.10-week10/",
	"title": "Week 10 - DevOps &amp; IaC",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Implement Infrastructure as Code (Terraform/CloudFormation). Setup CI/CD pipelines (CodePipeline/GitHub Actions). Tasks to be carried out this week: Day Task Reference 2 - IaC: Write Terraform code for VPC, SG, and IAM resources.\n- Manage state with S3 Backend. Terraform Docs 3 - CI/CD Setup: Configure CodeCommit/GitHub repo.\n- Setup CodeBuild projects for building Docker images. AWS CodeBuild 4 - Pipeline: Create CodePipeline to automate Build -\u0026gt; Deploy.\n- Implement Manual Approval stages. AWS CodePipeline 5 - Security: Integrate secrets management (SSM/Secrets Manager) into pipeline.\n- Verify automated deployments. FinOps Guide 6 - Review: Test full CI/CD flow from commit to deploy.\n- Weekly Report. - Week 10 Achievements: Automated infrastructure provisioning using Terraform. Built a fully automated CI/CD pipeline for container deployment. Eliminated hardcoded secrets in deployment scripts. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/11-cicd-jenkins-travis/",
	"title": "CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Task 13 Objective:\nSet up an automated CI/CD pipeline for the entire MLOps Retail Prediction project lifecycle:\nCI (Continuous Integration): build \u0026amp; test code, build Docker image, push to ECR CD (Continuous Delivery): retrain model when needed, update Model Registry, deploy new versions to EKS Monitoring hooks: automated rollback on API or model failures (CloudWatch triggers) â†’ Ensure continuous delivery, reduce manual errors, and save time and cost.\nğŸ“¥ Input from previous Tasks:\nTask 6 (ECR Container Registry): Repository URIs, lifecycle policies, image scanning and push commands; credentials and ECR access for CI runners Task 8 (API Deployment on EKS): Kubernetes manifests, service \u0026amp; deployment names, healthcheck endpoints, HPA and ServiceAccount/IRSA details used by CD stage Task 10 (CloudWatch Monitoring): Log groups, alarms, dashboards and Container Insights configuration used for deployment verification and automated rollback triggers Task 2 (IAM Roles \u0026amp; Audit): IAM roles, OIDC provider configuration and least-privilege policies for CI/CD runners (GitHub Actions / Jenkins) and SageMaker execution 1. Overall Pipeline Structure The Retail Prediction CI/CD pipeline automates the full process from code commits to production deployment, including retraining models when necessary.\nThe pipeline is divided into three environments:\nDEV: Build, test and validate code STAGING: Train and evaluate models PROD: Deploy and monitor in production 2. IAM Roles vÃ  Permissions 2.1 Create CI/CD Service Role # Create trust policy for CI/CD role cat \u0026gt; cicd-trust-policy.json \u0026lt;\u0026lt; EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: [ \u0026#34;ec2.amazonaws.com\u0026#34;, \u0026#34;codebuild.amazonaws.com\u0026#34;, \u0026#34;codepipeline.amazonaws.com\u0026#34; ] }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::YOUR_ACCOUNT_ID:user/jenkins-user\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOF # Create the role aws iam create-role \\ --role-name RetailForecastCICDRole \\ --assume-role-policy-document file://cicd-trust-policy.json \\ --description \u0026#34;Role for Retail Forecast CI/CD Pipeline\u0026#34; 2.2 CI/CD IAM Policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::retail-forecast-data-bucket\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-data-bucket/*\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-artifacts-bucket\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-artifacts-bucket/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetAuthorizationToken\u0026#34;, \u0026#34;ecr:BatchCheckLayerAvailability\u0026#34;, \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34;, \u0026#34;ecr:PutImage\u0026#34;, \u0026#34;ecr:InitiateLayerUpload\u0026#34;, \u0026#34;ecr:UploadLayerPart\u0026#34;, \u0026#34;ecr:CompleteLayerUpload\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sagemaker:CreateTrainingJob\u0026#34;, \u0026#34;sagemaker:DescribeTrainingJob\u0026#34;, \u0026#34;sagemaker:StopTrainingJob\u0026#34;, \u0026#34;sagemaker:CreateModel\u0026#34;, \u0026#34;sagemaker:CreateModelPackage\u0026#34;, \u0026#34;sagemaker:CreateModelPackageGroup\u0026#34;, \u0026#34;sagemaker:DescribeModelPackage\u0026#34;, \u0026#34;sagemaker:UpdateModelPackage\u0026#34;, \u0026#34;sagemaker:ListModelPackages\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;eks:DescribeCluster\u0026#34;, \u0026#34;eks:ListClusters\u0026#34;, \u0026#34;eks:DescribeNodegroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sts:GetCallerIdentity\u0026#34;, \u0026#34;sts:AssumeRole\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:GenerateDataKey\u0026#34;, \u0026#34;kms:CreateGrant\u0026#34;], \u0026#34;Resource\u0026#34;: [\u0026#34;arn:aws:kms:us-east-1:YOUR_ACCOUNT_ID:key/*\u0026#34;] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 2.3 Attach Policies # Create and attach the policy aws iam create-policy \\ --policy-name RetailForecastCICDPolicy \\ --policy-document file://cicd-policy.json aws iam attach-role-policy \\ --role-name RetailForecastCICDRole \\ --policy-arn arn:aws:iam::YOUR_ACCOUNT_ID:policy/RetailForecastCICDPolicy # Attach additional managed policies aws iam attach-role-policy \\ --role-name RetailForecastCICDRole \\ --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy aws iam attach-role-policy \\ --role-name RetailForecastCICDRole \\ --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy 3. Jenkins Pipeline Setup 3.1 Jenkins Installation on EC2 # Launch EC2 instance for Jenkins aws ec2 run-instances \\ --image-id ami-0c55b159cbfafe1d0 \\ --instance-type t3.medium \\ --key-name your-key-pair \\ --security-group-ids sg-jenkins \\ --subnet-id subnet-12345678 \\ --iam-instance-profile Name=JenkinsInstanceProfile \\ --user-data file://jenkins-install.sh \\ --tag-specifications \u0026#39;ResourceType=instance,Tags=[{Key=Name,Value=Jenkins-Server},{Key=Project,Value=RetailForecast}]\u0026#39; 3.2 Jenkins Installation Script #!/bin/bash # jenkins-install.sh # Update system yum update -y # Install Docker yum install -y docker systemctl start docker systemctl enable docker usermod -a -G docker ec2-user # Install Docker Compose curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose # Install Jenkins wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key yum upgrade -y yum install -y java-11-openjdk jenkins # Start Jenkins systemctl start jenkins systemctl enable jenkins # Install kubectl curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl chmod +x ./kubectl mv ./kubectl /usr/local/bin # Install AWS CLI v2 curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip ./aws/install # Install Python and dependencies yum install -y python3 python3-pip pip3 install boto3 sagemaker pandas scikit-learn # Configure Jenkins user for Docker usermod -a -G docker jenkins systemctl restart jenkins echo \u0026#34;Jenkins installation completed!\u0026#34; echo \u0026#34;Access Jenkins at: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ip):8080\u0026#34; echo \u0026#34;Initial admin password: $(cat /var/lib/jenkins/secrets/initialAdminPassword)\u0026#34; 3.3 Jenkinsfile for ML Pipeline // Jenkinsfile pipeline { agent any environment { AWS_DEFAULT_REGION = \u0026#39;us-east-1\u0026#39; AWS_ACCOUNT_ID = \u0026#39;123456789012\u0026#39; ECR_REPOSITORY = \u0026#39;retail-forecast\u0026#39; EKS_CLUSTER_NAME = \u0026#39;retail-forecast-cluster\u0026#39; NAMESPACE = \u0026#39;mlops\u0026#39; S3_DATA_BUCKET = \u0026#39;retail-forecast-data-bucket\u0026#39; S3_ARTIFACTS_BUCKET = \u0026#39;retail-forecast-artifacts-bucket\u0026#39; MODEL_PACKAGE_GROUP = \u0026#39;retail-forecast-model-group\u0026#39; } parameters { choice( name: \u0026#39;DEPLOY_ENVIRONMENT\u0026#39;, choices: [\u0026#39;dev\u0026#39;, \u0026#39;staging\u0026#39;, \u0026#39;prod\u0026#39;], description: \u0026#39;Target deployment environment\u0026#39; ) booleanParam( name: \u0026#39;RETRAIN_MODEL\u0026#39;, defaultValue: false, description: \u0026#39;Force model retraining\u0026#39; ) booleanParam( name: \u0026#39;SKIP_TESTS\u0026#39;, defaultValue: false, description: \u0026#39;Skip test execution\u0026#39; ) } stages { stage(\u0026#39;Setup\u0026#39;) { steps { script { // Clean workspace cleanWs() // Checkout code checkout scm // Set build info env.BUILD_VERSION = \u0026#34;${env.BUILD_NUMBER}-${env.GIT_COMMIT.take(7)}\u0026#34; env.IMAGE_TAG = \u0026#34;v${env.BUILD_VERSION}\u0026#34; echo \u0026#34;Build Version: ${env.BUILD_VERSION}\u0026#34; echo \u0026#34;Image Tag: ${env.IMAGE_TAG}\u0026#34; } } } stage(\u0026#39;Environment Setup\u0026#39;) { steps { script { // Install Python dependencies sh \u0026#39;\u0026#39;\u0026#39; python3 -m venv venv source venv/bin/activate pip install --upgrade pip pip install -r requirements.txt pip install pytest pytest-cov flake8 \u0026#39;\u0026#39;\u0026#39; // Configure AWS credentials sh \u0026#39;\u0026#39;\u0026#39; aws sts get-caller-identity aws configure set region $AWS_DEFAULT_REGION \u0026#39;\u0026#39;\u0026#39; // Configure kubectl sh \u0026#39;\u0026#39;\u0026#39; aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region $AWS_DEFAULT_REGION kubectl cluster-info \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Code Quality \u0026amp; Testing\u0026#39;) { when { not { params.SKIP_TESTS } } parallel { stage(\u0026#39;Linting\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate flake8 src/ --max-line-length=88 --exclude=venv \u0026#39;\u0026#39;\u0026#39; } } stage(\u0026#39;Unit Tests\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html \u0026#39;\u0026#39;\u0026#39; // Publish test results publishTestResults testResultsPattern: \u0026#39;test-results.xml\u0026#39; publishCoverage adapters: [coberturaAdapter(\u0026#39;coverage.xml\u0026#39;)], sourceFileResolver: sourceFiles(\u0026#39;STORE_LAST_BUILD\u0026#39;) } } stage(\u0026#39;Integration Tests\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate pytest tests/integration/ -v \u0026#39;\u0026#39;\u0026#39; } } } } stage(\u0026#39;Data Validation\u0026#39;) { steps { script { sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate python scripts/validate_data.py \\ --bucket $S3_DATA_BUCKET \\ --key training-data/train.csv \\ --output data-validation-report.json \u0026#39;\u0026#39;\u0026#39; // Archive validation report archiveArtifacts artifacts: \u0026#39;data-validation-report.json\u0026#39;, fingerprint: true } } } stage(\u0026#39;Model Training\u0026#39;) { when { anyOf { params.RETRAIN_MODEL changeset \u0026#34;src/training/**\u0026#34; changeset \u0026#34;data/**\u0026#34; } } steps { script { sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate python scripts/trigger_training.py \\ --job-name \u0026#34;retail-forecast-training-${BUILD_VERSION}\u0026#34; \\ --data-bucket $S3_DATA_BUCKET \\ --output-bucket $S3_ARTIFACTS_BUCKET \\ --role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/SageMakerExecutionRole \u0026#39;\u0026#39;\u0026#39; // Wait for training completion sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate python scripts/wait_for_training.py \\ --job-name \u0026#34;retail-forecast-training-${BUILD_VERSION}\u0026#34; \\ --timeout 3600 \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Model Validation \u0026amp; Registration\u0026#39;) { when { anyOf { params.RETRAIN_MODEL changeset \u0026#34;src/training/**\u0026#34; } } steps { script { // Validate model performance sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate python scripts/validate_model.py \\ --job-name \u0026#34;retail-forecast-training-${BUILD_VERSION}\u0026#34; \\ --baseline-accuracy 0.85 \\ --output model-validation-report.json \u0026#39;\u0026#39;\u0026#39; // Register model if validation passes sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate python scripts/register_model.py \\ --job-name \u0026#34;retail-forecast-training-${BUILD_VERSION}\u0026#34; \\ --model-package-group $MODEL_PACKAGE_GROUP \\ --approval-status \u0026#34;PendingManualApproval\u0026#34; \\ --model-version $BUILD_VERSION \u0026#39;\u0026#39;\u0026#39; archiveArtifacts artifacts: \u0026#39;model-validation-report.json\u0026#39;, fingerprint: true } } } stage(\u0026#39;Docker Build\u0026#39;) { steps { script { // Build Docker image sh \u0026#39;\u0026#39;\u0026#39; # Get latest approved model MODEL_URI=$(python scripts/get_latest_model.py --model-package-group $MODEL_PACKAGE_GROUP) echo \u0026#34;Using model: $MODEL_URI\u0026#34; # Build image with model URI docker build \\ --build-arg MODEL_URI=$MODEL_URI \\ --build-arg BUILD_VERSION=$BUILD_VERSION \\ -t $ECR_REPOSITORY:$IMAGE_TAG \\ -t $ECR_REPOSITORY:latest \\ . \u0026#39;\u0026#39;\u0026#39; // Security scan sh \u0026#39;\u0026#39;\u0026#39; docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ -v $(pwd):/root/.cache/ \\ aquasec/trivy:latest image \\ --exit-code 1 \\ --severity HIGH,CRITICAL \\ $ECR_REPOSITORY:$IMAGE_TAG \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Push to ECR\u0026#39;) { steps { script { sh \u0026#39;\u0026#39;\u0026#39; # Login to ECR aws ecr get-login-password --region $AWS_DEFAULT_REGION | \\ docker login --username AWS --password-stdin \\ $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com # Tag images docker tag $ECR_REPOSITORY:$IMAGE_TAG \\ $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG docker tag $ECR_REPOSITORY:latest \\ $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:latest # Push images docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:latest \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Deploy to EKS\u0026#39;) { steps { script { // Update Kubernetes deployment sh \u0026#39;\u0026#39;\u0026#39; # Update deployment image kubectl set image deployment/retail-forecast-api \\ retail-forecast-api=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG \\ -n $NAMESPACE # Wait for rollout kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE --timeout=600s # Verify deployment kubectl get pods -n $NAMESPACE -l app=retail-forecast-api \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Health Check\u0026#39;) { steps { script { sh \u0026#39;\u0026#39;\u0026#39; # Get service endpoint ENDPOINT=$(kubectl get service retail-forecast-service -n $NAMESPACE -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) if [ -z \u0026#34;$ENDPOINT\u0026#34; ]; then ENDPOINT=$(kubectl get service retail-forecast-service -n $NAMESPACE -o jsonpath=\u0026#39;{.spec.clusterIP}\u0026#39;) kubectl port-forward service/retail-forecast-service 8080:80 -n $NAMESPACE \u0026amp; ENDPOINT=\u0026#34;localhost:8080\u0026#34; PORT_FORWARD_PID=$! fi # Wait for service to be ready echo \u0026#34;Waiting for service to be ready...\u0026#34; for i in {1..30}; do if curl -f http://$ENDPOINT/healthz; then echo \u0026#34;Service is healthy!\u0026#34; break fi echo \u0026#34;Attempt $i/30 failed, retrying in 10 seconds...\u0026#34; sleep 10 done # Test prediction endpoint curl -X POST http://$ENDPOINT/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;features\u0026#34;: {\u0026#34;store_id\u0026#34;: 1, \u0026#34;product_id\u0026#34;: 123, \u0026#34;price\u0026#34;: 29.99}}\u0026#39; # Kill port-forward if used if [ ! -z \u0026#34;$PORT_FORWARD_PID\u0026#34; ]; then kill $PORT_FORWARD_PID fi \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Performance Testing\u0026#39;) { when { environment name: \u0026#39;DEPLOY_ENVIRONMENT\u0026#39;, value: \u0026#39;prod\u0026#39; } steps { script { sh \u0026#39;\u0026#39;\u0026#39; # Run load test python scripts/load_test.py \\ --endpoint http://$ENDPOINT \\ --duration 300 \\ --concurrent-users 10 \\ --output load-test-report.json \u0026#39;\u0026#39;\u0026#39; archiveArtifacts artifacts: \u0026#39;load-test-report.json\u0026#39;, fingerprint: true } } } } post { always { // Clean up sh \u0026#39;\u0026#39;\u0026#39; docker system prune -f rm -rf venv \u0026#39;\u0026#39;\u0026#39; // Archive logs archiveArtifacts artifacts: \u0026#39;logs/**\u0026#39;, allowEmptyArchive: true } success { script { // Send success notification sh \u0026#39;\u0026#39;\u0026#39; aws sns publish \\ --topic-arn arn:aws:sns:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:deployment-notifications \\ --message \u0026#34;âœ… Deployment successful for build $BUILD_VERSION\u0026#34; \\ --subject \u0026#34;Retail Forecast Deployment Success\u0026#34; \u0026#39;\u0026#39;\u0026#39; } } failure { script { // Send failure notification sh \u0026#39;\u0026#39;\u0026#39; aws sns publish \\ --topic-arn arn:aws:sns:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:deployment-notifications \\ --message \u0026#34;âŒ Deployment failed for build $BUILD_VERSION. Check Jenkins logs.\u0026#34; \\ --subject \u0026#34;Retail Forecast Deployment Failed\u0026#34; \u0026#39;\u0026#39;\u0026#39; // Rollback on production failure if (params.DEPLOY_ENVIRONMENT == \u0026#39;prod\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; echo \u0026#34;Rolling back production deployment...\u0026#34; kubectl rollout undo deployment/retail-forecast-api -n $NAMESPACE kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE \u0026#39;\u0026#39;\u0026#39; } } } } } 2. GitHub Actions CI/CD Pipeline ChÃºng ta sáº½ sá»­ dá»¥ng GitHub Actions Ä‘á»ƒ xÃ¢y dá»±ng pipeline CI/CD cho dá»± Ã¡n MLOps Retail Prediction vÃ¬ kháº£ nÄƒng tÃ­ch há»£p sáºµn vá»›i GitHub repository vÃ  tÃ­nh linh hoáº¡t cao.\n2.1 Cáº¥u hÃ¬nh workflow file Táº¡o file .github/workflows/mlops-pipeline.yml:\n# .github/workflows/mlops-pipeline.yml name: MLOps Retail Prediction Pipeline on: push: branches: [main, develop] paths-ignore: - \u0026#34;**.md\u0026#34; - \u0026#34;docs/**\u0026#34; pull_request: branches: [main] schedule: # Cháº¡y má»—i tuáº§n vÃ o thá»© 2 Ä‘á»ƒ kiá»ƒm tra Ä‘á»™ chÃ­nh xÃ¡c cá»§a model - cron: \u0026#34;0 2 * * 1\u0026#34; # Run weekly on Monday to verify model accuracy workflow_dispatch: inputs: environment: description: \u0026#34;Deployment environment\u0026#34; required: true default: \u0026#34;dev\u0026#34; type: choice options: - dev - staging - prod retrain_model: description: \u0026#34;Retrain model\u0026#34; required: false default: false type: boolean deploy_only: description: \u0026#34;Deploy only (do not build/retrain)\u0026#34; required: false default: false type: boolean env: AWS_REGION: us-east-1 ECR_REPOSITORY: retail-forecast EKS_CLUSTER_NAME: retail-forecast-cluster S3_DATA_BUCKET: retail-forecast-data S3_MODEL_BUCKET: retail-forecast-models MODEL_PACKAGE_GROUP: retail-forecast-models permissions: id-token: write # Cáº§n thiáº¿t cho OIDC vá»›i AWS contents: read # Cáº§n thiáº¿t Ä‘á»ƒ checkout code jobs: setup: name: Setup Pipeline runs-on: ubuntu-latest outputs: build-id: ${{ steps.generate-id.outputs.build_id }} environment: ${{ steps.set-env.outputs.environment }} should-train: ${{ steps.set-env.outputs.should_train }} should-deploy: ${{ steps.set-env.outputs.should_deploy }} steps: - name: Generate build ID id: generate-id run: | BUILD_ID=\u0026#34;build-${GITHUB_RUN_NUMBER}-${GITHUB_SHA::7}\u0026#34; echo \u0026#34;build_id=$BUILD_ID\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;Build ID: $BUILD_ID\u0026#34; - name: Set environment variables id: set-env run: | # XÃ¡c Ä‘á»‹nh mÃ´i trÆ°á»ng if [[ \u0026#34;${{ github.event_name }}\u0026#34; == \u0026#34;workflow_dispatch\u0026#34; ]]; then ENV=\u0026#34;${{ github.event.inputs.environment }}\u0026#34; elif [[ \u0026#34;${{ github.ref }}\u0026#34; == \u0026#34;refs/heads/main\u0026#34; ]]; then ENV=\u0026#34;prod\u0026#34; elif [[ \u0026#34;${{ github.ref }}\u0026#34; == \u0026#34;refs/heads/develop\u0026#34; ]]; then ENV=\u0026#34;staging\u0026#34; else ENV=\u0026#34;dev\u0026#34; fi echo \u0026#34;environment=$ENV\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT # XÃ¡c Ä‘á»‹nh cÃ³ nÃªn huáº¥n luyá»‡n model hay khÃ´ng if [[ \u0026#34;${{ github.event.inputs.retrain_model }}\u0026#34; == \u0026#34;true\u0026#34; ]] || \\ [[ \u0026#34;${{ github.event_name }}\u0026#34; == \u0026#34;schedule\u0026#34; ]]; then SHOULD_TRAIN=\u0026#34;true\u0026#34; else SHOULD_TRAIN=\u0026#34;false\u0026#34; fi echo \u0026#34;should_train=$SHOULD_TRAIN\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT # XÃ¡c Ä‘á»‹nh cÃ³ nÃªn deploy hay khÃ´ng if [[ \u0026#34;${{ github.event.inputs.deploy_only }}\u0026#34; == \u0026#34;true\u0026#34; ]] || \\ [[ \u0026#34;$ENV\u0026#34; == \u0026#34;prod\u0026#34; \u0026amp;\u0026amp; \u0026#34;${{ github.ref }}\u0026#34; == \u0026#34;refs/heads/main\u0026#34; ]] || \\ [[ \u0026#34;$ENV\u0026#34; == \u0026#34;staging\u0026#34; \u0026amp;\u0026amp; \u0026#34;${{ github.ref }}\u0026#34; == \u0026#34;refs/heads/develop\u0026#34; ]]; then SHOULD_DEPLOY=\u0026#34;true\u0026#34; else SHOULD_DEPLOY=\u0026#34;false\u0026#34; fi echo \u0026#34;should_deploy=$SHOULD_DEPLOY\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT # In thÃ´ng tin echo \u0026#34;Environment: $ENV\u0026#34; echo \u0026#34;Should train model: $SHOULD_TRAIN\u0026#34; echo \u0026#34;Should deploy: $SHOULD_DEPLOY\u0026#34; test: name: Code Quality \u0026amp; Testing needs: setup runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 - name: Setup Python uses: actions/setup-python@v4 with: python-version: \u0026#34;3.10\u0026#34; cache: \u0026#34;pip\u0026#34; - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r core/requirements.txt pip install -r server/requirements.txt pip install pytest pytest-cov pylint black - name: Code formatting check run: | black --check core/ server/ - name: Lint code run: | pylint --disable=C0111,C0103 core/ server/ - name: Run tests run: | pytest tests/ -v --cov=core --cov=server --cov-report=xml - name: Upload coverage report uses: codecov/codecov-action@v3 with: file: ./coverage.xml fail_ci_if_error: false data_validation: name: Data Validation needs: [setup, test] runs-on: ubuntu-latest if: needs.setup.outputs.should_train == \u0026#39;true\u0026#39; steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup Python uses: actions/setup-python@v4 with: python-version: \u0026#34;3.10\u0026#34; cache: \u0026#34;pip\u0026#34; - name: Install dependencies run: | pip install pandas boto3 great_expectations - name: Validate training data run: | python aws/script/validate_data.py \\ --bucket ${{ env.S3_DATA_BUCKET }} \\ --key training/sales_data.csv \\ --output validation_report.json - name: Upload validation report uses: actions/upload-artifact@v3 with: name: data-validation-report path: validation_report.json model_training: name: Model Training needs: [setup, test, data_validation] runs-on: ubuntu-latest if: needs.setup.outputs.should_train == \u0026#39;true\u0026#39; \u0026amp;\u0026amp; success() steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup Python uses: actions/setup-python@v4 with: python-version: \u0026#34;3.10\u0026#34; cache: \u0026#34;pip\u0026#34; - name: Install dependencies run: | pip install boto3 sagemaker pandas scikit-learn - name: Create training job id: training run: | python aws/script/create_training_job.py \\ --job-name \u0026#34;retail-forecast-${{ needs.setup.outputs.build-id }}\u0026#34; \\ --data-bucket ${{ env.S3_DATA_BUCKET }} \\ --output-bucket ${{ env.S3_MODEL_BUCKET }} \\ --instance-type ml.m5.large \\ --hyperparameters \u0026#34;{\\\u0026#34;n_estimators\\\u0026#34;:\\\u0026#34;200\\\u0026#34;,\\\u0026#34;max_depth\\\u0026#34;:\\\u0026#34;10\\\u0026#34;}\u0026#34; - name: Wait for training completion run: | aws sagemaker wait training-job-completed-or-stopped \\ --training-job-name \u0026#34;retail-forecast-${{ needs.setup.outputs.build-id }}\u0026#34; STATUS=$(aws sagemaker describe-training-job \\ --training-job-name \u0026#34;retail-forecast-${{ needs.setup.outputs.build-id }}\u0026#34; \\ --query \u0026#39;TrainingJobStatus\u0026#39; --output text) if [ \u0026#34;$STATUS\u0026#34; != \u0026#34;Completed\u0026#34; ]; then echo \u0026#34;Training failed with status: $STATUS\u0026#34; exit 1 fi model_evaluation: name: Model Evaluation \u0026amp; Registration needs: [setup, model_training] runs-on: ubuntu-latest outputs: model_approved: ${{ steps.evaluate.outputs.model_approved }} model_version: ${{ steps.register.outputs.model_version }} steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup Python uses: actions/setup-python@v4 with: python-version: \u0026#34;3.10\u0026#34; cache: \u0026#34;pip\u0026#34; - name: Install dependencies run: | pip install boto3 sagemaker pandas scikit-learn matplotlib seaborn - name: Evaluate model id: evaluate run: | python aws/script/processing_evaluate.py \\ --job-name \u0026#34;retail-forecast-${{ needs.setup.outputs.build-id }}\u0026#34; \\ --evaluation-data s3://${{ env.S3_DATA_BUCKET }}/validation/sales_data.csv \\ --baseline-metrics s3://${{ env.S3_MODEL_BUCKET }}/baselines/metrics.json \\ --threshold 0.85 # Check result if [ -f \u0026#34;model_approved.txt\u0026#34; ]; then MODEL_APPROVED=$(cat model_approved.txt) echo \u0026#34;model_approved=$MODEL_APPROVED\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;Model evaluation result: $MODEL_APPROVED\u0026#34; else echo \u0026#34;model_approved=false\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;Model evaluation failed\u0026#34; exit 1 fi - name: Register model id: register if: steps.evaluate.outputs.model_approved == \u0026#39;true\u0026#39; run: | MODEL_VERSION=$(python aws/script/register_model.py \\ --job-name \u0026#34;retail-forecast-${{ needs.setup.outputs.build-id }}\u0026#34; \\ --model-package-group ${{ env.MODEL_PACKAGE_GROUP }} \\ --approval-status \u0026#34;Approved\u0026#34;) echo \u0026#34;model_version=$MODEL_VERSION\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;Model registered with version: $MODEL_VERSION\u0026#34; - name: Upload evaluation results uses: actions/upload-artifact@v3 with: name: model-evaluation-report path: | evaluation_results.json plots/*.png build_docker: name: Build \u0026amp; Push Docker Image needs: [setup, test] runs-on: ubuntu-latest outputs: image_tag: ${{ steps.build.outputs.image_tag }} steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Login to Amazon ECR id: login-ecr uses: aws-actions/amazon-ecr-login@v1 - name: Build and push Docker image id: build env: ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }} run: | # Set tag IMAGE_TAG=\u0026#34;${{ needs.setup.outputs.build-id }}\u0026#34; echo \u0026#34;image_tag=$IMAGE_TAG\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT # Get latest model URI (or use placeholder for dev environment) if [[ \u0026#34;${{ needs.setup.outputs.environment }}\u0026#34; == \u0026#34;dev\u0026#34; ]]; then MODEL_URI=\u0026#34;placeholder\u0026#34; else MODEL_URI=$(aws sagemaker list-model-packages \\ --model-package-group-name ${{ env.MODEL_PACKAGE_GROUP }} \\ --sort-by CreationTime --sort-order Descending --max-items 1 \\ --query \u0026#39;ModelPackageSummaries[0].ModelPackageArn\u0026#39; --output text) fi echo \u0026#34;Using model URI: $MODEL_URI\u0026#34; # Build image docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \\ --build-arg MODEL_URI=$MODEL_URI \\ --build-arg BUILD_ID=$IMAGE_TAG \\ --build-arg ENV=${{ needs.setup.outputs.environment }} \\ server/ # Also tag as latest-{environment} docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \\ $ECR_REGISTRY/$ECR_REPOSITORY:latest-${{ needs.setup.outputs.environment }} # Security scan docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ aquasec/trivy:latest image --severity HIGH,CRITICAL \\ $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG # Push images docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest-${{ needs.setup.outputs.environment }} echo \u0026#34;Image built and pushed: $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG\u0026#34; deploy_eks: name: Deploy to EKS needs: [setup, test, build_docker, model_evaluation] runs-on: ubuntu-latest if: needs.setup.outputs.should_deploy == \u0026#39;true\u0026#39; environment: name: ${{ needs.setup.outputs.environment }} steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Login to Amazon ECR id: login-ecr uses: aws-actions/amazon-ecr-login@v1 - name: Update kubeconfig run: | aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} - name: Check if need to deploy new model id: check-model run: | if [[ \u0026#34;${{ needs.model_evaluation.outputs.model_approved }}\u0026#34; == \u0026#34;true\u0026#34; ]]; then echo \u0026#34;deploy_new_model=true\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT else echo \u0026#34;deploy_new_model=false\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT fi - name: Deploy to EKS run: | # Set environment variables NAMESPACE=\u0026#34;retail-forecast-${{ needs.setup.outputs.environment }}\u0026#34; IMAGE_TAG=\u0026#34;${{ needs.build_docker.outputs.image_tag }}\u0026#34; ECR_REGISTRY=\u0026#34;${{ steps.login-ecr.outputs.registry }}\u0026#34; # Update kustomization file with new image cd aws/k8s kustomize edit set image retail-forecast-api=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG # Apply changes kubectl apply -k overlays/${{ needs.setup.outputs.environment }}/ --namespace $NAMESPACE # Wait for deployment to complete kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE --timeout=300s - name: Create SageMaker endpoint (if new model approved) if: steps.check-model.outputs.deploy_new_model == \u0026#39;true\u0026#39; run: | python aws/script/deploy_endpoint.py \\ --model-package-arn \u0026#34;arn:aws:sagemaker:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:model-package/${{ env.MODEL_PACKAGE_GROUP }}/${{ needs.model_evaluation.outputs.model_version }}\u0026#34; \\ --endpoint-name \u0026#34;retail-forecast-${{ needs.setup.outputs.environment }}\u0026#34; \\ --instance-type ml.t2.medium \\ --instance-count 1 - name: Health check run: | # Get service endpoint NAMESPACE=\u0026#34;retail-forecast-${{ needs.setup.outputs.environment }}\u0026#34; SERVICE_HOST=$(kubectl get ingress -n $NAMESPACE retail-forecast-ingress -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) # If running locally, use port-forwarding if [ -z \u0026#34;$SERVICE_HOST\u0026#34; ]; then echo \u0026#34;No external hostname found, using port-forwarding\u0026#34; kubectl port-forward svc/retail-forecast-service 8080:80 -n $NAMESPACE \u0026amp; sleep 5 SERVICE_HOST=\u0026#34;localhost:8080\u0026#34; fi # Check health endpoint echo \u0026#34;Testing health endpoint: http://$SERVICE_HOST/health\u0026#34; HEALTH_STATUS=$(curl -s -o /dev/null -w \u0026#34;%{http_code}\u0026#34; http://$SERVICE_HOST/health) if [ \u0026#34;$HEALTH_STATUS\u0026#34; -eq 200 ]; then echo \u0026#34;Health check passed: $HEALTH_STATUS\u0026#34; else echo \u0026#34;Health check failed: $HEALTH_STATUS\u0026#34; exit 1 fi # Test prediction endpoint echo \u0026#34;Testing prediction endpoint\u0026#34; PREDICTION_RESULT=$(curl -s -X POST \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;store_id\u0026#34;: 1, \u0026#34;item_id\u0026#34;: 123, \u0026#34;date\u0026#34;: \u0026#34;2023-06-15\u0026#34;}\u0026#39; \\ http://$SERVICE_HOST/predict) echo \u0026#34;Prediction result: $PREDICTION_RESULT\u0026#34; # Check if response contains prediction if [[ $PREDICTION_RESULT == *\u0026#34;prediction\u0026#34;* ]]; then echo \u0026#34;Prediction endpoint working correctly\u0026#34; else echo \u0026#34;Prediction endpoint not returning expected response\u0026#34; exit 1 fi monitoring: name: Setup Monitoring needs: [setup, deploy_eks] runs-on: ubuntu-latest if: always() \u0026amp;\u0026amp; needs.deploy_eks.result == \u0026#39;success\u0026#39; steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup CloudWatch alarms for new deployment run: | # Create/update CloudWatch alarms for API and model performance NAMESPACE=\u0026#34;retail-forecast-${{ needs.setup.outputs.environment }}\u0026#34; DEPLOYMENT_ID=\u0026#34;${{ needs.setup.outputs.build-id }}\u0026#34; aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;RetailForecast-API-Error-Rate-$NAMESPACE\u0026#34; \\ --alarm-description \u0026#34;Monitor error rate for Retail Forecast API\u0026#34; \\ --metric-name \u0026#34;ErrorRate\u0026#34; \\ --namespace \u0026#34;RetailForecast/$NAMESPACE\u0026#34; \\ --statistic \u0026#34;Average\u0026#34; \\ --period 60 \\ --threshold 5 \\ --comparison-operator \u0026#34;GreaterThanThreshold\u0026#34; \\ --evaluation-periods 3 \\ --alarm-actions \u0026#34;arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts\u0026#34; \\ --dimensions \u0026#34;Name=DeploymentId,Value=$DEPLOYMENT_ID\u0026#34; aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;RetailForecast-API-Latency-$NAMESPACE\u0026#34; \\ --alarm-description \u0026#34;Monitor latency for Retail Forecast API\u0026#34; \\ --metric-name \u0026#34;Latency\u0026#34; \\ --namespace \u0026#34;RetailForecast/$NAMESPACE\u0026#34; \\ --statistic \u0026#34;Average\u0026#34; \\ --period 60 \\ --threshold 1000 \\ --comparison-operator \u0026#34;GreaterThanThreshold\u0026#34; \\ --evaluation-periods 3 \\ --alarm-actions \u0026#34;arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts\u0026#34; \\ --dimensions \u0026#34;Name=DeploymentId,Value=$DEPLOYMENT_ID\u0026#34; echo \u0026#34;CloudWatch alarms configured successfully\u0026#34; notify: name: Send Notifications needs: [setup, deploy_eks, monitoring] runs-on: ubuntu-latest if: always() steps: - name: Determine workflow status id: status run: | if [[ \u0026#34;${{ needs.deploy_eks.result }}\u0026#34; == \u0026#34;success\u0026#34; ]]; then echo \u0026#34;result=success\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;message=âœ… Deployment successful for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT elif [[ \u0026#34;${{ needs.deploy_eks.result }}\u0026#34; == \u0026#34;failure\u0026#34; ]]; then echo \u0026#34;result=failure\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;message=âŒ Deployment failed for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT else echo \u0026#34;result=skipped\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;message=â„¹ï¸ Deployment skipped for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT fi - name: Configure AWS credentials if: steps.status.outputs.result != \u0026#39;skipped\u0026#39; uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Send SNS notification if: steps.status.outputs.result != \u0026#39;skipped\u0026#39; run: | aws sns publish \\ --topic-arn \u0026#34;arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts\u0026#34; \\ --subject \u0026#34;Retail Forecast Deployment: ${{ steps.status.outputs.result }}\u0026#34; \\ --message \u0026#34;${{ steps.status.outputs.message }}\u0026#34; 3. IAM Role vÃ  Quyá»n háº¡n Äá»ƒ GitHub Actions cÃ³ thá»ƒ tÆ°Æ¡ng tÃ¡c vá»›i cÃ¡c dá»‹ch vá»¥ AWS, chÃºng ta cáº§n thiáº¿t láº­p IAM Role vá»›i quyá»n háº¡n phÃ¹ há»£p vÃ  sá»­ dá»¥ng OIDC Ä‘á»ƒ xÃ¡c thá»±c.\n3.1 Táº¡o IAM Role cho GitHub Actions # Táº¡o IAM Role cho GitHub Actions cat \u0026gt; trust-policy.json \u0026lt;\u0026lt; EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:oidc-provider/token.actions.githubusercontent.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:aud\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34; }, \u0026#34;StringLike\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:sub\u0026#34;: \u0026#34;repo:\u0026lt;GITHUB_ORG\u0026gt;/\u0026lt;REPO_NAME\u0026gt;:*\u0026#34; } } } ] } EOF # Táº¡o role aws iam create-role --role-name GitHubActionsRole \\ --assume-role-policy-document file://trust-policy.json \\ --description \u0026#34;Role for GitHub Actions CI/CD pipeline\u0026#34; 3.2 IAM Policy cho CI/CD Pipeline { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::retail-forecast-data*\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-data*/*\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-models*\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-models*/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sagemaker:CreateTrainingJob\u0026#34;, \u0026#34;sagemaker:DescribeTrainingJob\u0026#34;, \u0026#34;sagemaker:StopTrainingJob\u0026#34;, \u0026#34;sagemaker:ListTrainingJobs\u0026#34;, \u0026#34;sagemaker:CreateProcessingJob\u0026#34;, \u0026#34;sagemaker:DescribeProcessingJob\u0026#34;, \u0026#34;sagemaker:StopProcessingJob\u0026#34;, \u0026#34;sagemaker:CreateModel\u0026#34;, \u0026#34;sagemaker:DeleteModel\u0026#34;, \u0026#34;sagemaker:DescribeModel\u0026#34;, \u0026#34;sagemaker:CreateEndpoint\u0026#34;, \u0026#34;sagemaker:DescribeEndpoint\u0026#34;, \u0026#34;sagemaker:DeleteEndpoint\u0026#34;, \u0026#34;sagemaker:UpdateEndpoint\u0026#34;, \u0026#34;sagemaker:CreateEndpointConfig\u0026#34;, \u0026#34;sagemaker:DeleteEndpointConfig\u0026#34;, \u0026#34;sagemaker:DescribeEndpointConfig\u0026#34;, \u0026#34;sagemaker:CreateModelPackageGroup\u0026#34;, \u0026#34;sagemaker:DescribeModelPackageGroup\u0026#34;, \u0026#34;sagemaker:ListModelPackageGroups\u0026#34;, \u0026#34;sagemaker:CreateModelPackage\u0026#34;, \u0026#34;sagemaker:DescribeModelPackage\u0026#34;, \u0026#34;sagemaker:ListModelPackages\u0026#34;, \u0026#34;sagemaker:UpdateModelPackage\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetAuthorizationToken\u0026#34;, \u0026#34;ecr:BatchCheckLayerAvailability\u0026#34;, \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34;, \u0026#34;ecr:InitiateLayerUpload\u0026#34;, \u0026#34;ecr:UploadLayerPart\u0026#34;, \u0026#34;ecr:CompleteLayerUpload\u0026#34;, \u0026#34;ecr:PutImage\u0026#34;, \u0026#34;ecr:ListImages\u0026#34;, \u0026#34;ecr:DescribeImages\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;eks:DescribeCluster\u0026#34;, \u0026#34;eks:ListClusters\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudwatch:PutMetricData\u0026#34;, \u0026#34;cloudwatch:GetMetricData\u0026#34;, \u0026#34;cloudwatch:PutMetricAlarm\u0026#34;, \u0026#34;cloudwatch:DescribeAlarms\u0026#34;, \u0026#34;cloudwatch:DeleteAlarms\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sns:Publish\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:*:*:retail-forecast-*\u0026#34; } ] } 3.3 Gáº¯n Policy cho Role # Táº¡o vÃ  gáº¯n policy aws iam create-policy \\ --policy-name GitHubActionsPolicy \\ --policy-document file://github-actions-policy.json aws iam attach-role-policy \\ --role-name GitHubActionsRole \\ --policy-arn arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:policy/GitHubActionsPolicy # Gáº¯n thÃªm cÃ¡c managed policy cáº§n thiáº¿t aws iam attach-role-policy \\ --role-name GitHubActionsRole \\ --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy aws iam attach-role-policy \\ --role-name GitHubActionsRole \\ --policy-arn arn:aws:iam::aws:policy/AmazonECR-FullAccess 4. Script há»— trá»£ cho Pipeline 4.1 Script táº¡o Training Job # aws/script/create_training_job.py import boto3 import argparse import json import os from datetime import datetime def create_training_job(job_name, data_bucket, output_bucket, instance_type=\u0026#39;ml.m5.large\u0026#39;, hyperparameters=None): \u0026#34;\u0026#34;\u0026#34; Táº¡o SageMaker training job cho Retail Forecast model \u0026#34;\u0026#34;\u0026#34; sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) if hyperparameters is None: hyperparameters = { \u0026#39;n_estimators\u0026#39;: \u0026#39;100\u0026#39;, \u0026#39;max_depth\u0026#39;: \u0026#39;10\u0026#39;, \u0026#39;random_state\u0026#39;: \u0026#39;42\u0026#39; } elif isinstance(hyperparameters, str): hyperparameters = json.loads(hyperparameters) # Láº¥y SageMaker execution role tá»« mÃ´i trÆ°á»ng hoáº·c táº¡o má»›i role_arn = os.environ.get(\u0026#39;SAGEMAKER_ROLE_ARN\u0026#39;) if not role_arn: # TÃ¬m role máº·c Ä‘á»‹nh iam = boto3.client(\u0026#39;iam\u0026#39;) paginator = iam.get_paginator(\u0026#39;list_roles\u0026#39;) for page in paginator.paginate(): for role in page[\u0026#39;Roles\u0026#39;]: if \u0026#39;AmazonSageMaker-ExecutionRole\u0026#39; in role[\u0026#39;RoleName\u0026#39;]: role_arn = role[\u0026#39;Arn\u0026#39;] break if not role_arn: raise ValueError(\u0026#34;KhÃ´ng tÃ¬m tháº¥y SageMaker execution role\u0026#34;) # Cáº¥u hÃ¬nh training job training_params = { \u0026#39;TrainingJobName\u0026#39;: job_name, \u0026#39;HyperParameters\u0026#39;: hyperparameters, \u0026#39;AlgorithmSpecification\u0026#39;: { \u0026#39;TrainingImage\u0026#39;: \u0026#39;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3\u0026#39;, \u0026#39;TrainingInputMode\u0026#39;: \u0026#39;File\u0026#39; }, \u0026#39;RoleArn\u0026#39;: role_arn, \u0026#39;InputDataConfig\u0026#39;: [ { \u0026#39;ChannelName\u0026#39;: \u0026#39;train\u0026#39;, \u0026#39;DataSource\u0026#39;: { \u0026#39;S3DataSource\u0026#39;: { \u0026#39;S3DataType\u0026#39;: \u0026#39;S3Prefix\u0026#39;, \u0026#39;S3Uri\u0026#39;: f\u0026#39;s3://{data_bucket}/training/\u0026#39;, \u0026#39;S3DataDistributionType\u0026#39;: \u0026#39;FullyReplicated\u0026#39; } }, \u0026#39;ContentType\u0026#39;: \u0026#39;text/csv\u0026#39; }, { \u0026#39;ChannelName\u0026#39;: \u0026#39;validation\u0026#39;, \u0026#39;DataSource\u0026#39;: { \u0026#39;S3DataSource\u0026#39;: { \u0026#39;S3DataType\u0026#39;: \u0026#39;S3Prefix\u0026#39;, \u0026#39;S3Uri\u0026#39;: f\u0026#39;s3://{data_bucket}/validation/\u0026#39;, \u0026#39;S3DataDistributionType\u0026#39;: \u0026#39;FullyReplicated\u0026#39; } }, \u0026#39;ContentType\u0026#39;: \u0026#39;text/csv\u0026#39; } ], \u0026#39;OutputDataConfig\u0026#39;: { \u0026#39;S3OutputPath\u0026#39;: f\u0026#39;s3://{output_bucket}/models/\u0026#39; }, \u0026#39;ResourceConfig\u0026#39;: { \u0026#39;InstanceType\u0026#39;: instance_type, \u0026#39;InstanceCount\u0026#39;: 1, \u0026#39;VolumeSizeInGB\u0026#39;: 30 }, \u0026#39;StoppingCondition\u0026#39;: { \u0026#39;MaxRuntimeInSeconds\u0026#39;: 3600 }, \u0026#39;Tags\u0026#39;: [ {\u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailForecast\u0026#39;}, {\u0026#39;Key\u0026#39;: \u0026#39;CreatedBy\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;GitHubActions\u0026#39;}, {\u0026#39;Key\u0026#39;: \u0026#39;Environment\u0026#39;, \u0026#39;Value\u0026#39;: os.environ.get(\u0026#39;ENVIRONMENT\u0026#39;, \u0026#39;dev\u0026#39;)} ] } # Táº¡o training job response = sagemaker.create_training_job(**training_params) print(f\u0026#34;Training job created: {job_name}\u0026#34;) print(f\u0026#34;ARN: {response[\u0026#39;TrainingJobArn\u0026#39;]}\u0026#34;) return response[\u0026#39;TrainingJobArn\u0026#39;] if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Create SageMaker training job\u0026#39;) parser.add_argument(\u0026#39;--job-name\u0026#39;, type=str, required=True, help=\u0026#39;Name for the training job\u0026#39;) parser.add_argument(\u0026#39;--data-bucket\u0026#39;, type=str, required=True, help=\u0026#39;S3 bucket containing training data\u0026#39;) parser.add_argument(\u0026#39;--output-bucket\u0026#39;, type=str, required=True, help=\u0026#39;S3 bucket for output artifacts\u0026#39;) parser.add_argument(\u0026#39;--instance-type\u0026#39;, type=str, default=\u0026#39;ml.m5.large\u0026#39;, help=\u0026#39;SageMaker instance type\u0026#39;) parser.add_argument(\u0026#39;--hyperparameters\u0026#39;, type=str, default=None, help=\u0026#39;JSON string of hyperparameters\u0026#39;) args = parser.parse_args() create_training_job( args.job_name, args.data_bucket, args.output_bucket, args.instance_type, args.hyperparameters ) 4.2 Script Ä‘Ã¡nh giÃ¡ Model # aws/script/processing_evaluate.py import boto3 import argparse import json import os import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score def evaluate_model(job_name, evaluation_data, baseline_metrics=None, threshold=0.85): \u0026#34;\u0026#34;\u0026#34;ÄÃ¡nh giÃ¡ model vÃ  so sÃ¡nh vá»›i baseline metrics\u0026#34;\u0026#34;\u0026#34; # Táº¡o thÆ° má»¥c cho plots os.makedirs(\u0026#39;plots\u0026#39;, exist_ok=True) sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) s3 = boto3.client(\u0026#39;s3\u0026#39;) # Láº¥y thÃ´ng tin training job response = sagemaker.describe_training_job(TrainingJobName=job_name) if response[\u0026#39;TrainingJobStatus\u0026#39;] != \u0026#39;Completed\u0026#39;: raise Exception(f\u0026#34;Training job {job_name} chÆ°a hoÃ n thÃ nh\u0026#34;) model_artifacts = response[\u0026#39;ModelArtifacts\u0026#39;][\u0026#39;S3ModelArtifacts\u0026#39;] print(f\u0026#34;Model artifacts: {model_artifacts}\u0026#34;) # Download dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ tá»« S3 if evaluation_data.startswith(\u0026#39;s3://\u0026#39;): bucket, key = evaluation_data.replace(\u0026#39;s3://\u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;/\u0026#39;, 1) local_path = \u0026#39;evaluation_data.csv\u0026#39; s3.download_file(bucket, key, local_path) else: local_path = evaluation_data # Load dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ eval_data = pd.read_csv(local_path) # Trong dá»± Ã¡n thá»±c táº¿, á»Ÿ Ä‘Ã¢y sáº½ load model tá»« S3 vÃ  Ä‘Ã¡nh giÃ¡ # VÃ­ dá»¥ Ä‘Æ¡n giáº£n nÃ y giáº£ Ä‘á»‹nh chÃºng ta Ä‘Ã£ cÃ³ káº¿t quáº£ dá»± Ä‘oÃ¡n # Giáº£ Ä‘á»‹nh káº¿t quáº£ Ä‘Ã¡nh giÃ¡ # Trong thá»±c táº¿, báº¡n sáº½ load model vÃ  thá»±c hiá»‡n dá»± Ä‘oÃ¡n y_true = eval_data[\u0026#39;target\u0026#39;].values y_pred = y_true * 0.9 + np.random.normal(0, 0.2, size=y_true.shape) # TÃ­nh toÃ¡n metrics metrics = { \u0026#39;mse\u0026#39;: mean_squared_error(y_true, y_pred), \u0026#39;rmse\u0026#39;: np.sqrt(mean_squared_error(y_true, y_pred)), \u0026#39;mae\u0026#39;: mean_absolute_error(y_true, y_pred), \u0026#39;r2_score\u0026#39;: r2_score(y_true, y_pred), \u0026#39;accuracy\u0026#39;: 0.88 # Giáº£ Ä‘á»‹nh cho forecasting task } # So sÃ¡nh vá»›i baseline metrics náº¿u cÃ³ baseline = {} if baseline_metrics: if baseline_metrics.startswith(\u0026#39;s3://\u0026#39;): bucket, key = baseline_metrics.replace(\u0026#39;s3://\u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;/\u0026#39;, 1) local_baseline = \u0026#39;baseline_metrics.json\u0026#39; try: s3.download_file(bucket, key, local_baseline) with open(local_baseline, \u0026#39;r\u0026#39;) as f: baseline = json.load(f) except: print(f\u0026#34;KhÃ´ng tÃ¬m tháº¥y baseline metrics: {baseline_metrics}\u0026#34;) else: try: with open(baseline_metrics, \u0026#39;r\u0026#39;) as f: baseline = json.load(f) except: print(f\u0026#34;KhÃ´ng tÃ¬m tháº¥y baseline metrics: {baseline_metrics}\u0026#34;) # Quyáº¿t Ä‘á»‹nh model cÃ³ Ä‘Æ°á»£c cháº¥p nháº­n hay khÃ´ng approved = True if baseline: print(\u0026#34;So sÃ¡nh vá»›i baseline:\u0026#34;) for key in metrics: if key in baseline: improvement = (metrics[key] - baseline[key]) / baseline[key] * 100 print(f\u0026#34;{key}: {metrics[key]:.4f} (baseline: {baseline[key]:.4f}, {improvement:+.2f}%)\u0026#34;) # Kiá»ƒm tra tiÃªu chÃ­ cáº£i thiá»‡n if key == \u0026#39;accuracy\u0026#39; and metrics[key] \u0026lt; threshold: approved = False elif key in [\u0026#39;mse\u0026#39;, \u0026#39;rmse\u0026#39;, \u0026#39;mae\u0026#39;] and metrics[key] \u0026gt; baseline[key] * 1.1: # Cho phÃ©p tá»‡ hÆ¡n 10% approved = False else: print(\u0026#34;KhÃ´ng cÃ³ baseline Ä‘á»ƒ so sÃ¡nh\u0026#34;) # Táº¡o visualizations plt.figure(figsize=(10, 6)) plt.scatter(y_true, y_pred, alpha=0.5) plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \u0026#39;r--\u0026#39;) plt.xlabel(\u0026#39;Actual\u0026#39;) plt.ylabel(\u0026#39;Predicted\u0026#39;) plt.title(\u0026#39;Actual vs Predicted Values\u0026#39;) plt.savefig(\u0026#39;plots/actual_vs_predicted.png\u0026#39;) # Residual plot residuals = y_true - y_pred plt.figure(figsize=(10, 6)) plt.scatter(y_pred, residuals, alpha=0.5) plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors=\u0026#39;r\u0026#39;, linestyles=\u0026#39;--\u0026#39;) plt.xlabel(\u0026#39;Predicted\u0026#39;) plt.ylabel(\u0026#39;Residuals\u0026#39;) plt.title(\u0026#39;Residual Plot\u0026#39;) plt.savefig(\u0026#39;plots/residuals.png\u0026#39;) # LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ result = { \u0026#39;job_name\u0026#39;: job_name, \u0026#39;metrics\u0026#39;: metrics, \u0026#39;baseline\u0026#39;: baseline, \u0026#39;approved\u0026#39;: approved, \u0026#39;model_uri\u0026#39;: model_artifacts } with open(\u0026#39;evaluation_results.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(result, f, indent=2) # LÆ°u káº¿t quáº£ approved Ä‘á»ƒ GitHub Actions cÃ³ thá»ƒ Ä‘á»c with open(\u0026#39;model_approved.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(str(approved).lower()) print(f\u0026#34;Model approved: {approved}\u0026#34;) return approved if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Evaluate trained model\u0026#39;) parser.add_argument(\u0026#39;--job-name\u0026#39;, type=str, required=True, help=\u0026#39;Name of the SageMaker training job\u0026#39;) parser.add_argument(\u0026#39;--evaluation-data\u0026#39;, type=str, required=True, help=\u0026#39;S3 URI or local path to evaluation data\u0026#39;) parser.add_argument(\u0026#39;--baseline-metrics\u0026#39;, type=str, default=None, help=\u0026#39;S3 URI or local path to baseline metrics\u0026#39;) parser.add_argument(\u0026#39;--threshold\u0026#39;, type=float, default=0.85, help=\u0026#39;Accuracy threshold for model approval\u0026#39;) args = parser.parse_args() evaluate_model( args.job_name, args.evaluation_data, args.baseline_metrics, args.threshold ) 4.3 Script Ä‘Äƒng kÃ½ Model Registry # aws/script/register_model.py import boto3 import argparse import json import time def register_model(job_name, model_package_group, approval_status=\u0026#39;PendingManualApproval\u0026#39;): \u0026#34;\u0026#34;\u0026#34;ÄÄƒng kÃ½ model vÃ o Model Registry\u0026#34;\u0026#34;\u0026#34; sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) # Kiá»ƒm tra xem Model Package Group Ä‘Ã£ tá»“n táº¡i chÆ°a try: sagemaker.describe_model_package_group(ModelPackageGroupName=model_package_group) print(f\u0026#34;Model Package Group {model_package_group} Ä‘Ã£ tá»“n táº¡i\u0026#34;) except sagemaker.exceptions.ResourceNotFound: print(f\u0026#34;Táº¡o Model Package Group má»›i: {model_package_group}\u0026#34;) sagemaker.create_model_package_group( ModelPackageGroupName=model_package_group, ModelPackageGroupDescription=\u0026#34;Retail Forecast Models\u0026#34;, Tags=[ {\u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailForecast\u0026#39;} ] ) # Láº¥y thÃ´ng tin training job training_job = sagemaker.describe_training_job(TrainingJobName=job_name) model_data_url = training_job[\u0026#39;ModelArtifacts\u0026#39;][\u0026#39;S3ModelArtifacts\u0026#39;] image_uri = training_job[\u0026#39;AlgorithmSpecification\u0026#39;][\u0026#39;TrainingImage\u0026#39;] # ÄÄƒng kÃ½ model package model_package_name = f\u0026#34;{model_package_group}-{int(time.time())}\u0026#34; create_model_package_input_dict = { \u0026#39;ModelPackageGroupName\u0026#39;: model_package_group, \u0026#39;ModelPackageDescription\u0026#39;: f\u0026#34;Model trained by job {job_name}\u0026#34;, \u0026#39;InferenceSpecification\u0026#39;: { \u0026#39;Containers\u0026#39;: [ { \u0026#39;Image\u0026#39;: image_uri, \u0026#39;ModelDataUrl\u0026#39;: model_data_url } ], \u0026#39;SupportedContentTypes\u0026#39;: [\u0026#39;text/csv\u0026#39;], \u0026#39;SupportedResponseMIMETypes\u0026#39;: [\u0026#39;text/csv\u0026#39;] }, \u0026#39;ModelApprovalStatus\u0026#39;: approval_status, \u0026#39;CustomerMetadataProperties\u0026#39;: { \u0026#39;TrainingJobName\u0026#39;: job_name, \u0026#39;CreatedBy\u0026#39;: \u0026#39;GitHubActionsCI\u0026#39; }, \u0026#39;Tags\u0026#39;: [ {\u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailForecast\u0026#39;} ] } # ThÃªm model metrics náº¿u cÃ³ try: with open(\u0026#39;evaluation_results.json\u0026#39;, \u0026#39;r\u0026#39;) as f: eval_results = json.load(f) model_metrics = [] for metric_name, value in eval_results.get(\u0026#39;metrics\u0026#39;, {}).items(): model_metrics.append({ \u0026#39;Name\u0026#39;: metric_name, \u0026#39;Value\u0026#39;: float(value) }) if model_metrics: create_model_package_input_dict[\u0026#39;ModelMetrics\u0026#39;] = { \u0026#39;ModelQuality\u0026#39;: { \u0026#39;Statistics\u0026#39;: { \u0026#39;ContentType\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Values\u0026#39;: model_metrics } } } except Exception as e: print(f\u0026#34;KhÃ´ng thá»ƒ Ä‘á»c káº¿t quáº£ Ä‘Ã¡nh giÃ¡: {e}\u0026#34;) response = sagemaker.create_model_package(**create_model_package_input_dict) print(f\u0026#34;Model package Ä‘Ã£ Ä‘Æ°á»£c táº¡o: {response[\u0026#39;ModelPackageArn\u0026#39;]}\u0026#34;) # Láº¥y version tá»« ARN model_version = response[\u0026#39;ModelPackageArn\u0026#39;].split(\u0026#39;/\u0026#39;)[-1] print(f\u0026#34;Model version: {model_version}\u0026#34;) return model_version if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Register model to SageMaker Model Registry\u0026#39;) parser.add_argument(\u0026#39;--job-name\u0026#39;, type=str, required=True, help=\u0026#39;Name of the training job\u0026#39;) parser.add_argument(\u0026#39;--model-package-group\u0026#39;, type=str, required=True, help=\u0026#39;Model package group name\u0026#39;) parser.add_argument(\u0026#39;--approval-status\u0026#39;, type=str, default=\u0026#39;PendingManualApproval\u0026#39;, choices=[\u0026#39;Approved\u0026#39;, \u0026#39;Rejected\u0026#39;, \u0026#39;PendingManualApproval\u0026#39;], help=\u0026#39;Initial model approval status\u0026#39;) args = parser.parse_args() model_version = register_model( args.job_name, args.model_package_group, args.approval_status ) print(model_version) 4.4 Script triá»ƒn khai Endpoint # aws/script/deploy_endpoint.py import boto3 import argparse import time import uuid def deploy_endpoint(model_package_arn, endpoint_name, instance_type, instance_count=1): \u0026#34;\u0026#34;\u0026#34;Deploy model to SageMaker endpoint\u0026#34;\u0026#34;\u0026#34; sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) timestamp = int(time.time()) # Táº¡o model model_name = f\u0026#34;{endpoint_name}-model-{timestamp}\u0026#34; print(f\u0026#34;Creating model {model_name} from {model_package_arn}\u0026#34;) model_response = sagemaker.create_model( ModelName=model_name, PrimaryContainer={ \u0026#39;ModelPackageName\u0026#39;: model_package_arn }, ExecutionRoleArn=sagemaker.get_caller_identity()[\u0026#39;RoleArn\u0026#39;] ) # Táº¡o endpoint config config_name = f\u0026#34;{endpoint_name}-config-{timestamp}\u0026#34; print(f\u0026#34;Creating endpoint config {config_name}\u0026#34;) endpoint_config_response = sagemaker.create_endpoint_config( EndpointConfigName=config_name, ProductionVariants=[ { \u0026#39;VariantName\u0026#39;: \u0026#39;default\u0026#39;, \u0026#39;ModelName\u0026#39;: model_name, \u0026#39;InstanceType\u0026#39;: instance_type, \u0026#39;InitialInstanceCount\u0026#39;: instance_count, \u0026#39;InitialVariantWeight\u0026#39;: 1.0 } ], Tags=[ {\u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailForecast\u0026#39;} ] ) # Kiá»ƒm tra xem endpoint Ä‘Ã£ tá»“n táº¡i chÆ°a endpoint_exists = False try: response = sagemaker.describe_endpoint(EndpointName=endpoint_name) endpoint_exists = True except sagemaker.exceptions.ClientError: endpoint_exists = False # Táº¡o hoáº·c cáº­p nháº­t endpoint if endpoint_exists: print(f\u0026#34;Updating endpoint {endpoint_name}\u0026#34;) endpoint_response = sagemaker.update_endpoint( EndpointName=endpoint_name, EndpointConfigName=config_name ) else: print(f\u0026#34;Creating endpoint {endpoint_name}\u0026#34;) endpoint_response = sagemaker.create_endpoint( EndpointName=endpoint_name, EndpointConfigName=config_name, Tags=[ {\u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailForecast\u0026#39;} ] ) # Äá»£i endpoint sáºµn sÃ ng print(f\u0026#34;Waiting for endpoint {endpoint_name} to be in service...\u0026#34;) waiter = sagemaker.get_waiter(\u0026#39;endpoint_in_service\u0026#39;) waiter.wait(EndpointName=endpoint_name) # Láº¥y thÃ´ng tin endpoint endpoint_info = sagemaker.describe_endpoint(EndpointName=endpoint_name) print(f\u0026#34;Endpoint {endpoint_name} is ready (status: {endpoint_info[\u0026#39;EndpointStatus\u0026#39;]})\u0026#34;) return { \u0026#39;endpoint_name\u0026#39;: endpoint_name, \u0026#39;endpoint_arn\u0026#39;: endpoint_info[\u0026#39;EndpointArn\u0026#39;], \u0026#39;status\u0026#39;: endpoint_info[\u0026#39;EndpointStatus\u0026#39;] } if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Deploy model to SageMaker endpoint\u0026#39;) parser.add_argument(\u0026#39;--model-package-arn\u0026#39;, type=str, required=True, help=\u0026#39;ARN of model package to deploy\u0026#39;) parser.add_argument(\u0026#39;--endpoint-name\u0026#39;, type=str, required=True, help=\u0026#39;Name of the endpoint\u0026#39;) parser.add_argument(\u0026#39;--instance-type\u0026#39;, type=str, default=\u0026#39;ml.t2.medium\u0026#39;, help=\u0026#39;Instance type for inference\u0026#39;) parser.add_argument(\u0026#39;--instance-count\u0026#39;, type=int, default=1, help=\u0026#39;Number of instances\u0026#39;) args = parser.parse_args() result = deploy_endpoint( args.model_package_arn, args.endpoint_name, args.instance_type, args.instance_count ) print(f\u0026#34;Endpoint deployment complete: {result}\u0026#34;) 4.5 Script kiá»ƒm tra hiá»‡u suáº¥t Endpoint # aws/script/autoscaling_endpoint.py import boto3 import argparse import json import time import datetime def setup_autoscaling(endpoint_name, min_capacity=1, max_capacity=4, target_value=70.0, scale_in_cooldown=300, scale_out_cooldown=60): \u0026#34;\u0026#34;\u0026#34;Thiáº¿t láº­p autoscaling cho SageMaker endpoint\u0026#34;\u0026#34;\u0026#34; # Láº¥y variant name vÃ  ARN cá»§a endpoint sm = boto3.client(\u0026#39;sagemaker\u0026#39;) endpoint = sm.describe_endpoint(EndpointName=endpoint_name) endpoint_arn = endpoint[\u0026#39;EndpointArn\u0026#39;] config_name = endpoint[\u0026#39;EndpointConfigName\u0026#39;] config = sm.describe_endpoint_config(EndpointConfigName=config_name) variant_name = config[\u0026#39;ProductionVariants\u0026#39;][0][\u0026#39;VariantName\u0026#39;] # Chuáº©n bá»‹ resource ID cho autoscaling resource_id = f\u0026#34;endpoint/{endpoint_name}/variant/{variant_name}\u0026#34; # Thiáº¿t láº­p application autoscaling aas = boto3.client(\u0026#39;application-autoscaling\u0026#39;) # ÄÄƒng kÃ½ scalable target aas.register_scalable_target( ServiceNamespace=\u0026#39;sagemaker\u0026#39;, ResourceId=resource_id, ScalableDimension=\u0026#39;sagemaker:variant:DesiredInstanceCount\u0026#39;, MinCapacity=min_capacity, MaxCapacity=max_capacity ) # Táº¡o scaling policy response = aas.put_scaling_policy( PolicyName=f\u0026#34;{endpoint_name}-scaling-policy\u0026#34;, ServiceNamespace=\u0026#39;sagemaker\u0026#39;, ResourceId=resource_id, ScalableDimension=\u0026#39;sagemaker:variant:DesiredInstanceCount\u0026#39;, PolicyType=\u0026#39;TargetTrackingScaling\u0026#39;, TargetTrackingScalingPolicyConfiguration={ \u0026#39;TargetValue\u0026#39;: target_value, \u0026#39;PredefinedMetricSpecification\u0026#39;: { \u0026#39;PredefinedMetricType\u0026#39;: \u0026#39;SageMakerVariantInvocationsPerInstance\u0026#39; }, \u0026#39;ScaleInCooldown\u0026#39;: scale_in_cooldown, \u0026#39;ScaleOutCooldown\u0026#39;: scale_out_cooldown } ) print(f\u0026#34;Autoscaling configured for endpoint {endpoint_name}\u0026#34;) print(f\u0026#34;Min capacity: {min_capacity}, Max capacity: {max_capacity}\u0026#34;) print(f\u0026#34;Target value: {target_value} invocations per instance\u0026#34;) return { \u0026#39;endpoint_name\u0026#39;: endpoint_name, \u0026#39;resource_id\u0026#39;: resource_id, \u0026#39;scaling_policy_arn\u0026#39;: response[\u0026#39;PolicyARN\u0026#39;] } def load_test_endpoint(endpoint_name, test_data_path, duration=60, rate=10): \u0026#34;\u0026#34;\u0026#34;Test táº£i endpoint Ä‘á»ƒ kiá»ƒm tra hiá»‡u suáº¥t vÃ  autoscaling\u0026#34;\u0026#34;\u0026#34; sagemaker_runtime = boto3.client(\u0026#39;sagemaker-runtime\u0026#39;) # Load test data with open(test_data_path, \u0026#39;r\u0026#39;) as f: test_data = json.load(f) # Náº¿u test data lÃ  list, láº¥y pháº§n tá»­ Ä‘áº§u tiÃªn if isinstance(test_data, list): sample = test_data[0] else: sample = test_data # Chuáº©n bá»‹ test start_time = time.time() end_time = start_time + duration request_count = 0 success_count = 0 latencies = [] print(f\u0026#34;Starting load test on endpoint {endpoint_name}\u0026#34;) print(f\u0026#34;Duration: {duration} seconds, Rate: {rate} requests/second\u0026#34;) # Thá»±c hiá»‡n load test while time.time() \u0026lt; end_time: batch_start = time.time() for _ in range(rate): if time.time() \u0026gt;= end_time: break try: # Gá»­i request request_start = time.time() response = sagemaker_runtime.invoke_endpoint( EndpointName=endpoint_name, ContentType=\u0026#39;application/json\u0026#39;, Body=json.dumps(sample) ) # Äo latency latency = (time.time() - request_start) * 1000 # milliseconds latencies.append(latency) # Äá»c káº¿t quáº£ result = json.loads(response[\u0026#39;Body\u0026#39;].read().decode()) # Cáº­p nháº­t counter request_count += 1 success_count += 1 if request_count % 50 == 0: print(f\u0026#34;Processed {request_count} requests...\u0026#34;) except Exception as e: request_count += 1 print(f\u0026#34;Error invoking endpoint: {e}\u0026#34;) # Äá»£i Ä‘áº¿n Ä‘áº§u giÃ¢y tiáº¿p theo elapsed = time.time() - batch_start if elapsed \u0026lt; 1.0: time.sleep(1.0 - elapsed) # TÃ­nh toÃ¡n káº¿t quáº£ total_time = time.time() - start_time avg_rate = request_count / total_time success_rate = (success_count / request_count) * 100 if request_count \u0026gt; 0 else 0 if latencies: avg_latency = sum(latencies) / len(latencies) min_latency = min(latencies) max_latency = max(latencies) p95_latency = sorted(latencies)[int(len(latencies) * 0.95)] p99_latency = sorted(latencies)[int(len(latencies) * 0.99)] else: avg_latency = min_latency = max_latency = p95_latency = p99_latency = 0 # In káº¿t quáº£ results = { \u0026#39;endpoint\u0026#39;: endpoint_name, \u0026#39;duration_seconds\u0026#39;: total_time, \u0026#39;request_count\u0026#39;: request_count, \u0026#39;success_count\u0026#39;: success_count, \u0026#39;success_rate_percent\u0026#39;: success_rate, \u0026#39;avg_requests_per_second\u0026#39;: avg_rate, \u0026#39;latency_ms\u0026#39;: { \u0026#39;avg\u0026#39;: avg_latency, \u0026#39;min\u0026#39;: min_latency, \u0026#39;max\u0026#39;: max_latency, \u0026#39;p95\u0026#39;: p95_latency, \u0026#39;p99\u0026#39;: p99_latency } } print(\u0026#34;\\nLoad Test Results:\u0026#34;) print(json.dumps(results, indent=2)) with open(\u0026#39;load_test_results.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(results, f, indent=2) return results if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Setup autoscaling and test SageMaker endpoint\u0026#39;) subparsers = parser.add_subparsers(dest=\u0026#39;command\u0026#39;) # Subparser cho autoscaling autoscaling_parser = subparsers.add_parser(\u0026#39;autoscale\u0026#39;) autoscaling_parser.add_argument(\u0026#39;--endpoint-name\u0026#39;, type=str, required=True) autoscaling_parser.add_argument(\u0026#39;--min-capacity\u0026#39;, type=int, default=1) autoscaling_parser.add_argument(\u0026#39;--max-capacity\u0026#39;, type=int, default=4) autoscaling_parser.add_argument(\u0026#39;--target-value\u0026#39;, type=float, default=70.0) # Subparser cho load testing loadtest_parser = subparsers.add_parser(\u0026#39;loadtest\u0026#39;) loadtest_parser.add_argument(\u0026#39;--endpoint-name\u0026#39;, type=str, required=True) loadtest_parser.add_argument(\u0026#39;--test-data\u0026#39;, type=str, required=True) loadtest_parser.add_argument(\u0026#39;--duration\u0026#39;, type=int, default=60) loadtest_parser.add_argument(\u0026#39;--rate\u0026#39;, type=int, default=10) args = parser.parse_args() if args.command == \u0026#39;autoscale\u0026#39;: setup_autoscaling( args.endpoint_name, args.min_capacity, args.max_capacity, args.target_value ) elif args.command == \u0026#39;loadtest\u0026#39;: load_test_endpoint( args.endpoint_name, args.test_data, args.duration, args.rate ) else: parser.print_help() 5. GiÃ¡m sÃ¡t vÃ  ThÃ´ng bÃ¡o 5.1 Thiáº¿t láº­p SNS Topic # Táº¡o SNS topic cho thÃ´ng bÃ¡o deployment aws sns create-topic --name retail-forecast-alerts # ÄÄƒng kÃ½ email nháº­n thÃ´ng bÃ¡o aws sns subscribe \\ --topic-arn arn:aws:sns:us-east-1:\u0026lt;ACCOUNT_ID\u0026gt;:retail-forecast-alerts \\ --protocol email \\ --notification-endpoint team@example.com # ÄÄƒng kÃ½ webhook Slack aws sns subscribe \\ --topic-arn arn:aws:sns:us-east-1:\u0026lt;ACCOUNT_ID\u0026gt;:retail-forecast-alerts \\ --protocol https \\ --notification-endpoint https://hooks.slack.com/services/XXXX/YYYY/ZZZZ 5.2 CloudWatch Dashboard cho CI/CD Pipeline # Táº¡o CloudWatch dashboard cho pipeline aws cloudwatch put-dashboard \\ --dashboard-name RetailForecast-CI-CD-Pipeline \\ --dashboard-body \u0026#39;{ \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 0, \u0026#34;width\u0026#34;: 12, \u0026#34;height\u0026#34;: 6, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;AWS/SageMaker\u0026#34;, \u0026#34;TrainingJobsCompleted\u0026#34;, { \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;period\u0026#34;: 86400 } ], [ \u0026#34;AWS/SageMaker\u0026#34;, \u0026#34;TrainingJobsFailed\u0026#34;, { \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;period\u0026#34;: 86400 } ] ], \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;stacked\u0026#34;: false, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;SageMaker Training Jobs\u0026#34;, \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;x\u0026#34;: 12, \u0026#34;y\u0026#34;: 0, \u0026#34;width\u0026#34;: 12, \u0026#34;height\u0026#34;: 6, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;AWS/ApplicationELB\u0026#34;, \u0026#34;TargetResponseTime\u0026#34;, \u0026#34;LoadBalancer\u0026#34;, \u0026#34;app/retail-forecast/abcdef\u0026#34; ], [ \u0026#34;.\u0026#34;, \u0026#34;HTTPCode_Target_5XX_Count\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34; ], [ \u0026#34;.\u0026#34;, \u0026#34;HTTPCode_Target_4XX_Count\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34; ] ], \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;stacked\u0026#34;: false, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;API Performance\u0026#34;, \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 6, \u0026#34;width\u0026#34;: 12, \u0026#34;height\u0026#34;: 6, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;AWS/SageMaker\u0026#34;, \u0026#34;Invocations\u0026#34;, \u0026#34;EndpointName\u0026#34;, \u0026#34;retail-forecast-prod\u0026#34;, { \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;period\u0026#34;: 60 } ], [ \u0026#34;.\u0026#34;, \u0026#34;InvocationsPerInstance\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34; ] ], \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;stacked\u0026#34;: false, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;SageMaker Endpoint Invocations\u0026#34;, \u0026#34;period\u0026#34;: 300 } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;x\u0026#34;: 12, \u0026#34;y\u0026#34;: 6, \u0026#34;width\u0026#34;: 12, \u0026#34;height\u0026#34;: 6, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;RetailForecast/Pipeline\u0026#34;, \u0026#34;DeploymentFrequency\u0026#34; ], [ \u0026#34;.\u0026#34;, \u0026#34;FailureRate\u0026#34; ], [ \u0026#34;.\u0026#34;, \u0026#34;LeadTime\u0026#34; ] ], \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;stacked\u0026#34;: false, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;CI/CD Metrics\u0026#34;, \u0026#34;period\u0026#34;: 86400, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34; } } ] }\u0026#39; 5.3 Cáº¥u hÃ¬nh Pipeline Metrics # aws/script/pipeline_metrics.py import boto3 import json import argparse from datetime import datetime, timedelta def publish_pipeline_metrics(deploy_success=True, lead_time=None): \u0026#34;\u0026#34;\u0026#34;Publish CI/CD pipeline metrics to CloudWatch\u0026#34;\u0026#34;\u0026#34; cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) # Get date parts for daily metrics now = datetime.utcnow() today = now.strftime(\u0026#39;%Y-%m-%d\u0026#39;) # Increment deployment count cloudwatch.put_metric_data( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;DeploymentCount\u0026#39;, \u0026#39;Dimensions\u0026#39;: [ { \u0026#39;Name\u0026#39;: \u0026#39;Date\u0026#39;, \u0026#39;Value\u0026#39;: today } ], \u0026#39;Value\u0026#39;: 1.0, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39; } ] ) # Add deployment success/failure cloudwatch.put_metric_data( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;DeploymentSuccess\u0026#39;, \u0026#39;Dimensions\u0026#39;: [ { \u0026#39;Name\u0026#39;: \u0026#39;Date\u0026#39;, \u0026#39;Value\u0026#39;: today } ], \u0026#39;Value\u0026#39;: 1.0 if deploy_success else 0.0, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39; } ] ) # Add lead time if provided if lead_time is not None: cloudwatch.put_metric_data( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;LeadTimeMinutes\u0026#39;, \u0026#39;Dimensions\u0026#39;: [ { \u0026#39;Name\u0026#39;: \u0026#39;Date\u0026#39;, \u0026#39;Value\u0026#39;: today } ], \u0026#39;Value\u0026#39;: lead_time, \u0026#39;Unit\u0026#39;: \u0026#39;Minutes\u0026#39; } ] ) print(f\u0026#34;Pipeline metrics published successfully\u0026#34;) def calculate_pipeline_kpis(days=30): \u0026#34;\u0026#34;\u0026#34;Calculate KPIs for CI/CD pipeline\u0026#34;\u0026#34;\u0026#34; cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) end_time = datetime.utcnow() start_time = end_time - timedelta(days=days) # Get deployment frequency deploy_count_response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricName=\u0026#39;DeploymentCount\u0026#39;, StartTime=start_time, EndTime=end_time, Period=86400 * days, # entire period Statistics=[\u0026#39;Sum\u0026#39;] ) # Get deployment success/failure deploy_success_response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricName=\u0026#39;DeploymentSuccess\u0026#39;, StartTime=start_time, EndTime=end_time, Period=86400 * days, # entire period Statistics=[\u0026#39;Sum\u0026#39;] ) # Get lead time lead_time_response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricName=\u0026#39;LeadTimeMinutes\u0026#39;, StartTime=start_time, EndTime=end_time, Period=86400 * days, # entire period Statistics=[\u0026#39;Average\u0026#39;] ) # Calculate KPIs deploy_count = deploy_count_response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Sum\u0026#39;] if deploy_count_response[\u0026#39;Datapoints\u0026#39;] else 0 deploy_success = deploy_success_response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Sum\u0026#39;] if deploy_success_response[\u0026#39;Datapoints\u0026#39;] else 0 # Calculate metrics deployments_per_day = deploy_count / days if days \u0026gt; 0 else 0 failure_rate = (deploy_count - deploy_success) / deploy_count * 100 if deploy_count \u0026gt; 0 else 0 avg_lead_time = lead_time_response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Average\u0026#39;] if lead_time_response[\u0026#39;Datapoints\u0026#39;] else 0 kpis = { \u0026#39;period_days\u0026#39;: days, \u0026#39;total_deployments\u0026#39;: deploy_count, \u0026#39;successful_deployments\u0026#39;: deploy_success, \u0026#39;deployments_per_day\u0026#39;: deployments_per_day, \u0026#39;failure_rate_percent\u0026#39;: failure_rate, \u0026#39;avg_lead_time_minutes\u0026#39;: avg_lead_time } print(json.dumps(kpis, indent=2)) return kpis if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Manage CI/CD pipeline metrics\u0026#39;) subparsers = parser.add_subparsers(dest=\u0026#39;command\u0026#39;) # Subparser cho publish metrics publish_parser = subparsers.add_parser(\u0026#39;publish\u0026#39;) publish_parser.add_argument(\u0026#39;--success\u0026#39;, type=bool, default=True) publish_parser.add_argument(\u0026#39;--lead-time\u0026#39;, type=float, help=\u0026#39;Lead time in minutes\u0026#39;) # Subparser cho calculate KPIs kpi_parser = subparsers.add_parser(\u0026#39;kpi\u0026#39;) kpi_parser.add_argument(\u0026#39;--days\u0026#39;, type=int, default=30) args = parser.parse_args() if args.command == \u0026#39;publish\u0026#39;: publish_pipeline_metrics(args.success, args.lead_time) elif args.command == \u0026#39;kpi\u0026#39;: calculate_pipeline_kpis(args.days) else: parser.print_help() 6. Kiá»ƒm tra vÃ  XÃ¡c thá»±c 6.1 Checklist HoÃ n thÃ nh CÃ¡c thÃ nh pháº§n chÃ­nh Ä‘Ã£ triá»ƒn khai:\nPipeline GitHub Actions: Workflow Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh vá»›i cÃ¡c job Ä‘áº§y Ä‘á»§ IAM Role \u0026amp; Permissions: Role cho GitHub Actions vá»›i OIDC xÃ¡c thá»±c Automated Testing: Unit tests, code quality check, data validation Model Training: Tá»± Ä‘á»™ng trigger SageMaker training jobs Model Evaluation \u0026amp; Registry: ÄÃ¡nh giÃ¡ model vÃ  Ä‘Äƒng kÃ½ vÃ o Model Registry Docker Build: Tá»± Ä‘á»™ng build vÃ  push image lÃªn ECR EKS Deployment: Tá»± Ä‘á»™ng cáº­p nháº­t Kubernetes deployment Health Check: Kiá»ƒm tra vÃ  xÃ¡c nháº­n API hoáº¡t Ä‘á»™ng sau deployment CloudWatch Alarms: Tá»± Ä‘á»™ng cáº¥u hÃ¬nh alert cho deployment má»›i Notifications: ThÃ´ng bÃ¡o káº¿t quáº£ deployment qua SNS GitHub Actions Pipeline: Workflows configured with full jobs IAM Role \u0026amp; Permissions: Role for GitHub Actions with OIDC authentication Automated Testing: Unit tests, code quality checks, data validation Model Training: Automatically trigger SageMaker training jobs Model Evaluation \u0026amp; Registry: Evaluate models and register to the Model Registry Docker Build: Automatically build and push images to ECR EKS Deployment: Automatically update Kubernetes deployments Health Check: Verify API health after deployment CloudWatch Alarms: Automatically configure alerts for new deployments Notifications: Deployment results notified via SNS 6.2 Pipeline Testing To verify the pipeline operates correctly, perform the following steps:\nTrigger the pipeline by pushing a new commit # Create a test commit to trigger the CI/CD pipeline echo \u0026#34;# Test CI/CD pipeline\u0026#34; \u0026gt;\u0026gt; README.md git add README.md git commit -m \u0026#34;Test: Trigger CI/CD pipeline\u0026#34; git push origin main # Kiá»ƒm tra GitHub Actions # Pipeline sáº½ tá»± Ä‘á»™ng kÃ­ch hoáº¡t trong vÃ²ng 1 phÃºt KÃ­ch hoáº¡t pipeline thá»§ cÃ´ng vá»›i huáº¥n luyá»‡n model\n# Sá»­ dá»¥ng GitHub UI Ä‘á»ƒ kÃ­ch hoáº¡t workflow vá»›i cÃ¡c tÃ¹y chá»n: # - environment: prod # - retrain_model: true Kiá»ƒm tra ECR repository\n# Kiá»ƒm tra image má»›i trÃªn ECR aws ecr describe-images \\ --repository-name retail-forecast \\ --query \u0026#39;sort_by(imageDetails,\u0026amp; imagePushedAt)[-5:]\u0026#39; # Kiá»ƒm tra tag má»›i nháº¥t aws ecr list-images \\ --repository-name retail-forecast \\ --filter \u0026#34;tagStatus=TAGGED\u0026#34; \\ --query \u0026#39;imageIds[?contains(imageTag, `latest`)]\u0026#39; Kiá»ƒm tra SageMaker model vÃ  deployment\n# Kiá»ƒm tra training job má»›i nháº¥t aws sagemaker list-training-jobs \\ --sort-by CreationTime \\ --sort-order Descending \\ --max-items 5 # Kiá»ƒm tra model package má»›i nháº¥t aws sagemaker list-model-packages \\ --model-package-group-name retail-forecast-models \\ --sort-by CreationTime \\ --sort-order Descending \\ --max-items 5 # Kiá»ƒm tra endpoint aws sagemaker describe-endpoint \\ --endpoint-name retail-forecast-prod Kiá»ƒm tra EKS deployment\n# Kiá»ƒm tra deployment má»›i kubectl get deployments -n retail-forecast-prod # Kiá»ƒm tra pods kubectl get pods -n retail-forecast-prod # Kiá»ƒm tra image Ä‘Æ°á»£c sá»­ dá»¥ng kubectl describe deployment retail-forecast-api -n retail-forecast-prod | grep Image: # Kiá»ƒm tra history rollout kubectl rollout history deployment/retail-forecast-api -n retail-forecast-prod Test API endpoint\n# Láº¥y endpoint URL ENDPOINT=$(kubectl get ingress -n retail-forecast-prod retail-forecast-ingress -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) # Kiá»ƒm tra health endpoint curl -v http://$ENDPOINT/health # Test prediction endpoint curl -X POST \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;store_id\u0026#34;: 1, \u0026#34;item_id\u0026#34;: 123, \u0026#34;date\u0026#34;: \u0026#34;2023-12-01\u0026#34;}\u0026#39; \\ http://$ENDPOINT/predict 6.3 Monitoring Commands # Kiá»ƒm tra CloudWatch metrics cá»§a API aws cloudwatch get-metric-statistics \\ --namespace \u0026#34;RetailForecast/prod\u0026#34; \\ --metric-name \u0026#34;Latency\u0026#34; \\ --dimensions Name=DeploymentId,Value=\u0026lt;latest-deployment-id\u0026gt; \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%SZ) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\ --period 60 \\ --statistics Average # Kiá»ƒm tra SageMaker endpoint metrics aws cloudwatch get-metric-statistics \\ --namespace \u0026#34;AWS/SageMaker\u0026#34; \\ --metric-name \u0026#34;ModelLatency\u0026#34; \\ --dimensions Name=EndpointName,Value=retail-forecast-prod \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%SZ) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\ --period 60 \\ --statistics Average # Kiá»ƒm tra Pipeline metrics aws cloudwatch get-metric-statistics \\ --namespace \u0026#34;RetailForecast/Pipeline\u0026#34; \\ --metric-name \u0026#34;DeploymentSuccess\u0026#34; \\ --start-time $(date -u -d \u0026#39;7 days ago\u0026#39; +%Y-%m-%dT%H:%M:%SZ) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\ --period 86400 \\ --statistics Sum # Kiá»ƒm tra CloudWatch Logs cho pipeline aws logs get-log-events \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs\u0026#34; \\ --log-stream-name \u0026lt;latest-training-job-name\u0026gt; 7. Best Practices \u0026amp; Tá»‘i Æ°u 7.1 Pipeline Optimization Parallel Execution: Cháº¡y song song cÃ¡c job khÃ´ng phá»¥ thuá»™c nhau Caching: Sá»­ dá»¥ng caching cho pip vÃ  Docker layers Conditional Execution: Chá»‰ cháº¡y cÃ¡c job cáº§n thiáº¿t dá»±a trÃªn loáº¡i thay Ä‘á»•i Matrix Builds: Test trÃªn nhiá»u phiÃªn báº£n Python hoáº·c mÃ´i trÆ°á»ng Artifact Sharing: Sá»­ dá»¥ng artifacts Ä‘á»ƒ chia sáº» dá»¯ liá»‡u giá»¯a cÃ¡c job 7.2 Security Best Practices OIDC Integration: Sá»­ dá»¥ng federation thay vÃ¬ access keys Least Privilege IAM: Role vá»›i quyá»n háº¡n tá»‘i thiá»ƒu cáº§n thiáº¿t Secret Management: Sá»­ dá»¥ng GitHub Secrets hoáº·c AWS Secrets Manager Image Scanning: Kiá»ƒm tra lá»—i báº£o máº­t trong container images Network Security: Giá»›i háº¡n network access trong pipeline 7.3 Quality Gates Code Quality Checks: Linting, formatting, static analysis Unit \u0026amp; Integration Tests: Test tá»± Ä‘á»™ng cho má»i thay Ä‘á»•i code Data Validation: Kiá»ƒm tra tÃ­nh toÃ n váº¹n vÃ  cháº¥t lÆ°á»£ng dá»¯ liá»‡u Model Evaluation: ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t model vá»›i baseline metrics Performance Testing: Kiá»ƒm tra Ä‘á»™ trá»… vÃ  kháº£ nÄƒng xá»­ lÃ½ táº£i cá»§a API 7.4 Observability Pipeline Metrics: Theo dÃµi táº§n suáº¥t deployment, thá»i gian thá»±c hiá»‡n Model Monitoring: Theo dÃµi model drift vÃ  data drift API Metrics: Latency, error rate, throughput cá»§a endpoints Logging: Log táº­p trung vá»›i cáº¥u trÃºc thá»‘ng nháº¥t Alerting: Cáº£nh bÃ¡o sá»›m khi phÃ¡t hiá»‡n váº¥n Ä‘á» Summary The CI/CD pipeline has been fully implemented using GitHub Actions to automate the MLOps workflow for the Retail Prediction project:\nContinuous Integration (CI): Automatically run tests, linting, and build Docker images. Automated Training: Trigger SageMaker training jobs when needed. Model Registry: Evaluate models and register versions in the Model Registry. Continuous Delivery (CD): Automatically deploy to the EKS cluster. Monitoring: CloudWatch metrics and alerts across the system. This pipeline ensures a reliable, automated, and scalable MLOps process, allowing the team to focus on improving models and features instead of manual operational tasks.\nğŸ¯ Task 11 Complete - CI/CD Pipeline\nGitHub Actions workflow configured with full CI/CD stages IAM Role \u0026amp; OIDC setup for secure authentication Automated testing and quality gates SageMaker integration for model training \u0026amp; registry EKS deployment automation with health checks CloudWatch monitoring and notifications 8. Clean Up CI/CD Resources 8.1 XÃ³a GitHub Actions Workflow # Disable GitHub Actions workflows # Thá»±c hiá»‡n trá»±c tiáº¿p trÃªn GitHub repository hoáº·c qua GitHub CLI # Náº¿u sá»­ dá»¥ng GitHub CLI gh workflow disable mlops-pipeline.yml # List táº¥t cáº£ workflow runs gh run list --limit 50 # Cancel running workflows gh run list --status in_progress --json databaseId --jq \u0026#39;.[].databaseId\u0026#39; | xargs -I {} gh run cancel {} # XÃ³a workflow artifacts (cÃ³ thá»ƒ tá»‘n phÃ­ storage) gh api repos/:owner/:repo/actions/artifacts --paginate | jq -r \u0026#39;.artifacts[] | select(.expired == false) | .id\u0026#39; | xargs -I {} gh api --method DELETE repos/:owner/:repo/actions/artifacts/{} 8.2 XÃ³a IAM Roles vÃ  Policies # List táº¥t cáº£ IAM roles liÃªn quan Ä‘áº¿n CI/CD aws iam list-roles \\ --query \u0026#39;Roles[?contains(RoleName, `GitHub`) || contains(RoleName, `CICD`) || contains(RoleName, `RetailForecast`)].RoleName\u0026#39; \\ --output table # Detach policies trÆ°á»›c khi xÃ³a role aws iam list-attached-role-policies \\ --role-name GitHubActionsRole \\ --query \u0026#39;AttachedPolicies[].PolicyArn\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read policy_arn; do echo \u0026#34;Detaching policy: $policy_arn\u0026#34; aws iam detach-role-policy --role-name GitHubActionsRole --policy-arn \u0026#34;$policy_arn\u0026#34; done # XÃ³a custom policies aws iam delete-policy \\ --policy-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/GitHubActionsPolicy aws iam delete-policy \\ --policy-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/RetailForecastCICDPolicy # XÃ³a roles aws iam delete-role --role-name GitHubActionsRole aws iam delete-role --role-name RetailForecastCICDRole # XÃ³a instance profiles (náº¿u cÃ³) aws iam remove-role-from-instance-profile \\ --instance-profile-name JenkinsInstanceProfile \\ --role-name JenkinsRole || true aws iam delete-instance-profile \\ --instance-profile-name JenkinsInstanceProfile || true aws iam delete-role --role-name JenkinsRole || true 8.3 XÃ³a Jenkins Infrastructure (náº¿u sá»­ dá»¥ng) # Terminate Jenkins EC2 instances aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:Name,Values=Jenkins-Server\u0026#34; \u0026#34;Name=instance-state-name,Values=running\u0026#34; \\ --query \u0026#39;Reservations[].Instances[].InstanceId\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read instance_id; do echo \u0026#34;Terminating Jenkins instance: $instance_id\u0026#34; aws ec2 terminate-instances --instance-ids \u0026#34;$instance_id\u0026#34; done # XÃ³a Jenkins security group JENKINS_SG_ID=$(aws ec2 describe-security-groups \\ --filters \u0026#34;Name=group-name,Values=sg-jenkins\u0026#34; \\ --query \u0026#39;SecurityGroups[0].GroupId\u0026#39; \\ --output text) if [ \u0026#34;$JENKINS_SG_ID\u0026#34; != \u0026#34;None\u0026#34; ]; then aws ec2 delete-security-group --group-id \u0026#34;$JENKINS_SG_ID\u0026#34; fi # XÃ³a Jenkins key pair aws ec2 delete-key-pair --key-name jenkins-key-pair || true 8.4 XÃ³a SNS Topics vÃ  Subscriptions # List táº¥t cáº£ SNS topics liÃªn quan aws sns list-topics \\ --query \u0026#39;Topics[?contains(TopicArn, `retail-forecast`) || contains(TopicArn, `mlops`)].TopicArn\u0026#39; \\ --output table # XÃ³a subscriptions trÆ°á»›c SNS_TOPIC_ARN=$(aws sns list-topics \\ --query \u0026#39;Topics[?contains(TopicArn, `retail-forecast-alerts`)].TopicArn\u0026#39; \\ --output text) if [ \u0026#34;$SNS_TOPIC_ARN\u0026#34; != \u0026#34;\u0026#34; ]; then # List vÃ  xÃ³a subscriptions aws sns list-subscriptions-by-topic \\ --topic-arn \u0026#34;$SNS_TOPIC_ARN\u0026#34; \\ --query \u0026#39;Subscriptions[].SubscriptionArn\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read sub_arn; do if [ \u0026#34;$sub_arn\u0026#34; != \u0026#34;PendingConfirmation\u0026#34; ]; then echo \u0026#34;Unsubscribing: $sub_arn\u0026#34; aws sns unsubscribe --subscription-arn \u0026#34;$sub_arn\u0026#34; fi done # XÃ³a topic aws sns delete-topic --topic-arn \u0026#34;$SNS_TOPIC_ARN\u0026#34; fi 8.5 XÃ³a CloudWatch Resources cho CI/CD # XÃ³a CloudWatch dashboard cho CI/CD aws cloudwatch delete-dashboards \\ --dashboard-names \u0026#34;RetailForecast-CI-CD-Pipeline\u0026#34; || true # XÃ³a custom metrics namespace aws cloudwatch list-metrics \\ --namespace \u0026#34;RetailForecast/Pipeline\u0026#34; \\ --query \u0026#39;Metrics[].MetricName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read metric; do echo \u0026#34;Found pipeline metric: $metric\u0026#34; # Note: CloudWatch metrics tá»± Ä‘á»™ng expire sau 15 thÃ¡ng done # XÃ³a alarms liÃªn quan Ä‘áº¿n CI/CD aws cloudwatch describe-alarms \\ --alarm-name-prefix \u0026#34;Pipeline-\u0026#34; \\ --query \u0026#39;MetricAlarms[].AlarmName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read alarm; do echo \u0026#34;Deleting pipeline alarm: $alarm\u0026#34; aws cloudwatch delete-alarms --alarm-names \u0026#34;$alarm\u0026#34; done # XÃ³a log groups cho CI/CD aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/codebuild/retail-forecast-build\u0026#34; || true aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/codepipeline/retail-forecast-pipeline\u0026#34; || true 8.6 XÃ³a ECR Images vÃ  Tags # List táº¥t cáº£ images trong ECR repository aws ecr describe-images \\ --repository-name mlops/retail-api \\ --region ap-southeast-1 \\ --query \u0026#39;imageDetails[].imageTags[]\u0026#39; \\ --output table # XÃ³a specific CI/CD tags (giá»¯ láº¡i production tags) aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --region ap-southeast-1 \\ --image-ids imageTag=dev imageTag=staging imageTag=feature-* || true # XÃ³a untagged images aws ecr describe-images \\ --repository-name mlops/retail-api \\ --region ap-southeast-1 \\ --filter tagStatus=UNTAGGED \\ --query \u0026#39;imageDetails[].imageDigest\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read digest; do echo \u0026#34;Deleting untagged image: $digest\u0026#34; aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --region ap-southeast-1 \\ --image-ids imageDigest=\u0026#34;$digest\u0026#34; done 8.7 Clean Up SageMaker Resources # List training jobs created by CI/CD aws sagemaker list-training-jobs \\ --name-contains \u0026#34;retail-forecast\u0026#34; \\ --max-results 50 \\ --query \u0026#39;TrainingJobSummaries[].TrainingJobName\u0026#39; \\ --output table # Stop running training jobs aws sagemaker list-training-jobs \\ --status-equals InProgress \\ --name-contains \u0026#34;retail-forecast\u0026#34; \\ --query \u0026#39;TrainingJobSummaries[].TrainingJobName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read job_name; do echo \u0026#34;Stopping training job: $job_name\u0026#34; aws sagemaker stop-training-job --training-job-name \u0026#34;$job_name\u0026#34; done # XÃ³a model versions trong Model Registry (giá»¯ approved models) aws sagemaker list-model-packages \\ --model-package-group-name \u0026#34;retail-forecast-models\u0026#34; \\ --model-approval-status PendingManualApproval \\ --query \u0026#39;ModelPackageSummaryList[].ModelPackageArn\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read package_arn; do echo \u0026#34;Deleting pending model package: $package_arn\u0026#34; aws sagemaker delete-model-package --model-package-name \u0026#34;$package_arn\u0026#34; done # XÃ³a endpoints tá»« failed deployments aws sagemaker list-endpoints \\ --name-contains \u0026#34;retail-forecast-dev\u0026#34; \\ --query \u0026#39;Endpoints[?EndpointStatus==`Failed`].EndpointName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read endpoint; do echo \u0026#34;Deleting failed endpoint: $endpoint\u0026#34; aws sagemaker delete-endpoint --endpoint-name \u0026#34;$endpoint\u0026#34; done 8.8 Verification # Verify IAM resources Ä‘Ã£ bá»‹ xÃ³a aws iam get-role --role-name GitHubActionsRole 2\u0026gt;/dev/null || echo \u0026#34;GitHubActionsRole deleted\u0026#34; # Verify SNS topics Ä‘Ã£ bá»‹ xÃ³a aws sns list-topics \\ --query \u0026#39;Topics[?contains(TopicArn, `retail-forecast-alerts`)]\u0026#39; || echo \u0026#34;SNS topics cleaned\u0026#34; # Verify CloudWatch resources aws cloudwatch describe-dashboards \\ --dashboard-name-prefix \u0026#34;RetailForecast-CI-CD\u0026#34; || echo \u0026#34;Dashboards cleaned\u0026#34; # Verify no running training jobs aws sagemaker list-training-jobs \\ --status-equals InProgress \\ --name-contains \u0026#34;retail-forecast\u0026#34; \\ --query \u0026#39;TrainingJobSummaries\u0026#39; || echo \u0026#34;No running training jobs\u0026#34; # Check GitHub Actions status gh run list --limit 5 --status completed 9. Báº£ng giÃ¡ CI/CD Pipeline (ap-southeast-1) 9.1. GitHub Actions Pricing Plan Included Minutes Price per minute Storage Free (Public repos) Unlimited $0 500MB Free (Private repos) 2,000 min/month $0.008 500MB Pro 3,000 min/month $0.008 1GB Team 10,000 min/month $0.008 2GB Enterprise 50,000 min/month $0.008 50GB Runner costs:\nUbuntu: Standard rate macOS: 10x Standard rate Windows: 2x Standard rate Self-hosted: Free compute, infrastructure cost only 9.2. AWS IAM vÃ  Security Costs Service Cost Description IAM Roles \u0026amp; Policies Free Unlimited roles and policies STS AssumeRole calls $0.002/1000 calls OIDC authentication AWS Config (compliance) $0.003/configuration item Policy compliance tracking Example calculation:\n100 CI/CD runs/month Ã— 5 STS calls = 500 calls = $0.001/month 9.3. SageMaker Training Costs trong CI/CD Instance Type Cost per Hour Typical Job Duration Cost per Run ml.m5.large $0.134 15 minutes $0.034 ml.m5.xlarge $0.269 10 minutes $0.045 ml.c5.xlarge $0.238 8 minutes $0.032 ml.p3.2xlarge $4.284 5 minutes $0.357 Monthly costs by frequency:\nDaily training: 30 runs Ã— $0.045 = $1.35 Weekly training: 4 runs Ã— $0.045 = $0.18 On-demand training: 2 runs Ã— $0.045 = $0.09 9.4. ECR Storage vÃ  Transfer Costs Component Cost Volume Monthly Cost Storage $0.10/GB/month 5GB images $0.50 Data Transfer IN Free Upload images $0 Data Transfer OUT $0.12/GB Download to EKS Variable Image management costs:\n# Example: 10 images Ã— 500MB each = 5GB storage # Monthly cost: 5GB Ã— $0.10 = $0.50 # Transfer to EKS: 5GB Ã— $0.12 = $0.60 (one-time per deployment) 9.5. CloudWatch Monitoring cho CI/CD Metric Type Quantity Unit Cost Monthly Cost Custom Metrics 20 metrics $0.30/metric $6.00 API Calls 100K calls $0.01/1K calls $1.00 Alarms 10 alarms $0.10/alarm $1.00 Dashboard 1 dashboard $3.00/dashboard $3.00 Total Monitoring $11.00 9.6. SNS Notification Costs Notification Type Volume Cost per Message Monthly Cost Email 200 notifications $0.75/million $0.0002 SMS 50 notifications $0.8/message $40.00 Slack Webhook 200 notifications $0.75/million $0.0002 Push Mobile 100 notifications $0.75/million $0.0001 9.7. Jenkins Infrastructure Costs (náº¿u self-hosted) Component Instance Type Monthly Hours Monthly Cost Jenkins Master t3.medium 730 hours $30.37 Build Agents t3.large (2 agents) 100 hours $13.25 EBS Storage 100GB gp3 - $8.00 Data Transfer 50GB/month $0.12/GB $6.00 Total Jenkins $57.62 9.8. CI/CD Pipeline Scenarios Scenario 1: Small Team (GitHub Actions)\nComponent Usage Monthly Cost GitHub Actions (private) 2,000 min included $0 SageMaker training 4 runs/month $0.18 ECR storage 2GB images $0.20 CloudWatch basic 5 metrics, 3 alarms $1.80 SNS notifications Email only $0.0002 Total Small Team $2.18/month Scenario 2: Medium Team (GitHub Actions Pro)\nComponent Usage Monthly Cost GitHub Actions Pro 3,000 min + 500 extra $4.00 SageMaker training 12 runs/month $0.54 ECR storage 8GB images $0.80 CloudWatch full 15 metrics, 8 alarms $5.30 SNS notifications Email + Slack $0.0004 Total Medium Team $10.64/month Scenario 3: Enterprise (Self-hosted Jenkins)\nComponent Usage Monthly Cost Jenkins infrastructure t3.medium + agents $57.62 SageMaker training 60 runs/month $2.70 ECR storage 20GB images $2.00 CloudWatch enterprise 50 metrics, 25 alarms $17.50 SNS notifications Multi-channel $40.50 Total Enterprise $120.32/month 9.9. Cost Optimization Strategies GitHub Actions Optimization:\n# Use matrix strategy Ä‘á»ƒ giáº£m runtime strategy: matrix: python-version: [3.8, 3.9, 3.10] # Cache dependencies - uses: actions/cache@v3 with: path: ~/.cache/pip key: ${{ runner.os }}-pip-${{ hashFiles(\u0026#39;**/requirements.txt\u0026#39;) }} # Conditional jobs if: github.ref == \u0026#39;refs/heads/main\u0026#39; SageMaker Training Optimization:\n# Sá»­ dá»¥ng Spot instances cho training training_params = { \u0026#39;TrainingJobName\u0026#39;: job_name, \u0026#39;ResourceConfig\u0026#39;: { \u0026#39;InstanceType\u0026#39;: \u0026#39;ml.m5.large\u0026#39;, \u0026#39;InstanceCount\u0026#39;: 1, \u0026#39;VolumeSizeInGB\u0026#39;: 30, \u0026#39;UseSpotInstances\u0026#39;: True, # 90% cost savings \u0026#39;MaxRuntimeInSeconds\u0026#39;: 3600 } } ECR Cost Optimization:\n# Lifecycle policy Ä‘á»ƒ tá»± Ä‘á»™ng xÃ³a old images aws ecr put-lifecycle-policy \\ --repository-name mlops/retail-api \\ --lifecycle-policy-text \u0026#39;{ \u0026#34;rules\u0026#34;: [ { \u0026#34;rulePriority\u0026#34;: 1, \u0026#34;selection\u0026#34;: { \u0026#34;tagStatus\u0026#34;: \u0026#34;untagged\u0026#34;, \u0026#34;countType\u0026#34;: \u0026#34;sinceImagePushed\u0026#34;, \u0026#34;countUnit\u0026#34;: \u0026#34;days\u0026#34;, \u0026#34;countNumber\u0026#34;: 7 }, \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;expire\u0026#34; } } ] }\u0026#39; 9.10. ROI Analysis cho CI/CD Investment Benefit Manual Process Automated CI/CD Time Saved Cost Benefit Code Testing 2 hours/week 5 minutes 1.9 hours $95/week Model Training 30 min setup Automatic 2 hours/month $100/month Deployment 1 hour/deploy 5 minutes 55 min/deploy $45/deploy Rollback 2 hours 5 minutes 1.9 hours $95/incident Annual ROI calculation:\nInvestment: $127.68/month Ã— 12 = $1,532 Savings: (2 hours/week Ã— 52 weeks + 2 hours/month Ã— 12) Ã— $50/hour = $6,400 ROI: 322% return on investment 9.11. Monitoring CI/CD Costs # Track GitHub Actions usage gh api /repos/:owner/:repo/actions/billing/usage # Monitor SageMaker training costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon SageMaker\u0026#34;]}}\u0026#39; # ECR storage costs aws ecr describe-registry-statistics --region ap-southeast-1 # CloudWatch costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon CloudWatch\u0026#34;]}}\u0026#39; ğŸ’° Cost Summary cho Task 11:\nSmall Team (GitHub Free): $2.18/month Medium Team (GitHub Pro): $10.64/month Enterprise (Self-hosted): $120.32/month ROI: 322% vá»›i automation benefits Break-even point: ~3 months cho medium team setup Next Step: Task 12: Cost Optimization \u0026amp; Teardown\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/3-blogstranslated/3.12-blog12/",
	"title": "Simplify IAM policy creation with IAM Policy Autopilot, a new open source MCP server for builders",
	"tags": [],
	"description": "",
	"content": "Author: Micah Walter â€” 30 NOV 2025\nToday, weâ€™re announcing IAM Policy Autopilot, a new open source Model Context Protocol (MCP) server that analyzes your application code and helps your AI coding assistants generate AWS Identity and Access Management (IAM) identity-based policies. IAM Policy Autopilot accelerates initial development by providing builders with a starting point that they can review and further refine. It integrates with AI coding assistants such as Kiro, Claude Code, Cursor, and Cline, and it provides them with AWS Identity and Access Management (IAM) knowledge and understanding of the latest AWS services and features. IAM Policy Autopilot is available at no additional cost, runs locally, and you can get started by visiting our GitHub repository.\nAmazon Web Services (AWS) applications require IAM policies for their roles. Builders on AWS, from developers to business leaders, engage with IAM as part of their workflow. Developers typically start with broader permissions and refine them over time, balancing rapid development with security. They often use AI coding assistants in hopes of accelerating development and authoring IAM permissions. However, these AI tools donâ€™t fully understand the nuances of IAM and can miss permissions or suggest invalid actions. Builders seek solutions that provide reliable IAM knowledge, integrate with AI assistants and get them started with policy creation, so that they can focus on building applications.\nCreate valid policies with AWS knowledge IAM Policy Autopilot addresses these challenges by generating identity-based IAM policies directly from your application code. Using deterministic code analysis, it creates reliable and valid policies, so you spend less time authoring and debugging permissions. IAM Policy Autopilot incorporates AWS knowledge, including published AWS service reference implementation, to stay up to date. It uses this information to understand how code and SDK calls map to IAM actions and stays current with the latest AWS services and operations.\nThe generated policies provide a starting point for you to review and scope down to implement least privilege permissions. As you modify your application codeâ€”whether adding new AWS service integrations or updating existing onesâ€”you only need to run IAM Policy Autopilot again to get updated permissions.\nGetting started with IAM Policy Autopilot Developers can get started with IAM Policy Autopilot in minutes by downloading and integrating it with their workflow.\nAs an MCP server, IAM Policy Autopilot operates in the background as builders converse with their AI coding assistants. When your application needs IAM policies, your coding assistants can call IAM Policy Autopilot to analyze AWS SDK calls within your application and generate required identity-based IAM policies, providing you with necessary permissions to start with. After permissions are created, if you still encounter Access Denied errors during testing, the AI coding assistant invokes IAM Policy Autopilot to analyze the denial and propose targeted IAM policy fixes. After you review and approve the suggested changes, IAM Policy Autopilot updates the permissions.\nYou can also use IAM Policy Autopilot as a standalone command line interface (CLI) tool to generate policies directly or fix missing permissions. Both the CLI tool and the MCP server provide the same policy creation and troubleshooting capabilities, so you can choose the integration that best fits your workflow.\nWhen using IAM Policy Autopilot, you should also understand the best practices to maximize its benefits. IAM Policy Autopilot generates identity-based policies and doesnâ€™t create resource-based policies, permission boundaries, service control policies (SCPs) or resource control policies (RCPs). IAM Policy Autopilot generates policies that prioritize functionality over minimal permissions. You should always review the generated policies and refine if necessary so they align with your security requirements before deploying them.\nLetâ€™s try it out To set up IAM Policy Autopilot, I first need to install it on my system. To do so, I just need to run a one-liner script:\ncurl -sSL https://github.com/awslabs/iam-policy-autopilot/raw/refs/heads/main/install.sh | sudo sh Then I can follow the instructions to install any MCP server for my IDE of choice. Today, Iâ€™m using Kiro! In a new chat session in Kiro, I start with a straightforward prompt, where I ask Kiro to read the files in my file-to-queue folder and create a new AWS CloudFormation file so I can deploy the application. This folder contains an automated Amazon Simple Storage Service (Amazon S3) file router that scans a bucket and sends notifications to Amazon Simple Queue Service (Amazon SQS) queues or Amazon EventBridge based on configurable prefix-matching rules, enabling event-driven workflows triggered by file locations.\nThe last part asks Kiro to make sure Iâ€™m including necessary IAM policies. This should be enough to get Kiro to use the IAM Policy Autopilot MCP server. Next, Kiro uses the IAM Policy Autopilot MCP server to generate a new policy document, as depicted in the following image. After itâ€™s done, Kiro will move on to building out our CloudFormation template and some additional documentation and relevant code files. Finally, we can see our generated CloudFormation template with a new policy document, all generated using the IAM Policy Autopilot MCP server! Enhanced development workflow IAM Policy Autopilot integrates with AWS services across multiple areas. For core AWS services, IAM Policy Autopilot analyzes your applicationâ€™s usage of services such as Amazon S3, AWS Lambda, Amazon DynamoDB, Amazon Elastic Compute Cloud (Amazon EC2), and Amazon CloudWatch Logs, then generates necessary permissions your code needs based on the SDK calls it discovers. After the policies are created, you can copy the policy directly into your CloudFormation template, AWS Cloud Development Kit (AWS CDK) stack, or Terraform configuration. You can also prompt your AI coding assistants to integrate it for you.\nIAM Policy Autopilot also complements existing IAM tools such as AWS IAM Access Analyzer by providing functional policies as a starting point, which you can then validate using IAM Access Analyzer policy validation or refine over time with unused access analysis.\nNow available IAM Policy Autopilot is available as an open source tool on GitHub at no additional cost. The tool currently supports Python, TypeScript, and Go applications.\nThese capabilities represent a significant step forward in simplifying the AWS development experience so builders of different experience levels can develop and deploy applications more efficiently.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.11-week11/",
	"title": "Week 11 - Observability &amp; FinOps",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Implement comprehensive monitoring (CloudWatch, X-Ray). Optimize cloud costs (Budgets, Tagging, Rightsizing). Tasks to be carried out this week: Day Task Reference 2 - Monitoring: Setup CloudWatch Dashboards and Alarms (CPU, 5xx Errors).\n- Enable X-Ray tracing. AWS CloudWatch 3 - Logs: Centralize logs (CloudWatch Logs) with retention policies.\n- Use Logs Insights for querying. AWS X-Ray 4 - FinOps: Implement Cost Allocation Tags.\n- Setup AWS Budgets and Cost Anomaly Detection. AWS Budgets 5 - Optimization: Review Cost Explorer.\n- Implement rightsizing and cleanup unused resources. AWS Cost Explorer 6 - Review: Validate observability coverage and cost saving measures.\n- Weekly Report. - Week 11 Achievements: Established a unified observability dashboard for system health. Implemented cost control mechanisms using Budgets and Tagging. Optimized resource utilization based on monitoring data. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/5-workshop/12-cost--teardown/",
	"title": "Cost Management &amp; Teardown",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Task 12 Objective: Manage and optimize the operating costs of the entire MLOps infrastructure on AWS:\nMinimize compute costs (EC2, SageMaker, ALB) Automatically scale down or remove unused resources Apply lifecycle policies for data and container images Ensure pipelines run efficiently while minimizing expenses 1. Operational Costs of the MLOps System When deploying a complete MLOps system such as the Retail Prediction API, operating costs can quickly escalate if not properly managed. Major components contributing to costs include:\n1.1. Cost Breakdown by Component Service Non-optimized cost Cause Solution EKS NodeGroup 0.04 USD/hr/node On-demand instances running 24/7 Spot instances + Auto scaling + Scheduling SageMaker Training 0.3 USD/job Large instance types, long runtime Spot training + optimized hyperparameter tuning S3 Storage 0.023 USD/GB/month Storing all data in Standard tier Lifecycle policies + Intelligent-Tiering CloudWatch Logs 0.50 USD/GB Unlimited log retention Log retention policy + optimized Insights queries ALB 0.027 USD/hr Running continuously even with no traffic Schedule shutdown when idle ECR Storage 0.10 USD/GB/month Storing all image versions Lifecycle policy to remove old images 2. Using EC2 Spot Instances EC2 Spot Instances are one of the most effective ways to save compute costs on AWS, offering reductions of up to 70â€“90% compared to On-demand instances.\n2.1. Use Spot for EKS NodeGroup Update the Terraform configuration for the EKS NodeGroup:\nmodule \u0026#34;eks_managed_node_group\u0026#34; { source = \u0026#34;terraform-aws-modules/eks/aws//modules/eks-managed-node-group\u0026#34; name = \u0026#34;retail-forecast-nodes\u0026#34; cluster_name = module.eks.cluster_name cluster_version = module.eks.cluster_version # Configure Spot Instance capacity_type = \u0026#34;SPOT\u0026#34; # instead of ON_DEMAND # Diversify instance types Ä‘á»ƒ tÄƒng kháº£ nÄƒng cÃ³ Spot instance_types = [\u0026#34;t3.medium\u0026#34;, \u0026#34;t3a.medium\u0026#34;, \u0026#34;t2.medium\u0026#34;] min_size = 2 max_size = 5 desired_size = 2 # ThÃªm labels vÃ  taints cho Kubernetes scheduler labels = { app = \u0026#34;retail-api\u0026#34; } } 2.2. Sá»­ dá»¥ng Spot cho SageMaker Training 2.2. Use Spot for SageMaker Training Update the script that creates SageMaker training jobs to use Spot instances:\n# aws/script/create_training_job.py import boto3 import argparse import time def create_training_job(job_name, data_bucket, output_bucket, instance_type, use_spot=True): sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) # Calculate stop time for Spot (limit 1 hour) current_time = int(time.time()) stop_time = current_time + 3600 # 1 hour # Cáº¥u hÃ¬nh training job training_params = { \u0026#39;TrainingJobName\u0026#39;: job_name, \u0026#39;AlgorithmSpecification\u0026#39;: { \u0026#39;TrainingImage\u0026#39;: \u0026#39;123456789012.dkr.ecr.us-east-1.amazonaws.com/retail-forecast-training:latest\u0026#39;, \u0026#39;TrainingInputMode\u0026#39;: \u0026#39;File\u0026#39; }, \u0026#39;RoleArn\u0026#39;: \u0026#39;arn:aws:iam::123456789012:role/SageMakerExecutionRole\u0026#39;, \u0026#39;InputDataConfig\u0026#39;: [ { \u0026#39;ChannelName\u0026#39;: \u0026#39;train\u0026#39;, \u0026#39;DataSource\u0026#39;: { \u0026#39;S3DataSource\u0026#39;: { \u0026#39;S3DataType\u0026#39;: \u0026#39;S3Prefix\u0026#39;, \u0026#39;S3Uri\u0026#39;: f\u0026#39;s3://{data_bucket}/train\u0026#39;, \u0026#39;S3DataDistributionType\u0026#39;: \u0026#39;FullyReplicated\u0026#39; } } } ], \u0026#39;OutputDataConfig\u0026#39;: { \u0026#39;S3OutputPath\u0026#39;: f\u0026#39;s3://{output_bucket}/output\u0026#39; }, \u0026#39;ResourceConfig\u0026#39;: { \u0026#39;InstanceType\u0026#39;: instance_type, \u0026#39;InstanceCount\u0026#39;: 1, \u0026#39;VolumeSizeInGB\u0026#39;: 10 }, \u0026#39;StoppingCondition\u0026#39;: { \u0026#39;MaxRuntimeInSeconds\u0026#39;: 3600 }, \u0026#39;Tags\u0026#39;: [ { \u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailMLOps\u0026#39; } ] } # Configure Spot training if requested if use_spot: training_params[\u0026#39;EnableManagedSpotTraining\u0026#39;] = True training_params[\u0026#39;StoppingCondition\u0026#39;][\u0026#39;MaxWaitTimeInSeconds\u0026#39;] = 3900 # Add maximum wait time response = sagemaker.create_training_job(**training_params) return response if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--job-name\u0026#39;, type=str, required=True) parser.add_argument(\u0026#39;--data-bucket\u0026#39;, type=str, required=True) parser.add_argument(\u0026#39;--output-bucket\u0026#39;, type=str, required=True) parser.add_argument(\u0026#39;--instance-type\u0026#39;, type=str, default=\u0026#39;ml.m5.large\u0026#39;) parser.add_argument(\u0026#39;--use-spot\u0026#39;, type=bool, default=True) args = parser.parse_args() response = create_training_job( args.job_name, args.data_bucket, args.output_bucket, args.instance_type, args.use_spot ) print(f\u0026#34;Training job created: {response[\u0026#39;TrainingJobArn\u0026#39;]}\u0026#34;) 2.3. Handling Spot Instance Interruptions To ensure the system remains available when Spot Instances are reclaimed, configure the following:\nPod Disruption Budget (PDB) to ensure a minimum number of pods: # aws/k8s/pdb/retail-forecast-pdb.yaml apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: retail-forecast-pdb namespace: retail-forecast spec: minAvailable: 1 # LuÃ´n Ä‘áº£m báº£o Ã­t nháº¥t 1 pod Ä‘ang cháº¡y selector: matchLabels: app: retail-forecast-api Cluster Autoscaler with Spot instance handling: # aws/k8s/addons/cluster-autoscaler.yaml apiVersion: apps/v1 kind: Deployment metadata: name: cluster-autoscaler namespace: kube-system labels: app: cluster-autoscaler spec: replicas: 1 selector: matchLabels: app: cluster-autoscaler template: metadata: labels: app: cluster-autoscaler spec: containers: - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.26.2 name: cluster-autoscaler command: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=aws - --skip-nodes-with-local-storage=false - --expander=least-waste - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/retail-forecast-cluster - --balance-similar-node-groups - --skip-nodes-with-system-pods=false volumeMounts: - name: ssl-certs mountPath: /etc/ssl/certs/ca-certificates.crt readOnly: true volumes: - name: ssl-certs hostPath: path: \u0026#34;/etc/ssl/certs/ca-bundle.crt\u0026#34; 3. S3 Lifecycle \u0026amp; Intelligent-Tiering 3. S3 Lifecycle \u0026amp; Intelligent-Tiering 3.1. Configure Lifecycle Policy for S3 Bucket Deploy a lifecycle policy via Terraform:\n# infra/modules/s3/main.tf resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;retail_forecast_data\u0026#34; { bucket = \u0026#34;retail-forecast-data-${var.environment}\u0026#34; tags = { Project = \u0026#34;RetailMLOps\u0026#34; } } # Lifecycle configuration resource \u0026#34;aws_s3_bucket_lifecycle_configuration\u0026#34; \u0026#34;data_lifecycle\u0026#34; { bucket = aws_s3_bucket.retail_forecast_data.id rule { id = \u0026#34;raw-data-tier\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { prefix = \u0026#34;raw/\u0026#34; } transition { days = 30 storage_class = \u0026#34;INTELLIGENT_TIERING\u0026#34; } } rule { id = \u0026#34;silver-data-tier\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { prefix = \u0026#34;silver/\u0026#34; } transition { days = 30 storage_class = \u0026#34;INTELLIGENT_TIERING\u0026#34; } } rule { id = \u0026#34;artifacts-archive\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { prefix = \u0026#34;artifacts/\u0026#34; } transition { days = 90 storage_class = \u0026#34;GLACIER_IR\u0026#34; } transition { days = 180 storage_class = \u0026#34;DEEP_ARCHIVE\u0026#34; } } rule { id = \u0026#34;logs-archive\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { prefix = \u0026#34;logs/\u0026#34; } transition { days = 90 storage_class = \u0026#34;GLACIER_IR\u0026#34; } transition { days = 180 storage_class = \u0026#34;DEEP_ARCHIVE\u0026#34; } } rule { id = \u0026#34;temp-cleanup\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { and { prefix = \u0026#34;tmp/\u0026#34; tag { key = \u0026#34;temporary\u0026#34; value = \u0026#34;true\u0026#34; } } } expiration { days = 7 } } } 3.2. Configure Intelligent-Tiering # infra/modules/s3/intelligent_tiering.tf resource \u0026#34;aws_s3_bucket_intelligent_tiering_configuration\u0026#34; \u0026#34;retail_data_tiering\u0026#34; { bucket = aws_s3_bucket.retail_forecast_data.id name = \u0026#34;RetailDataTiering\u0026#34; tiering { access_tier = \u0026#34;ARCHIVE_ACCESS\u0026#34; days = 90 } tiering { access_tier = \u0026#34;DEEP_ARCHIVE_ACCESS\u0026#34; days = 180 } filter { prefix = \u0026#34;data/\u0026#34; } } 4. Tá»± Ä‘á»™ng dá»«ng tÃ i nguyÃªn ngoÃ i giá» 4. Automatically Stop Resources Outside Business Hours 4.1. Schedule Lambda with EventBridge Create a Lambda function to stop/start resources:\n# aws/scripts/resource_scheduler.py import boto3 import os def lambda_handler(event, context): action = event.get(\u0026#39;action\u0026#39;, \u0026#39;stop\u0026#39;) # \u0026#39;stop\u0026#39; or \u0026#39;start\u0026#39; # EKS NodeGroup scaling eks = boto3.client(\u0026#39;eks\u0026#39;) autoscaling = boto3.client(\u0026#39;autoscaling\u0026#39;) cluster_name = os.environ.get(\u0026#39;EKS_CLUSTER_NAME\u0026#39;, \u0026#39;retail-forecast-cluster\u0026#39;) nodegroup_name = os.environ.get(\u0026#39;NODEGROUP_NAME\u0026#39;, \u0026#39;retail-forecast-nodes\u0026#39;) if action == \u0026#39;stop\u0026#39;: # Scale down to 0 print(f\u0026#34;Scaling down nodegroup {nodegroup_name} in cluster {cluster_name}\u0026#34;) # Láº¥y auto scaling group name tá»« nodegroup response = eks.describe_nodegroup( clusterName=cluster_name, nodegroupName=nodegroup_name ) asg_name = response[\u0026#39;nodegroup\u0026#39;][\u0026#39;resources\u0026#39;][\u0026#39;autoScalingGroups\u0026#39;][0][\u0026#39;name\u0026#39;] # Scale down ASG vá» 0 autoscaling.update_auto_scaling_group( AutoScalingGroupName=asg_name, MinSize=0, DesiredCapacity=0 ) print(f\u0026#34;NodeGroup {nodegroup_name} scaled down to 0\u0026#34;) elif action == \u0026#39;start\u0026#39;: # Scale up to desired capacity print(f\u0026#34;Scaling up nodegroup {nodegroup_name} in cluster {cluster_name}\u0026#34;) response = eks.describe_nodegroup( clusterName=cluster_name, nodegroupName=nodegroup_name ) asg_name = response[\u0026#39;nodegroup\u0026#39;][\u0026#39;resources\u0026#39;][\u0026#39;autoScalingGroups\u0026#39;][0][\u0026#39;name\u0026#39;] # Scale up ASG to desired capacity autoscaling.update_auto_scaling_group( AutoScalingGroupName=asg_name, MinSize=2, DesiredCapacity=2 ) print(f\u0026#34;NodeGroup {nodegroup_name} scaled up to 2\u0026#34;) # SageMaker Endpoint if os.environ.get(\u0026#39;SAGEMAKER_ENDPOINT\u0026#39;): sm_client = boto3.client(\u0026#39;sagemaker\u0026#39;) endpoint_name = os.environ.get(\u0026#39;SAGEMAKER_ENDPOINT\u0026#39;) if action == \u0026#39;stop\u0026#39;: # Endpoints cannot be stopped but can be deleted and re-created later print(f\u0026#34;Deleting SageMaker endpoint {endpoint_name}\u0026#34;) try: sm_client.delete_endpoint(EndpointName=endpoint_name) print(f\u0026#34;Endpoint {endpoint_name} deleted\u0026#34;) except Exception as e: print(f\u0026#34;Error deleting endpoint: {e}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#34;Successfully executed {action} action\u0026#34; } Terraform Ä‘á»ƒ táº¡o EventBridge schedule vÃ  Lambda:\n# infra/modules/scheduler/main.tf resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;resource_scheduler\u0026#34; { function_name = \u0026#34;retail-forecast-resource-scheduler\u0026#34; handler = \u0026#34;resource_scheduler.lambda_handler\u0026#34; runtime = \u0026#34;python3.9\u0026#34; role = aws_iam_role.lambda_role.arn filename = \u0026#34;resource_scheduler.zip\u0026#34; timeout = 300 environment { variables = { EKS_CLUSTER_NAME = var.eks_cluster_name NODEGROUP_NAME = var.nodegroup_name SAGEMAKER_ENDPOINT = var.sagemaker_endpoint } } } # Create EventBridge rule to stop resources at 19:00 UTC resource \u0026#34;aws_cloudwatch_event_rule\u0026#34; \u0026#34;stop_resources\u0026#34; { name = \u0026#34;retail-forecast-stop-resources\u0026#34; description = \u0026#34;Stop resources at 19:00 UTC\u0026#34; schedule_expression = \u0026#34;cron(0 19 * * ? *)\u0026#34; } # Gáº¯n Lambda vá»›i rule stop resource \u0026#34;aws_cloudwatch_event_target\u0026#34; \u0026#34;stop_resources_target\u0026#34; { rule = aws_cloudwatch_event_rule.stop_resources.name target_id = \u0026#34;RetailForecastStopResources\u0026#34; arn = aws_lambda_function.resource_scheduler.arn input = jsonencode({ action = \u0026#34;stop\u0026#34; }) } # Táº¡o EventBridge rule Ä‘á»ƒ khá»Ÿi Ä‘á»™ng tÃ i nguyÃªn lÃºc 7:00 UTC resource \u0026#34;aws_cloudwatch_event_rule\u0026#34; \u0026#34;start_resources\u0026#34; { name = \u0026#34;retail-forecast-start-resources\u0026#34; description = \u0026#34;Start resources at 7:00 UTC\u0026#34; schedule_expression = \u0026#34;cron(0 7 * * ? *)\u0026#34; } # Attach Lambda to the start rule resource \u0026#34;aws_cloudwatch_event_target\u0026#34; \u0026#34;start_resources_target\u0026#34; { rule = aws_cloudwatch_event_rule.start_resources.name target_id = \u0026#34;RetailForecastStartResources\u0026#34; arn = aws_lambda_function.resource_scheduler.arn input = jsonencode({ action = \u0026#34;start\u0026#34; }) } # Grant permission for EventBridge to invoke Lambda resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_eventbridge_stop\u0026#34; { statement_id = \u0026#34;AllowExecutionFromEventBridgeStop\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = aws_lambda_function.resource_scheduler.function_name principal = \u0026#34;events.amazonaws.com\u0026#34; source_arn = aws_cloudwatch_event_rule.stop_resources.arn } resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_eventbridge_start\u0026#34; { statement_id = \u0026#34;AllowExecutionFromEventBridgeStart\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = aws_lambda_function.resource_scheduler.function_name principal = \u0026#34;events.amazonaws.com\u0026#34; source_arn = aws_cloudwatch_event_rule.start_resources.arn } 4.2. Terraform Destroy cho CI/CD Pipeline ThÃªm job vÃ o GitHub Actions workflow Ä‘á»ƒ xÃ³a toÃ n bá»™ tÃ i nguyÃªn sau khi hoÃ n thÃ nh:\n# .github/workflows/mlops-pipeline.yml jobs: # CÃ¡c job hiá»‡n táº¡i... terraform_destroy: name: Destroy Infrastructure runs-on: ubuntu-latest needs: [deploy_eks, monitoring] if: github.event.inputs.destroy_after_demo == \u0026#39;true\u0026#39; environment: name: ${{ needs.setup.outputs.environment }} steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup Terraform uses: hashicorp/setup-terraform@v2 - name: Terraform Init run: | cd aws/infra terraform init - name: Terraform Destroy run: | cd aws/infra terraform destroy -auto-approve 5. Cost Visibility \u0026amp; Alerts 5.1. Tagging Strategy for AWS Resources # Add into all resource modules locals { common_tags = { Project = \u0026#34;RetailMLOps\u0026#34; } } # Apply these tags to all resources 5.2. AWS Budgets Alert # infra/modules/budget/main.tf resource \u0026#34;aws_budgets_budget\u0026#34; \u0026#34;cost\u0026#34; { name = \u0026#34;retail-forecast-${var.environment}-monthly-budget\u0026#34; budget_type = \u0026#34;COST\u0026#34; limit_amount = var.monthly_limit limit_unit = \u0026#34;USD\u0026#34; time_unit = \u0026#34;MONTHLY\u0026#34; time_period_start = \u0026#34;2023-01-01_00:00\u0026#34; cost_filter { name = \u0026#34;TagKeyValue\u0026#34; values = [ \u0026#34;user:Project$RetailForecastMLOps\u0026#34; ] } notification { comparison_operator = \u0026#34;GREATER_THAN\u0026#34; threshold = 80 threshold_type = \u0026#34;PERCENTAGE\u0026#34; notification_type = \u0026#34;ACTUAL\u0026#34; subscriber_email_addresses = var.notification_emails } notification { comparison_operator = \u0026#34;GREATER_THAN\u0026#34; threshold = 100 threshold_type = \u0026#34;PERCENTAGE\u0026#34; notification_type = \u0026#34;ACTUAL\u0026#34; subscriber_email_addresses = var.notification_emails subscriber_sns_topic_arns = [var.sns_topic_arn] } } Script to create an AWS Budget via CLI:\n# aws/scripts/create_budget.sh #!/bin/bash ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) EMAIL=\u0026#34;your-email@example.com\u0026#34; BUDGET_NAME=\u0026#34;MLOpsBudget\u0026#34; BUDGET_LIMIT=5 # USD # Create AWS Budget aws budgets create-budget \\ --account-id $ACCOUNT_ID \\ --budget \u0026#39;{ \u0026#34;BudgetName\u0026#34;: \u0026#34;\u0026#39;$BUDGET_NAME\u0026#39;\u0026#34;, \u0026#34;BudgetLimit\u0026#34;: { \u0026#34;Amount\u0026#34;: \u0026#34;\u0026#39;$BUDGET_LIMIT\u0026#39;\u0026#34;, \u0026#34;Unit\u0026#34;: \u0026#34;USD\u0026#34; }, \u0026#34;CostFilters\u0026#34;: { \u0026#34;TagKeyValue\u0026#34;: [ \u0026#34;user:Project$RetailForecastMLOps\u0026#34; ] }, \u0026#34;BudgetType\u0026#34;: \u0026#34;COST\u0026#34;, \u0026#34;TimeUnit\u0026#34;: \u0026#34;MONTHLY\u0026#34; }\u0026#39; \\ --notifications-with-subscribers \u0026#39;[ { \u0026#34;Notification\u0026#34;: { \u0026#34;NotificationType\u0026#34;: \u0026#34;ACTUAL\u0026#34;, \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GREATER_THAN\u0026#34;, \u0026#34;Threshold\u0026#34;: 80, \u0026#34;ThresholdType\u0026#34;: \u0026#34;PERCENTAGE\u0026#34; }, \u0026#34;Subscribers\u0026#34;: [ { \u0026#34;SubscriptionType\u0026#34;: \u0026#34;EMAIL\u0026#34;, \u0026#34;Address\u0026#34;: \u0026#34;\u0026#39;$EMAIL\u0026#39;\u0026#34; } ] }, { \u0026#34;Notification\u0026#34;: { \u0026#34;NotificationType\u0026#34;: \u0026#34;ACTUAL\u0026#34;, \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GREATER_THAN\u0026#34;, \u0026#34;Threshold\u0026#34;: 100, \u0026#34;ThresholdType\u0026#34;: \u0026#34;PERCENTAGE\u0026#34; }, \u0026#34;Subscribers\u0026#34;: [ { \u0026#34;SubscriptionType\u0026#34;: \u0026#34;EMAIL\u0026#34;, \u0026#34;Address\u0026#34;: \u0026#34;\u0026#39;$EMAIL\u0026#39;\u0026#34; } ] } ]\u0026#39; 6. ECR \u0026amp; CloudWatch Optimization 6.1. ECR Lifecycle Policy # infra/modules/ecr/main.tf resource \u0026#34;aws_ecr_repository\u0026#34; \u0026#34;retail_forecast\u0026#34; { name = \u0026#34;retail-forecast\u0026#34; image_tag_mutability = \u0026#34;MUTABLE\u0026#34; image_scanning_configuration { scan_on_push = true } } resource \u0026#34;aws_ecr_lifecycle_policy\u0026#34; \u0026#34;retail_forecast_policy\u0026#34; { repository = aws_ecr_repository.retail_forecast.name policy = jsonencode({ rules = [ { rulePriority = 1, description = \u0026#34;Keep only 3 latest untagged images\u0026#34;, selection = { tagStatus = \u0026#34;untagged\u0026#34;, countType = \u0026#34;imageCountMoreThan\u0026#34;, countNumber = 3 }, action = { type = \u0026#34;expire\u0026#34; } }, { rulePriority = 2, description = \u0026#34;Keep only 3 latest images per tag prefix\u0026#34;, selection = { tagStatus = \u0026#34;tagged\u0026#34;, tagPrefixList = [\u0026#34;prod\u0026#34;, \u0026#34;stage\u0026#34;, \u0026#34;dev\u0026#34;], countType = \u0026#34;imageCountMoreThan\u0026#34;, countNumber = 3 }, action = { type = \u0026#34;expire\u0026#34; } }, { rulePriority = 3, description = \u0026#34;Keep only the 10 most recent images\u0026#34;, selection = { tagStatus = \u0026#34;any\u0026#34;, countType = \u0026#34;imageCountMoreThan\u0026#34;, countNumber = 10 }, action = { type = \u0026#34;expire\u0026#34; } } ] }) } Or use the AWS CLI:\naws ecr put-lifecycle-policy \\ --repository-name retail-forecast \\ --lifecycle-policy-text \u0026#39;{ \u0026#34;rules\u0026#34;: [ { \u0026#34;rulePriority\u0026#34;: 1, \u0026#34;description\u0026#34;: \u0026#34;Keep only 3 latest untagged images\u0026#34;, \u0026#34;selection\u0026#34;: { \u0026#34;tagStatus\u0026#34;: \u0026#34;untagged\u0026#34;, \u0026#34;countType\u0026#34;: \u0026#34;imageCountMoreThan\u0026#34;, \u0026#34;countNumber\u0026#34;: 3 }, \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;expire\u0026#34; } }, { \u0026#34;rulePriority\u0026#34;: 2, \u0026#34;description\u0026#34;: \u0026#34;Keep only 3 latest images per tag prefix\u0026#34;, \u0026#34;selection\u0026#34;: { \u0026#34;tagStatus\u0026#34;: \u0026#34;tagged\u0026#34;, \u0026#34;tagPrefixList\u0026#34;: [\u0026#34;prod\u0026#34;, \u0026#34;stage\u0026#34;, \u0026#34;dev\u0026#34;], \u0026#34;countType\u0026#34;: \u0026#34;imageCountMoreThan\u0026#34;, \u0026#34;countNumber\u0026#34;: 3 }, \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;expire\u0026#34; } }, { \u0026#34;rulePriority\u0026#34;: 3, \u0026#34;description\u0026#34;: \u0026#34;Keep only the 10 most recent images\u0026#34;, \u0026#34;selection\u0026#34;: { \u0026#34;tagStatus\u0026#34;: \u0026#34;any\u0026#34;, \u0026#34;countType\u0026#34;: \u0026#34;imageCountMoreThan\u0026#34;, \u0026#34;countNumber\u0026#34;: 10 }, \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;expire\u0026#34; } } ] }\u0026#39;\u0026#39; 6.2. CloudWatch Log Retention Policy # infra/modules/logs/main.tf resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;eks_logs\u0026#34; { name = \u0026#34;/aws/eks/mlops-retail-cluster/cluster\u0026#34; retention_in_days = 7 } resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;app_logs\u0026#34; { name = \u0026#34;/aws/retail/api\u0026#34; retention_in_days = 7 } Or use the AWS CLI:\n# Set retention policy for CloudWatch Logs aws logs put-retention-policy \\ --log-group-name \u0026#34;/aws/eks/retail-forecast-cluster/cluster\u0026#34; \\ --retention-in-days 30 aws logs put-retention-policy \\ --log-group-name \u0026#34;/aws/retail-forecast/api\u0026#34; \\ --retention-in-days 30 aws logs put-retention-policy \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs\u0026#34; \\ --retention-in-days 30 7. Automated Teardown of the Entire Infrastructure 7.1. Terraform Destroy Script # aws/scripts/teardown.sh #!/bin/bash echo \u0026#34;Starting teardown...\u0026#34; # Delete Kubernetes resources kubectl delete namespace mlops --ignore-not-found=true # Delete ECR images aws ecr batch-delete-image --repository-name mlops/retail-api --image-ids imageTag=latest imageTag=v2 imageTag=v3 # Terraform destroy cd ../infra terraform destroy -auto-approve echo \u0026#34;Teardown completed!\u0026#34; 7.2. Add to GitHub Actions Workflow # .github/workflows/teardown.yml name: MLOps Infrastructure Teardown on: workflow_dispatch: inputs: confirmation: description: \u0026#39;Type \u0026#34;destroy\u0026#34; to confirm deletion of all resources\u0026#39; required: true env: AWS_REGION: us-east-1 TF_VAR_environment: dev jobs: teardown: name: Teardown Infrastructure runs-on: ubuntu-latest if: github.event.inputs.confirmation == \u0026#39;destroy\u0026#39; steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup kubectl uses: azure/setup-kubectl@v3 - name: Update kubeconfig run: | aws eks update-kubeconfig --name retail-forecast-cluster --region ${{ env.AWS_REGION }} - name: Delete Kubernetes resources run: | kubectl delete namespace retail-forecast --ignore-not-found=true - name: Delete SageMaker endpoints run: | ENDPOINTS=$(aws sagemaker list-endpoints --name-contains retail-forecast --query \u0026#34;Endpoints[].EndpointName\u0026#34; --output text) if [ ! -z \u0026#34;$ENDPOINTS\u0026#34; ]; then for ENDPOINT in $ENDPOINTS; do echo \u0026#34;Deleting endpoint: $ENDPOINT\u0026#34; aws sagemaker delete-endpoint --endpoint-name $ENDPOINT done fi - name: Setup Terraform uses: hashicorp/setup-terraform@v2 - name: Terraform Init run: | cd aws/infra terraform init - name: Terraform Destroy run: | cd aws/infra terraform destroy -auto-approve - name: Send notification if: always() run: | if [ \u0026#34;${{ job.status }}\u0026#34; == \u0026#34;success\u0026#34; ]; then MESSAGE=\u0026#34;âœ… Infrastructure teardown completed successfully\u0026#34; else MESSAGE=\u0026#34;âŒ Infrastructure teardown failed\u0026#34; fi aws sns publish \\ --topic-arn ${{ secrets.SNS_TOPIC_ARN }} \\ --subject \u0026#34;MLOps Infrastructure Teardown\u0026#34; \\ --message \u0026#34;$MESSAGE\u0026#34; 8. Estimated Costs After Optimization The following table shows projected costs after applying optimization measures:\nComponent Before Optimizing After Optimizing Savings (%) EKS NodeGroup 28.80 USD 2.40 USD 92% S3 Storage 1.15 USD 0.63 USD 45% CloudWatch Logs 2.50 USD 0.75 USD 70% LoadBalancer 19.44 USD 5.40 USD 72% ECR Storage 0.50 USD 0.20 USD 60% Total Cost 52.39 USD 9.38 USD 82% 9. Expected Results âœ… Completed Checklist EC2 Spot Instances: Configure EKS NodeGroup and SageMaker to use Spot S3 Lifecycle: Deploy lifecycle policies for data and artifacts Auto Schedule: Lambda + EventBridge to automatically stop/start resources Budget Alerts: Set up cost monitoring and alerts ECR Lifecycle: Keep only the 3 most recent image versions Log Retention: CloudWatch logs retention policy of 30 days Complete Teardown: Script to fully remove resources after demos ğŸ“Š Verification Steps # 1. Check EKS NodeGroup capacity type is Spot aws eks describe-nodegroup \\ --cluster-name retail-forecast-cluster \\ --nodegroup-name retail-forecast-nodes \\ --query \u0026#39;nodegroup.capacityType\u0026#39; # 2. Check S3 lifecycle policy aws s3api get-bucket-lifecycle-configuration \\ --bucket retail-forecast-data-dev # 3. Check CloudWatch logs retention aws logs describe-log-groups \\ --log-group-name-prefix /aws/retail-forecast \\ --query \u0026#39;logGroups[*].[logGroupName,retentionInDays]\u0026#39; # 4. Check ECR lifecycle policy aws ecr get-lifecycle-policy \\ --repository-name retail-forecast # 5. Check AWS Budget was created aws budgets describe-budgets \\ --account-id $(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) # 6. Run teardown and verify resources are removed cd aws/scripts ./teardown.sh # Verify no remaining resources aws eks list-clusters --query \u0026#39;clusters[*]\u0026#39; | grep retail aws s3 ls | grep retail-forecast aws ecr describe-repositories --query \u0026#39;repositories[*].repositoryName\u0026#39; | grep retail aws sagemaker list-endpoints --query \u0026#39;Endpoints[*].EndpointName\u0026#39; | grep retail Summary Effective cost management is one of the most important aspects of MLOps on AWS. By applying optimization strategies such as Spot Instances, S3 lifecycle policies, auto scheduling, and resource cleanup, we can reduce operating costs by up to 80% while maintaining system scalability and performance.\nThese cost optimization measures not only help save budget but also make the MLOps system more efficient through automated resource management, cost monitoring, and lifecycle best practices for data and container images.\nKey outcomes:\nSave ~82% of operating costs (~9.38 USD/month vs ~52.39 USD/month) Automatic scheduling to start/stop resources S3 lifecycle policies to save storage costs CloudWatch logs retention set to 7 days Complete teardown script "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/1-worklog/1.12-week12/",
	"title": "Week 12 - Capstone: Personal Project",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Consolidate all learned skills into a final \u0026ldquo;Retail Prediction MLOps\u0026rdquo; project. Finalize documentation and teardown resources. Tasks to be carried out this week: Day Task Reference 2 - Integration: Deploy full stack (VPC, EKS/ECS, RDS/DynamoDB).\n- Connect Frontend (CDN) and Backend (API). Project Docs 3 - Testing: Perform Load Testing and Failure Injection (Chaos Engineering).\n- Verify Auto Scaling and Rollbacks. Testing "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/categories/advanced/",
	"title": "Advanced",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/categories/aws-public-sector-blog/",
	"title": "AWS Public Sector Blog",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/categories/aws-waf/",
	"title": "AWS WAF",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/categories/identity--compliance/",
	"title": "Identity &amp; Compliance",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/categories/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/categories/artificial-intelligence/",
	"title": "Artificial Intelligence",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/tags/announcements/",
	"title": "Announcements",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/tags/aws-partner-network/",
	"title": "AWS Partner Network",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/categories/aws-partner-network-apn-blog/",
	"title": "AWS Partner Network (APN) Blog",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/tags/aws-reinvent/",
	"title": "AWS Re:Invent",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/tags/customer-solutions/",
	"title": "Customer Solutions",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/tags/foundational-100/",
	"title": "Foundational (100)",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/categories/aws-web3-blog/",
	"title": "AWS Web3 Blog",
	"tags": [],
	"description": "",
	"content": ""
}]