[
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/1-introduction/",
	"title": "Thiáº¿t káº¿ kiáº¿n trÃºc MLOps",
	"tags": [],
	"description": "",
	"content": "AWS MLOps Retail Prediction Platform AWS MLOps Retail Prediction lÃ  má»™t há»‡ thá»‘ng MLOps end-to-end hoÃ n chá»‰nh Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn AWS Cloud, tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ quy trÃ¬nh tá»« xÃ¢y dá»±ng háº¡ táº§ng, huáº¥n luyá»‡n mÃ´ hÃ¬nh, triá»ƒn khai inference API, Ä‘áº¿n giÃ¡m sÃ¡t vÃ  tá»‘i Æ°u chi phÃ­. Dá»± Ã¡n Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh má»Ÿ rá»™ng, Ä‘á»™ tin cáº­y vÃ  báº£o máº­t cao cho cÃ¡c á»©ng dá»¥ng Machine Learning trong thá»±c táº¿.\n1. Kiáº¿n trÃºc MLOps trÃªn AWS Cloud 1.1 Má»¥c tiÃªu dá»± Ã¡n Tá»± Ä‘á»™ng hÃ³a hoÃ n toÃ n quy trÃ¬nh MLOps:\nğŸ—ï¸ Infrastructure as Code: XÃ¢y dá»±ng háº¡ táº§ng tá»± Ä‘á»™ng báº±ng Terraform (VPC, EKS, IAM, EC2, ECR, S3) ğŸ¤– ML Training: Huáº¥n luyá»‡n mÃ´ hÃ¬nh phÃ¢n tÃ¡n trÃªn SageMaker vá»›i model registry ğŸš€ Container Deployment: ÄÃ³ng gÃ³i \u0026amp; triá»ƒn khai inference API trÃªn EKS vá»›i autoscaling ğŸ“Š Monitoring \u0026amp; Security: GiÃ¡m sÃ¡t báº±ng CloudWatch, báº£o máº­t báº±ng KMS \u0026amp; CloudTrail ğŸ”„ CI/CD Pipeline: Pipeline tá»± Ä‘á»™ng tá»« thay Ä‘á»•i code/data â†’ build â†’ train â†’ deploy ğŸ’° Cost Optimization: TÃ­ch há»£p DataOps vÃ  teardown Ä‘á»ƒ tá»‘i Æ°u chi phÃ­ 1.2 Flow tá»•ng quÃ¡t Infrastructure â†’ Training â†’ Deployment â†’ Monitoring â†’ CI/CD â†’ Cost Optimization\n2. BÃ i toÃ¡n dá»± Ä‘oÃ¡n Ä‘á»™ nháº¡y giÃ¡ khÃ¡ch hÃ ng 2.1 MÃ´ táº£ dataset - dunnhumby Source Files Nguá»“n dá»¯ liá»‡u: dunnhumby Source Files\nTáº­p dá»¯ liá»‡u sá»­ dá»¥ng: transaction.csv (â‰ˆ 2.67 triá»‡u dÃ²ng, 22 cá»™t)\nMÃ´ táº£: Má»—i dÃ²ng dá»¯ liá»‡u Ä‘áº¡i diá»‡n cho má»™t sáº£n pháº©m trong má»™t láº§n mua hÃ ng cá»§a khÃ¡ch hÃ ng.\n2.1.1 Cáº¥u trÃºc dá»¯ liá»‡u chi tiáº¿t TÃªn cá»™t Kiá»ƒu dá»¯ liá»‡u MÃ´ táº£ / Ã nghÄ©a VÃ­ dá»¥ giÃ¡ trá»‹ SHOP_WEEK int64 Tuáº§n mua hÃ ng (theo Ä‘á»‹nh dáº¡ng YYYYWW) 200807 SHOP_DATE int64 NgÃ y mua hÃ ng (Ä‘á»‹nh dáº¡ng YYYYMMDD) 20080407 SHOP_WEEKDAY int64 Thá»© trong tuáº§n (1=Chá»§ nháº­t, 2=Thá»© hai, â€¦, 7=Thá»© báº£y) 2 SHOP_HOUR int64 Giá» giao dá»‹ch (0â€“23) 14 QUANTITY int64 Sá»‘ lÆ°á»£ng sáº£n pháº©m mua trong dÃ²ng giao dá»‹ch 1 SPEND float64 Sá»‘ tiá»n chi tiÃªu (Ä‘Æ¡n vá»‹: báº£ng Anh Â£) cho dÃ²ng giao dá»‹ch (sáº£n pháº©m Ã— sá»‘ lÆ°á»£ng) 1.01 PROD_CODE object MÃ£ sáº£n pháº©m chi tiáº¿t (cáº¥p tháº¥p nháº¥t) PRD0900005 PROD_CODE_10 object MÃ£ nhÃ³m sáº£n pháº©m cáº¥p 1 (chuyÃªn má»¥c chÃ­nh) CL00155 PROD_CODE_20 object MÃ£ nhÃ³m sáº£n pháº©m cáº¥p 2 DEP00053 PROD_CODE_30 object MÃ£ nhÃ³m sáº£n pháº©m cáº¥p 3 G00016 PROD_CODE_40 object MÃ£ nhÃ³m sáº£n pháº©m cáº¥p 4 (cÃ³ thá»ƒ mÃ´ táº£ danh má»¥c con) NaN / G00420 CUST_CODE object MÃ£ Ä‘á»‹nh danh khÃ¡ch hÃ ng (áº©n danh) CUST0000123 seg_1 object NhÃ³m phÃ¢n khÃºc khÃ¡ch hÃ ng cáº¥p 1 (phÃ¢n loáº¡i hÃ nh vi tá»•ng quan) BG / AZ / NaN seg_2 object NhÃ³m phÃ¢n khÃºc khÃ¡ch hÃ ng cáº¥p 2 (chi tiáº¿t hÆ¡n seg_1) DI / CZ / BU BASKET_ID int64 MÃ£ Ä‘á»‹nh danh giá» hÃ ng (má»—i láº§n mua cá»§a khÃ¡ch) 994110500233340 BASKET_SIZE object KÃ­ch thÆ°á»›c giá» hÃ ng (Small/Medium/Large) S / M / L BASKET_PRICE_SENSITIVITY object Má»©c Ä‘á»™ nháº¡y cáº£m vá»›i giÃ¡ cá»§a khÃ¡ch hÃ ng trong giao dá»‹ch (Low/Medium/High hoáº·c mÃ£ viáº¿t táº¯t nhÆ° LA, MM, UM) MM / LA / UM BASKET_TYPE object Loáº¡i giá» hÃ ng (Full Shop / Small Shop / Top Up / Fresh / Nonfood\u0026hellip;) Full Shop BASKET_DOMINANT_MISSION object Má»¥c Ä‘Ã­ch chÃ­nh cá»§a giá» hÃ ng (Mixed, Grocery, Fresh, Nonfood, \u0026hellip;), thá»ƒ hiá»‡n loáº¡i sáº£n pháº©m chá»§ yáº¿u Mixed / Fresh STORE_CODE object MÃ£ cá»­a hÃ ng nÆ¡i giao dá»‹ch diá»…n ra STORE00001 STORE_FORMAT object Äá»‹nh dáº¡ng cá»­a hÃ ng (LS = Large Store, SS = Small Store, Express, v.v.) LS STORE_REGION object Khu vá»±c Ä‘á»‹a lÃ½ cá»§a cá»­a hÃ ng (E01â€“E05, tÆ°Æ¡ng á»©ng cÃ¡c vÃ¹ng táº¡i Anh quá»‘c) E02 2.1.2 NhÃ³m features theo nghiá»‡p vá»¥ NhÃ³m Cá»™t Ã nghÄ©a ğŸ›’ Giá» hÃ ng BASKET_SIZE, BASKET_TYPE, BASKET_DOMINANT_MISSION KÃ­ch cá»¡, loáº¡i vÃ  má»¥c Ä‘Ã­ch giá» hÃ ng ğŸ’¸ Chi tiÃªu SPEND, QUANTITY Sá»‘ tiá»n vÃ  sá»‘ lÆ°á»£ng mua ğŸ¬ Cá»­a hÃ ng STORE_REGION, STORE_FORMAT Khu vá»±c vÃ  loáº¡i cá»­a hÃ ng ğŸ“¦ Sáº£n pháº©m PROD_CODE_20, PROD_CODE_30 NhÃ³m sáº£n pháº©m chÃ­nh ğŸ¯ NhÃ£n BASKET_PRICE_SENSITIVITY Äá»™ nháº¡y giÃ¡ â€“ Low / Medium / High NhÃ³m Cá»™t Ã nghÄ©a ğŸ›’ Giá» hÃ ng BASKET_SIZE, BASKET_TYPE, BASKET_DOMINANT_MISSION KÃ­ch cá»¡, loáº¡i vÃ  má»¥c Ä‘Ã­ch giá» hÃ ng ğŸ’¸ Chi tiÃªu SPEND, QUANTITY Sá»‘ tiá»n vÃ  sá»‘ lÆ°á»£ng mua ğŸ¬ Cá»­a hÃ ng STORE_REGION, STORE_FORMAT Khu vá»±c vÃ  loáº¡i cá»­a hÃ ng ğŸ“¦ Sáº£n pháº©m PROD_CODE_20, PROD_CODE_30 NhÃ³m sáº£n pháº©m chÃ­nh ğŸ¯ NhÃ£n BASKET_PRICE_SENSITIVITY Äá»™ nháº¡y giÃ¡ â€“ Low / Medium / High 2.2 Má»¥c tiÃªu bÃ i toÃ¡n XÃ¢y dá»±ng mÃ´ hÃ¬nh machine learning phÃ¢n loáº¡i Ä‘a lá»›p (multi-class) Ä‘á»ƒ dá»± Ä‘oÃ¡n má»©c Ä‘á»™ nháº¡y giÃ¡ (Low / Medium / High) cá»§a khÃ¡ch hÃ ng trong má»—i giao dá»‹ch, dá»±a trÃªn Ä‘áº·c trÆ°ng cá»§a giá» hÃ ng, cá»­a hÃ ng vÃ  hÃ nh vi mua sáº¯m.\nğŸ’¡ á»¨ng dá»¥ng: PhÃ¢n nhÃ³m khÃ¡ch hÃ ng theo Ä‘á»™ nháº¡y giÃ¡ â†’ Ä‘á»‹nh giÃ¡ linh hoáº¡t, cÃ¡ nhÃ¢n hoÃ¡ khuyáº¿n mÃ£i vÃ  tá»‘i Æ°u doanh thu.\n2.3 Loáº¡i bÃ i toÃ¡n vÃ  mÃ´ hÃ¬nh Loáº¡i: PhÃ¢n loáº¡i Ä‘a lá»›p (Supervised Learning)\nMÃ´ hÃ¬nh dá»± kiáº¿n:\nMÃ´ hÃ¬nh LÃ½ do Decision Tree Dá»… giáº£i thÃ­ch feature impact Random Forest Äá»™ chÃ­nh xÃ¡c cao, giáº£m overfitting Logistic Regression (multi-class) Baseline so sÃ¡nh XGBoost Hiá»‡u quáº£ vá»›i tabular data MÃ´ hÃ¬nh LÃ½ do Decision Tree Dá»… giáº£i thÃ­ch feature impact Random Forest Äá»™ chÃ­nh xÃ¡c cao, giáº£m overfitting Logistic Regression (multi-class) Baseline so sÃ¡nh XGBoost Hiá»‡u quáº£ vá»›i tabular data ÄÃ¡nh giÃ¡: Accuracy, Precision, Recall, F1-score, Confusion Matrix\n2.4 Kiáº¿n trÃºc tá»•ng thá»ƒ (AWS MLOps) ThÃ nh pháº§n chÃ­nh:\nAmazon S3: LÆ°u raw/silver/gold dataset, partition theo STORE_REGION, BASKET_TYPE Glue/Athena: ETL vÃ  khÃ¡m phÃ¡ dá»¯ liá»‡u SageMaker Feature Store: Quáº£n lÃ½ feature parity train â†” inference SageMaker Training \u0026amp; Model Registry: Huáº¥n luyá»‡n vÃ  phiÃªn báº£n mÃ´ hÃ¬nh EKS (FastAPI): Triá»ƒn khai API real-time dá»± Ä‘oÃ¡n Ä‘á»™ nháº¡y giÃ¡ CloudWatch: Theo dÃµi Ä‘á»™ trá»…, accuracy thá»±c táº¿, chi phÃ­ 2.5 KPI vÃ  káº¿t quáº£ ká»³ vá»ng NhÃ³m Chá»‰ sá»‘ Má»¥c tiÃªu ML Accuracy â‰¥ 0.75 ML Macro F1 â‰¥ 0.70 ML Precision (per class) â‰¥ 0.65 Ops P95 latency (API) \u0026lt; 200 ms Ops Throughput (requests/s) â‰¥ 100 Business Giáº£m sai sÃ³t Ä‘á»‹nh giÃ¡ â‰¥ 10% Cost Infrastructure cost/month \u0026lt; $500 NhÃ³m Chá»‰ sá»‘ Má»¥c tiÃªu ML Accuracy â‰¥ 0.75 ML Macro F1 â‰¥ 0.70 ML Precision (per class) â‰¥ 0.65 Ops P95 latency (API) \u0026lt; 200 ms Ops Throughput (requests/s) â‰¥ 100 Business Giáº£m sai sÃ³t Ä‘á»‹nh giÃ¡ â‰¥ 10% Cost Infrastructure cost/month \u0026lt; $500 3. Project Structure Dá»± Ã¡n Ä‘Æ°á»£c tá»• chá»©c theo cáº¥u trÃºc modularity vá»›i separation of concerns rÃµ rÃ ng:\nretail-forecast/\râ”œâ”€â”€ README.md # Project overview \u0026amp; setup guide\râ”œâ”€â”€ .gitignore # Git ignore patterns\râ”œâ”€â”€ aws/ # AWS-specific configurations\râ”‚ â”œâ”€â”€ .travis.yml # Travis CI configuration\râ”‚ â”œâ”€â”€ Jenkinsfile # Jenkins pipeline configuration\râ”‚ â”œâ”€â”€ infra/ # Terraform infrastructure\râ”‚ â”‚ â”œâ”€â”€ main.tf # Main infrastructure config\râ”‚ â”‚ â”œâ”€â”€ variables.tf # Input variables\râ”‚ â”‚ â””â”€â”€ output.tf # Output values\râ”‚ â”œâ”€â”€ k8s/ # Kubernetes manifests\râ”‚ â”‚ â”œâ”€â”€ deployment.yaml # Application deployment\râ”‚ â”‚ â”œâ”€â”€ service.yaml # Service configuration\râ”‚ â”‚ â”œâ”€â”€ hpa.yaml # Horizontal Pod Autoscaler\râ”‚ â”‚ â””â”€â”€ namespace.yaml # Namespace definition\râ”‚ â””â”€â”€ script/ # Automation scripts\râ”‚ â”œâ”€â”€ create_training_job.py # SageMaker training job\râ”‚ â”œâ”€â”€ register_model.py # Model registry script\râ”‚ â”œâ”€â”€ deploy_endpoint.py # Model deployment\râ”‚ â””â”€â”€ autoscaling_endpoint.py # Auto-scaling setup\râ”œâ”€â”€ azure/ # Azure-specific configurations\râ”‚ â”œâ”€â”€ azure-pipelines.yml # Azure DevOps pipeline\râ”‚ â”œâ”€â”€ aml/ # Azure ML configurations\râ”‚ â”‚ â”œâ”€â”€ train-job.yml # Training job definition\râ”‚ â”‚ â”œâ”€â”€ train.Dockerfile # Training container\râ”‚ â”‚ â””â”€â”€ infer.Dockerfile # Inference container\râ”‚ â”œâ”€â”€ infra/ # Bicep infrastructure\râ”‚ â”‚ â””â”€â”€ main.bicep # Azure infrastructure\râ”‚ â””â”€â”€ k8s/ # AKS manifests\râ”‚ â”œâ”€â”€ deployment.yaml # Application deployment\râ”‚ â”œâ”€â”€ service.yaml # Service configuration\râ”‚ â””â”€â”€ hpa.yaml # Horizontal Pod Autoscaler\râ”œâ”€â”€ core/ # Shared ML core modules\râ”‚ â””â”€â”€ requirements.txt # Core Python dependencies\râ”œâ”€â”€ server/ # Inference API server\râ”‚ â”œâ”€â”€ DockerFile # Container definition\râ”‚ â”œâ”€â”€ requirements.txt # Server dependencies\râ”‚ â””â”€â”€ Readme.md # Server documentation\râ””â”€â”€ tests/ # Test suites\râ””â”€â”€ (test files) # Unit \u0026amp; integration tests 3.1 Cáº¥u trÃºc thÆ° má»¥c chi tiáº¿t ğŸ“‚ aws/ - AWS Implementation\ninfra/: Terraform Infrastructure as Code k8s/: Kubernetes manifests cho EKS deployment script/: Python scripts cho SageMaker automation CI/CD configurations (Jenkins, Travis) ğŸ“‚ azure/ - Azure Implementation\nğŸ“‚ azure/ - Azure Implementation\ninfra/: Bicep templates cho Azure resources aml/: Azure ML configurations k8s/: AKS manifests Azure DevOps pipeline ğŸ“‚ core/ - Shared Components\nCommon ML utilities vÃ  libraries Shared dependencies vÃ  configurations ğŸ“‚ server/ - Inference API\nFastAPI application Docker containerization API documentation ğŸ“‚ tests/ - Testing Framework\nUnit tests cho ML pipeline Integration tests cho infrastructure End-to-end testing scenarios 4. CÃ´ng nghá»‡ sá»­ dá»¥ng 4.1 Infrastructure \u0026amp; Platform Stack Infrastructure as Code: Terraform cho automated provisioning\nContainer Platform: Amazon EKS (Kubernetes) vá»›i managed node groups\nContainer Registry: Amazon ECR vá»›i vulnerability scanning\nInfrastructure as Code: Terraform cho automated provisioning\nContainer Platform: Amazon EKS (Kubernetes) vá»›i managed node groups\nContainer Registry: Amazon ECR vá»›i vulnerability scanning\nNetworking: VPC multi-AZ, NAT gateways, security groups\nLoad Balancing: Application Load Balancer vá»›i health checks\n4.2 ML \u0026amp; Data Platform Stack ML Training: Amazon SageMaker vá»›i distributed training Data Storage: Amazon S3 data lake vá»›i versioning Model Registry: SageMaker Model Registry cho version control Data Processing: Automated preprocessing vÃ  feature engineering ML Framework: TensorFlow/PyTorch trÃªn SageMaker training jobs 4.3 DevOps \u0026amp; Security Stack CI/CD Platform: Jenkins hoáº·c Travis CI cho automated pipelines Monitoring: CloudWatch (logs, metrics, dashboards, alarms) Security: KMS encryption, CloudTrail audit, IAM vá»›i IRSA DataOps: S3-based data versioning vÃ  lifecycle management 5. Kiáº¿n trÃºc MLOps chi tiáº¿t 5.1 Phase 1: Infrastructure Foundation Terraform Infrastructure as Code\nVPC vá»›i multi-AZ public/private subnets EKS cluster vá»›i managed node groups (auto-scaling) IAM roles vá»›i IRSA (IAM Roles for Service Accounts) Security groups vá»›i least privilege access ECR repositories cho container images Network Architecture\nPublic subnets: NAT Gateway, Load Balancer Private subnets: EKS worker nodes, SageMaker VPC endpoints: S3, ECR, CloudWatch (giáº£m data transfer cost) 5.2 Phase 2: ML Training \u0026amp; Model Management SageMaker Training Pipeline\nData Ingestion: S3 data lake vá»›i automated validation Distributed Training: SageMaker training jobs vá»›i spot instances Model Registry: Versioned model artifacts vá»›i metadata tracking Experiment Tracking: Performance metrics vÃ  hyperparameter tuning Data Management Strategy\nRaw data â†’ Processed data â†’ Feature store â†’ Model artifacts S3 intelligent tiering cho cost optimization Data lineage tracking vÃ  version control 5.3 Phase 3: Containerized Inference Platform EKS Deployment Architecture\nDocker Containers: FastAPI inference service\nKubernetes Deployment: Rolling updates vá»›i zero downtime\nDocker Containers: FastAPI inference service\nKubernetes Deployment: Rolling updates vá»›i zero downtime\nHorizontal Pod Autoscaler: Dynamic scaling dá»±a trÃªn CPU/memory\nService Discovery: Internal service communication\nApplication Load Balancer: External access vá»›i SSL termination\nMonitoring \u0026amp; Observability\nCloudWatch Logs: Centralized logging tá»« táº¥t cáº£ components Custom Metrics: Model performance, latency, throughput Alarms \u0026amp; Notifications: Automated alerting khi cÃ³ issues Dashboards: Real-time visualization cá»§a system health 5.4 Phase 4: CI/CD \u0026amp; Automation Automated Pipeline Flow\n1. Code/Data Change â†’ Git Webhook 2. Jenkins/Travis Build â†’ Run Tests 3. SageMaker Training â†’ Model Validation ```bash 1. Code/Data Change â†’ Git Webhook 2. Jenkins/Travis Build â†’ Run Tests 3. SageMaker Training â†’ Model Validation 4. Docker Build â†’ Push to ECR 5. Kubernetes Deploy â†’ Rolling Update 6. Health Check â†’ Monitor Performance DataOps Workflow\nData Versioning: S3 vá»›i metadata tracking Data Quality: Automated validation vÃ  testing Feature Engineering: Reproducible pipelines Model Deployment: A/B testing capabilities 6. Scope \u0026amp; Expected Outcomes 6.1 In Scope âœ… Complete Infrastructure: Terraform IaC cho toÃ n bá»™ AWS resources\nâœ… ML Training: SageMaker distributed training vá»›i hyperparameter tuning\nâœ… Container Deployment: EKS vá»›i autoscaling vÃ  load balancing\nâœ… Security Best Practices: KMS encryption, CloudTrail audit, IAM least privilege\nâœ… Monitoring \u0026amp; Alerting: CloudWatch comprehensive monitoring\nâœ… CI/CD Automation: End-to-end pipeline tá»« code Ä‘áº¿n production\nâœ… Cost Optimization: Auto-scaling, spot instances, lifecycle policies\n6.2 Out of Scope âŒ Multi-region deployment (focus on ap-southeast-1)\nâŒ Advanced ML features (A/B testing, canary deployments)\nâŒ Real-time streaming inference (batch-focused)\nâŒ Custom monitoring solutions (CloudWatch-only)\n6.3 Expected Outcomes âœ… Cost Optimization: Auto-scaling, spot instances, lifecycle policies\n6.2 Out of Scope âŒ Multi-region deployment (focus on ap-southeast-1)\nâŒ Advanced ML features (A/B testing, canary deployments)\nâŒ Real-time streaming inference (batch-focused)\nâŒ Custom monitoring solutions (CloudWatch-only)\n6.3 Expected Outcomes ğŸ¯ Production-Ready MLOps Platform: Scalable, reliable, cost-effective\nğŸ¯ Automated ML Lifecycle: Tá»« data ingestion Ä‘áº¿n model deployment\nğŸ¯ Infrastructure Reproducibility: Terraform state management\nğŸ¯ Operational Excellence: Comprehensive monitoring vÃ  alerting\nğŸ¯ Cost Efficiency: Optimized resource usage vá»›i auto-scaling\nğŸ¯ Cost Efficiency: Optimized resource usage vá»›i auto-scaling\nKiáº¿n trÃºc nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ support enterprise-grade ML workloads vá»›i kháº£ nÄƒng scale tá»« proof-of-concept Ä‘áº¿n production vá»›i hÃ ng triá»‡u requests/day.\nPrerequisites: AWS account vá»›i permissions cho EKS, SageMaker, ECR, S3, CloudWatch, IAM. Terraform \u0026gt;= 1.0, kubectl, AWS CLI, Docker, Python 3.8+.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/",
	"title": "BÃ¡o cÃ¡o Thá»±c táº­p",
	"tags": [],
	"description": "",
	"content": "BÃ¡o cÃ¡o Thá»±c táº­p ThÃ´ng tin sinh viÃªn: Há» vÃ  tÃªn: HÃ  Kháº£ NguyÃªn\nSá»‘ Ä‘iá»‡n thoáº¡i: 0827979337\nEmail: hakhanguyen09052004@gmail.com\nTrÆ°á»ng: TrÆ°á»ng Äáº¡i há»c CÃ´ng nghá»‡ ThÃ´ng tin (UIT)\nChuyÃªn ngÃ nh: Há»‡ thá»‘ng ThÃ´ng tin\nLá»›p: HTTT2022.2\nCÃ´ng ty thá»±c táº­p: Amazon Web Services Vietnam Co., Ltd.\nVá»‹ trÃ­ thá»±c táº­p: FCJ Cloud Intern\nThá»i gian thá»±c táº­p: Tá»« 12/08/2025 Ä‘áº¿n 12/11/2025\nNá»™i dung bÃ¡o cÃ¡o Nháº­t kÃ½ cÃ´ng viá»‡c Äá» xuáº¥t (Proposal) BÃ i blog dá»‹ch Sá»± kiá»‡n Ä‘Ã£ tham gia Workshop Tá»± Ä‘Ã¡nh giÃ¡ Chia sáº» vÃ  gÃ³p Ã½ "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/",
	"title": "Nháº­t kÃ½ cÃ´ng viá»‡c (Worklog)",
	"tags": [],
	"description": "",
	"content": "Táº¡i trang nÃ y, tÃ´i sáº½ giá»›i thiá»‡u nháº­t kÃ½ cÃ´ng viá»‡c trong ká»³ thá»±c táº­p.\nThá»i lÆ°á»£ng: 12 tuáº§n. Tá»•ng quan: ChÆ°Æ¡ng trÃ¬nh thá»±c táº­p Ä‘Æ°á»£c thiáº¿t káº¿ nháº±m cung cáº¥p cÃ¡i nhÃ¬n toÃ n diá»‡n vá» cÃ¡c dá»‹ch vá»¥ AWS, báº¯t Ä‘áº§u tá»« cÃ¡c khÃ¡i niá»‡m ná»n táº£ng vÃ  tiáº¿n dáº§n Ä‘áº¿n cÃ¡c chá»§ Ä‘á» nÃ¢ng cao, káº¿t thÃºc báº±ng má»™t Dá»± Ã¡n Capstone. Ná»™i dung theo tuáº§n:\nTuáº§n 1: Giá»›i thiá»‡u vá» AWS Tuáº§n 2: Máº¡ng (Networking) trÃªn AWS Tuáº§n 3: Dá»‹ch vá»¥ mÃ¡y áº£o/Compute (VM) trÃªn AWS Tuáº§n 4: Dá»‹ch vá»¥ lÆ°u trá»¯ (Storage) trÃªn AWS Tuáº§n 5: Dá»‹ch vá»¥ báº£o máº­t (Security) trÃªn AWS Tuáº§n 6: Dá»‹ch vá»¥ cÆ¡ sá»Ÿ dá»¯ liá»‡u (Database) trÃªn AWS Tuáº§n 7: Dá»‹ch vá»¥ Data Lake trÃªn AWS Tuáº§n 8: Äiá»‡n toÃ¡n Serverless Tuáº§n 9: Äiá»‡n toÃ¡n Container Tuáº§n 10: DevOps \u0026amp; IaC (Háº¡ táº§ng nhÆ° mÃ£) Tuáº§n 11: Observability (GiÃ¡m sÃ¡t/Quan sÃ¡t) \u0026amp; FinOps Tuáº§n 12: Capstone: Dá»± Ã¡n cÃ¡ nhÃ¢n "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/2-iam-roles-audit/",
	"title": "IAM Roles &amp; Audit For MLops",
	"tags": [],
	"description": "",
	"content": "ğŸ¯ Má»¥c tiÃªu Task 2 Thiáº¿t láº­p phÃ¢n quyá»n truy cáº­p (IAM) cho toÃ n bá»™ dá»‹ch vá»¥ AWS trong pipeline vÃ  báº­t CloudTrail Ä‘á»ƒ giÃ¡m sÃ¡t, ghi láº¡i má»i hoáº¡t Ä‘á»™ng trÃªn tÃ i khoáº£n AWS.\nâ†’ Äáº£m báº£o báº£o máº­t, kiá»ƒm soÃ¡t truy cáº­p, vÃ  minh chá»©ng hoáº¡t Ä‘á»™ng nhÃ³m.\nğŸ“¥ Input\nAWS Account vá»›i quyá»n admin\nConvention Ä‘áº·t tÃªn dá»± Ã¡n: mlops-retail-prediction-dev\nVÃ¹ng má»¥c tiÃªu: ap-southeast-1\nCloudTrail Ä‘a vÃ¹ng Ä‘Æ°á»£c báº­t\nInput tá»« Task 1: Task 1 (Introduction) â€” project conventions, naming, and high-level objectives\nâœ… Output\nCÃ¡c dá»‹ch vá»¥ AWS cÃ³ quyá»n Least Privilege phÃ¹ há»£p vai trÃ² ToÃ n bá»™ thao tÃ¡c Ä‘á»u Ä‘Æ°á»£c CloudTrail ghi láº¡i ÄÃ¡p á»©ng tiÃªu chÃ­ rubric: báº£o máº­t, phÃ¢n quyá»n, quáº£n lÃ½ dá»± Ã¡n trÃªn cloud ğŸ’° Chi phÃ­ Æ°á»›c tÃ­nh â‰ˆ 0.05 USD/thÃ¡ng (CloudTrail + lÆ°u trá»¯ S3 cho logs)\nğŸ“Œ CÃ¡c bÆ°á»›c chÃ­nh\nCloudTrail Setup - Thiáº¿t láº­p audit logging Ä‘a vÃ¹ng S3 CloudTrail Bucket - LÆ°u trá»¯ log táº­p trung EKS Cluster Service Role - Role cho control plane EKS Node Group Role - Role cho worker node SageMaker Execution Role - Role cho training \u0026amp; deploy IRSA Foundation - Chuáº©n bá»‹ quyá»n á»Ÿ má»©c Pod âœ… Deliverables\nCloudTrail multi-region trail vá»›i logging vÃ o S3 EKS Cluster Service Role (Console) EKS Node Group Role vá»›i quyá»n ECR/S3/CloudWatch SageMaker Execution Role vá»›i quyá»n S3 cáº§n thiáº¿t Ná»n táº£ng an toÃ n cho thiáº¿t láº­p IRSA ğŸ“Š Acceptance Criteria\nCloudTrail ghi láº¡i táº¥t cáº£ API calls vÃ  hoáº¡t Ä‘á»™ng ngÆ°á»i dÃ¹ng EKS cluster cÃ³ thá»ƒ táº¡o vá»›i cÃ¡c service role phÃ¹ há»£p SageMaker training jobs cÃ³ quyá»n Ä‘á»c/ghi S3 Node groups cÃ³ thá»ƒ pull image tá»« ECR Táº¥t cáº£ IAM role tuÃ¢n thá»§ principle of least privilege Audit trail sáºµn sÃ ng cho compliance vÃ  monitoring 1. CloudTrail Setup - Ná»n táº£ng Audit 1.1. Táº¡o S3 Bucket cho CloudTrail Äi tá»›i S3 Console: AWS Console â†’ S3 â†’ \u0026ldquo;Create bucket\u0026rdquo;\nCáº¥u hÃ¬nh Bucket:\nBucket name: mlops-cloudtrail-logs-us-east-1-842676018087\rRegion: us-east-1 (pháº£i cÃ¹ng region vá»›i CloudTrail trail)\rBlock all public access: âœ… Enabled\rVersioning: âœ… Enabled Default encryption: âœ… AWS KMS\rKMS key: alias/mlops-retail-prediction-dev-cloudtrail-key Cáº¥u hÃ¬nh Lifecycle Policy:\nBÆ°á»›c 1. S3 Console â†’ chá»n bucket mlops-cloudtrail-logs-ap-southeast-1 â†’ Management â†’ Create lifecycle rule.\nBÆ°á»›c 2. Äáº·t tÃªn (vÃ­ dá»¥ CloudTrailLogLifecycle), Apply to all objects hoáº·c dÃ¹ng Prefix mlops-logs/.\nChi tiáº¿t cÃ¡c trÆ°á»ng cáº¥u hÃ¬nh:\nObject tags:\nâŒ KhÃ´ng cáº§n thÃªm tags vÃ¬ chÃºng ta Ä‘Ã£ dÃ¹ng prefix Ä‘á»ƒ lá»c Object size:\nâŒ KhÃ´ng cáº§n specify minimum object size âŒ KhÃ´ng cáº§n specify maximum object size CloudTrail logs thÆ°á»ng cÃ³ kÃ­ch thÆ°á»›c nhá» vÃ  Ä‘á»“ng Ä‘á»u Lifecycle rule actions:\nâœ… Transition current versions of objects between storage classes Chá»n Ä‘á»ƒ tá»± Ä‘á»™ng chuyá»ƒn logs sang storage class ráº» hÆ¡n âŒ Transition noncurrent versions of objects between storage classes KhÃ´ng cáº§n vÃ¬ CloudTrail logs khÃ´ng cÃ³ nhiá»u versions âŒ Expire current versions of objects KhÃ´ng expire vÃ¬ cáº§n giá»¯ logs cho audit âŒ Permanently delete noncurrent versions of objects KhÃ´ng xÃ³a vÃ¬ cáº§n giá»¯ lá»‹ch sá»­ logs âœ… Delete expired object delete markers or incomplete multipart uploads Chá»n Ä‘á»ƒ dá»n dáº¹p cÃ¡c markers vÃ  uploads lá»—i BÆ°á»›c 3. Chá»n actions (Current versions):\nAfter 30 days â†’ STANDARD_IA After 90 days â†’ GLACIER / GLACIER_IR (tÃ¹y chá»n) After 365 days â†’ DEEP_ARCHIVE BÆ°á»›c 4. Kiá»ƒm tra rule Ä‘Ã£ Active trong tab Management.\n1.2 Cáº¥u hÃ¬nh Trail âš ï¸ LÆ°u Ã½ vá» Region:\nCloudTrail lÃ  multi-region service nhÆ°ng trail pháº£i Ä‘Æ°á»£c táº¡o á»Ÿ má»™t region cá»¥ thá»ƒ (home region) S3 bucket vÃ  KMS key pháº£i Ä‘Æ°á»£c táº¡o á»Ÿ cÃ¹ng region vá»›i CloudTrail trail Trong trÆ°á»ng há»£p nÃ y, chÃºng ta sáº½ táº¡o táº¥t cáº£ resource á»Ÿ region us-east-1 1.3. Khuyáº¿n nghá»‹: Äá»“ng bá»™ region giá»¯a S3 vÃ  SageMaker Project Ngáº¯n gá»n: Náº¿u dá»¯ liá»‡u chÃ­nh cá»§a pipeline (prefix gold/ vÃ  artifacts/) náº±m trong us-east-1, hÃ£y táº¡o SageMaker Domain / Project á»Ÿ us-east-1 Ä‘á»ƒ trÃ¡nh lá»—i cross-region (S3 301), phá»©c táº¡p vá»›i KMS keys, vÃ  cÃ¡c endpoint khÃ¡c.\nNáº¿u tá»• chá»©c yÃªu cáº§u SageMaker pháº£i á»Ÿ ap-southeast-1, báº¡n cáº§n sao chÃ©p hoáº·c replicate dá»¯ liá»‡u sang bucket á»Ÿ ap-southeast-1 trÆ°á»›c khi táº¡o Project. VÃ­ dá»¥ lá»‡nh sync (PowerShell / CloudShell):\naws s3 mb s3://mlops-retail-prediction-dev-842676018087-apse1 --region ap-southeast-1 aws s3 sync s3://mlops-retail-prediction-dev-842676018087/gold/ s3://mlops-retail-prediction-dev-842676018087-apse1/gold/ --acl bucket-owner-full-control aws s3 sync s3://mlops-retail-prediction-dev-842676018087/artifacts/ s3://mlops-retail-prediction-dev-842676018087-apse1/artifacts/ --acl bucket-owner-full-control KÃ¨m theo:\nTáº¡o KMS key á»Ÿ region Ä‘Ã­ch náº¿u dÃ¹ng SSE-KMS. Cáº­p nháº­t IAM policies Ä‘á»ƒ cho phÃ©p SageMaker role truy cáº­p bucket má»›i. Gá»£i Ã½: Cho lab vÃ  debug nhanh, phÆ°Æ¡ng Ã¡n Ã­t rá»§i ro lÃ  táº¡o Project/Domain á»Ÿ nÆ¡i bucket hiá»‡n cÃ³ (á»Ÿ lab nÃ y lÃ  us-east-1).\n1.2.1 Táº¡o KMS Key cho CloudTrail Táº¡o KMS Key (á»Ÿ region us-east-1): AWS Console â†’ KMS â†’ us-east-1 â†’ Customer managed keys â†’ Create key Cáº¥u hÃ¬nh Key:\nKey type: âœ… Symmetric (mÃ£ hÃ³a vÃ  giáº£i mÃ£ dá»¯ liá»‡u)\rKey usage: âœ… Encrypt and decrypt ThÃªm labels:\nAlias: alias/mlops-retail-prediction-dev-cloudtrail-key\rDescription (optional): KMS key for CloudTrail logs encryption\rTags (optional): - Key: Project\r- Value: MLOps-Retail-Prediction Cáº¥u hÃ¬nh quyá»n quáº£n trá»‹:\nKey administrators: Chá»n IAM users/roles Ä‘Æ°á»£c phÃ©p quáº£n lÃ½ key\rKey deletion: CÃ³ cho phÃ©p xÃ³a key hay khÃ´ng Cáº¥u hÃ¬nh quyá»n sá»­ dá»¥ng:\nKey users: - ThÃªm service principal: cloudtrail.amazonaws.com Chá»‰nh sá»­a key policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;Key policy created by CloudTrail\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Enable IAM User Permissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: [ \u0026#34;arn:aws:iam::842676018087:root\u0026#34; ] }, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow CloudTrail to encrypt logs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:GenerateDataKey*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;kms:EncryptionContext:aws:cloudtrail:arn\u0026#34;: [ \u0026#34;arn:aws:cloudtrail:*:842676018087:trail/*\u0026#34; ] }, \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:us-east-1:842676018087:trail/mlops-retail-prediction-audit-trail\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow CloudTrail to describe key\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } CÃ¡c Ä‘iá»ƒm quan trá»ng trong KMS policy: Enable IAM User Permissions: Cho phÃ©p root account quáº£n lÃ½ key Allow CloudTrail to encrypt logs:\nCho phÃ©p generateDataKey vá»›i Ä‘iá»u kiá»‡n EncryptionContext vÃ  SourceArn EncryptionContext giá»›i háº¡n cho CloudTrail trails trong account SourceArn chá»‰ Ä‘á»‹nh chÃ­nh xÃ¡c trail Ä‘Æ°á»£c phÃ©p sá»­ dá»¥ng Allow CloudTrail to describe key: Cho phÃ©p CloudTrail xem thÃ´ng tin key Key Policy máº·c Ä‘á»‹nh cho CloudTrail: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;Key policy created by CloudTrail\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Enable IAM User Permissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::842676018087:root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow CloudTrail to encrypt logs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:GenerateDataKey*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:us-east-1:842676018087:trail/mlops-retail-prediction-audit-trail\u0026#34; }, \u0026#34;StringLike\u0026#34;: { \u0026#34;kms:EncryptionContext:aws:cloudtrail:arn\u0026#34;: \u0026#34;arn:aws:cloudtrail:*:842676018087:trail/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow CloudTrail to describe key\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow principals in the account to decrypt log files\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;*\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:ReEncryptFrom\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;kms:CallerAccount\u0026#34;: \u0026#34;842676018087\u0026#34; }, \u0026#34;StringLike\u0026#34;: { \u0026#34;kms:EncryptionContext:aws:cloudtrail:arn\u0026#34;: \u0026#34;arn:aws:cloudtrail:*:842676018087:trail/*\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;Enable cross account log decryption\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;*\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:ReEncryptFrom\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;kms:CallerAccount\u0026#34;: \u0026#34;842676018087\u0026#34; }, \u0026#34;StringLike\u0026#34;: { \u0026#34;kms:EncryptionContext:aws:cloudtrail:arn\u0026#34;: \u0026#34;arn:aws:cloudtrail:*:842676018087:trail/*\u0026#34; } } } ] } Key Policy bao gá»“m:\nCho phÃ©p root account quáº£n lÃ½ key Cho phÃ©p CloudTrail mÃ£ hÃ³a logs vá»›i Ä‘iá»u kiá»‡n trail ARN khá»›p Cho phÃ©p CloudTrail xem thÃ´ng tin key Cho phÃ©p cÃ¡c principal trong account giáº£i mÃ£ logs Há»— trá»£ giáº£i mÃ£ logs cross-account náº¿u cáº§n KMS key pháº£i Ä‘Æ°á»£c táº¡o á»Ÿ cÃ¹ng Region vá»›i S3 bucket vÃ  cÃ³ Ä‘Ãºng policy cho phÃ©p CloudTrail sá»­ dá»¥ng.\n1.2.2 Cáº¥u hÃ¬nh S3 Bucket Policy VÃ o S3 bucket permissions:\nS3 Console â†’ mlops-cloudtrail-logs-ap-southeast-1 â†’ Permissions â†’ Bucket policy Policy máº·c Ä‘á»‹nh cho S3 bucket:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AWSCloudTrailAclCheck\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::mlops-cloudtrail-logs-ap-southeast-1-842676018087\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:us-east-1:842676018087:trail/mlops-retail-prediction-audit-trail\u0026#34; } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AWSCloudTrailWrite\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudtrail.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:PutObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::mlops-cloudtrail-logs-ap-southeast-1-842676018087/mlops-logs/AWSLogs/842676018087/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudtrail:us-east-1:842676018087:trail/mlops-retail-prediction-audit-trail\u0026#34;, \u0026#34;s3:x-amz-acl\u0026#34;: \u0026#34;bucket-owner-full-control\u0026#34; } } } ] } S3 Bucket Policy bao gá»“m:\nAWSCloudTrailAclCheck: Cho phÃ©p CloudTrail kiá»ƒm tra ACL cá»§a bucket AWSCloudTrailWrite: Cho phÃ©p CloudTrail ghi logs vÃ o bucket Conditions: aws:SourceArn: Äáº£m báº£o chá»‰ trail cá»¥ thá»ƒ cÃ³ thá»ƒ truy cáº­p s3:x-amz-acl: Äáº£m báº£o bucket owner cÃ³ full control vá»›i objects Policy nÃ y cho phÃ©p CloudTrail kiá»ƒm tra ACL cá»§a bucket vÃ  ghi logs vÃ o bucket.\n1.2.3 Táº¡o CloudTrail BÆ°á»›c 1: Táº¡o Trail má»›i\nAWS Console â†’ us-east-1 â†’ CloudTrail â†’ Create trail BÆ°á»›c 2: Cáº¥u hÃ¬nh Trail cÆ¡ báº£n (á»Ÿ region us-east-1)\nMá»¥c GiÃ¡ trá»‹ Trail name mlops-retail-prediction-audit-trail Apply trail to all regions âœ… Yes Management events âœ… Read/Write Data events âœ… S3 bucket data events Insights events âœ… Enabled (phÃ¡t hiá»‡n hÃ nh vi báº¥t thÆ°á»ng) BÆ°á»›c 3: Cáº¥u hÃ¬nh Storage\nMá»¥c GiÃ¡ trá»‹ S3 bucket mlops-cloudtrail-logs-ap-southeast-1 Log file prefix mlops-logs/ Log file SSE-KMS encryption âœ… Enabled AWS KMS alias alias/mlops-retail-prediction-dev-cloudtrail-key (chá»n key Ä‘Ã£ táº¡o) BÆ°á»›c 4: TÃ­ch há»£p CloudWatch Logs (tÃ¹y chá»n)\nMá»¥c GiÃ¡ trá»‹ CloudWatch Logs âœ… Enabled Log group mlops-cloudtrail-log-group IAM Role CloudTrail_CloudWatchLogs_Role (auto-created) Role CloudTrail_CloudWatchLogs sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng táº¡o vá»›i cÃ¡c quyá»n cáº§n thiáº¿t: logs:PutLogEvents, logs:CreateLogStream, logs:DescribeLogStreams\nBÆ°á»›c 5: Review vÃ  Create trail\nThá»© tá»± quan trá»ng Ä‘á»ƒ trÃ¡nh lá»—i:\nâœ… Táº¡o KMS key vá»›i policy phÃ¹ há»£p âœ… Cáº¥u hÃ¬nh S3 bucket policy âœ… Táº¡o CloudTrail vá»›i KMS vÃ  S3 Ä‘Ã£ setup âœ… Kiá»ƒm tra logs Ä‘Æ°á»£c ghi thÃ nh cÃ´ng 2. Thiáº¿t láº­p IAM Roles - Quyá»n cho dá»‹ch vá»¥ 2.1. EKS Cluster Service Role AWS Console â†’ IAM â†’ Roles â†’ \u0026ldquo;Create role\u0026rdquo; Trusted Entity Type: AWS service\rService: EKS - Cluster GÃ¡n Policy: Policy: AmazonEKSClusterPolicy Chi tiáº¿t Role: Role name: mlops-retail-prediction-dev-eks-cluster-role\rDescription: EKS cluster service role for retail prediction MLOps platform 2.2. EKS Node Group Role TÆ°Æ¡ng tá»± Trusted Entity Type: AWS service\rService: EC2 GÃ¡n Policies: âœ… AmazonEKSWorkerNodePolicy\râœ… AmazonEKS_CNI_Policy\râœ… AmazonEC2ContainerRegistryReadOnly\râœ… CloudWatchAgentServerPolicy Chi tiáº¿t Role: Role name: mlops-retail-prediction-dev-eks-nodegroup-role\rDescription: EKS node group role with ECR, S3, and CloudWatch access for retail prediction 2.3. SageMaker Execution Role Trusted Entity Type:\nAWS service\rService: SageMaker GÃ¡n Policies:\nâœ… AmazonSageMakerFullAccess\râœ… AmazonS3FullAccess (cho lÆ°u trá»¯ dá»¯ liá»‡u vÃ  model)\râœ… CloudWatchLogsFullAccess (cho training job logs) ThÃªm Inline Policy cho EC2 (Báº®T BUá»˜C cho Projects):\nPolicy Name: SageMakerEC2Access\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeVpcs\u0026#34;, \u0026#34;ec2:DescribeSubnets\u0026#34;, \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;, \u0026#34;ec2:DescribeAvailabilityZones\u0026#34;, \u0026#34;ec2:DescribeRouteTables\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } CÃ¡ch thÃªm:\nIAM Console â†’ Roles â†’ mlops-retail-prediction-dev-sagemaker-execution Permissions tab â†’ Add permissions â†’ Create inline policy JSON tab â†’ paste policy trÃªn Policy name: SageMakerEC2Access Chi tiáº¿t Role:\nRole name: mlops-retail-prediction-dev-sagemaker-execution\rDescription: SageMaker execution role for retail prediction training jobs and model deployment 2.4. Báº®T BUá»˜C: ThÃªm EC2 Permissions VÃ¬ SageMaker Projects lÃ  báº¯t buá»™c, cáº§n thÃªm EC2 permissions ngay:\nIAM Console â†’ Roles â†’ mlops-retail-prediction-dev-sagemaker-execution Permissions tab â†’ Add permissions â†’ Create inline policy JSON tab â†’ paste policy sau: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:DescribeVpcs\u0026#34;, \u0026#34;ec2:DescribeSubnets\u0026#34;, \u0026#34;ec2:DescribeSecurityGroups\u0026#34;, \u0026#34;ec2:DescribeNetworkInterfaces\u0026#34;, \u0026#34;ec2:DescribeAvailabilityZones\u0026#34;, \u0026#34;ec2:DescribeRouteTables\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Review â†’ Policy name: SageMakerEC2Access Create policy âœ… XÃ¡c minh: Role pháº£i cÃ³ 4 policies:\nAmazonSageMakerFullAccess (AWS managed) AmazonS3FullAccess (AWS managed) CloudWatchLogsFullAccess (AWS managed) SageMakerEC2Access (inline policy vá»«a táº¡o) SageMaker Unified Studio (2024+) yÃªu cáº§u:\nâœ… Projects lÃ  báº¯t buá»™c - khÃ´ng thá»ƒ bá» qua âœ… EC2 permissions lÃ  Báº®T BUá»˜C - pháº£i thÃªm inline policy âœ… Project profile cáº§n Ä‘Æ°á»£c setup trÆ°á»›c Náº¿u thiáº¿u EC2 permissions:\nâŒ \u0026ldquo;Create project\u0026rdquo; sáº½ fail âŒ \u0026ldquo;Insufficient permissions to describe VPCs\u0026rdquo; âŒ KhÃ´ng thá»ƒ truy cáº­p Studio notebooks âœ… GIáº¢I PHÃP:\nBáº¯t buá»™c thÃªm inline policy EC2 á»Ÿ trÃªn Táº¡o Project vá»›i \u0026ldquo;ML and generative AI model development\u0026rdquo; Studio sáº½ hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng 3. XÃ¡c thá»±c \u0026amp; Kiá»ƒm tra an ninh 3.1. XÃ¡c minh CloudTrail Kiá»ƒm tra tráº¡ng thÃ¡i CloudTrail: AWS Console â†’ CloudTrail â†’ Trails\nâœ… mlops-retail-prediction-audit-trail: Active\râœ… Multi-region trail: Enabled\râœ… Management events: Read/Write\râœ… Data events: S3 configured\râœ… CloudWatch Logs: Integrated XÃ¡c minh S3 Logging: AWS Console â†’ S3 â†’ mlops-cloudtrail-logs-ap-southeast-1\nâœ… Log files Ä‘Æ°á»£c táº¡o: /mlops-logs/AWSLogs/[account-id]/CloudTrail/\râœ… Encryption: SSE-S3 enabled\râœ… Lifecycle policy: Applied\râœ… Access logging: Configured 3.2. Tá»•ng há»£p IAM Roles Äi tá»›i IAM â†’ Roles vÃ  kiá»ƒm tra:\nâœ… mlops-retail-prediction-dev-eks-cluster-role\râœ… mlops-retail-prediction-dev-eks-nodegroup-role âœ… mlops-retail-prediction-dev-sagemaker-execution\râœ… CloudTrail_CloudWatchLogs_Role (auto-created) Kiá»ƒm tra Trust Relationships:\nVÃ o tá»«ng role â†’ tab Trust relationships XÃ¡c minh cÃ¡c trusted entities Ä‘Ãºng: eks.amazonaws.com (EKS cluster role) ec2.amazonaws.com (EKS node group role) sagemaker.amazonaws.com (SageMaker execution role) cloudtrail.amazonaws.com (CloudTrail logging role) 3.3. Kiá»ƒm tra báº£o máº­t Kiá»ƒm tra CloudTrail Logging:\nThá»±c hiá»‡n má»™t API call thá»­ nghiá»‡m (vÃ­ dá»¥ list S3 buckets) Kiá»ƒm tra CloudTrail logs trong 5-10 phÃºt XÃ¡c nháº­n event xuáº¥t hiá»‡n trong CloudWatch Logs Kiá»ƒm tra quyá»n IAM:\n# Test SageMaker role cÃ³ thá»ƒ assume vÃ  truy cáº­p S3 aws sts assume-role --role-arn arn:aws:iam::ACCOUNT:role/mlops-retail-prediction-dev-sagemaker-execution --role-session-name test Kiá»ƒm tra SageMaker role Ä‘áº§y Ä‘á»§:\n# Test S3 access aws s3 ls s3://mlops-retail-prediction-dev-ACCOUNT/ --profile sagemaker-test # Test SageMaker training job permissions aws sagemaker list-training-jobs --region us-east-1 --profile sagemaker-test # Test EC2 permissions (náº¿u Ä‘Ã£ thÃªm) aws ec2 describe-vpcs --region us-east-1 --profile sagemaker-test # Test EKS roles sáºµn sÃ ng cho viá»‡c táº¡o cluster aws eks describe-cluster --name test-cluster --region ap-southeast-1 4. Tá»‘i Æ°u chi phÃ­ \u0026amp; TuÃ¢n thá»§ 4.1. Quáº£n lÃ½ chi phÃ­ CloudTrail â€” Báº£ng so sÃ¡nh Háº¡ng má»¥c ÄÆ¡n giÃ¡ Ghi chÃº / Assumptions VÃ­ dá»¥ Æ°á»›c tÃ­nh S3 â€” Standard $0.023 / GBâ€‘month Hot logs (ngÃ y 0â€“30) 1 GB â†’ $0.023 S3 â€” Standardâ€‘IA $0.0125 / GBâ€‘month Sau 30 ngÃ y (truy cáº­p Ã­t) 1 GB â†’ $0.0125 S3 â€” Glacier $0.004 / GBâ€‘month LÆ°u trá»¯ dÃ i háº¡n (90â€“365 ngÃ y) 1 GB â†’ $0.004 S3 â€” Deep Archive $0.00099 / GBâ€‘month Retention \u0026gt;365 ngÃ y 1 GB â†’ $0.00099 CloudTrail â€” Management events Miá»…n phÃ­ (báº£n sao Ä‘áº§u tiÃªn) Management API calls â€” CloudTrail â€” Data events $0.10 / 100,000 events S3 object-level, Lambda, v.v. 100k events â†’ $0.10 CloudTrail â€” Insights $0.35 / 100,000 events TÃ¹y chá»n phÃ¡t hiá»‡n báº¥t thÆ°á»ng 100k events â†’ $0.35 TÃ¬nh huá»‘ng máº«u (Æ°á»›c tÃ­nh hÃ ng thÃ¡ng)\nMinimal (vÃ­ dá»¥ project nhá»): 0.5 GB lÆ°u trá»¯ (chá»§ yáº¿u Standardâ€‘IA) + 10k data events\nâ†’ S3 â‰ˆ 0.5 * $0.0125 = $0.0063 ; Data events â‰ˆ (10k/100k)*$0.10 = $0.01\nâ†’ Tá»•ng â‰ˆ $0.016 â†’ khá»›p khoáº£ng \u0026ldquo;â‰ˆ $0.01â€“$0.02\u0026rdquo; Typical (logs tÄƒng, vÃ i chá»¥c GB, vÃ i chá»¥c nghÃ¬n events): 5 GB pha trá»™n cÃ¡c lá»›p + 50k data events\nâ†’ S3 (mix) â‰ˆ $0.02â€“$0.04 ; Data events â‰ˆ $0.05 ; Insights (tuá»³ dÃ¹ng) cÃ³ thá»ƒ thÃªm $0.00â€“$0.35\nâ†’ Tá»•ng ~ $0.02â€“$0.05 (thÆ°á»ng tháº¥y cho dá»± Ã¡n nhá») Ghi chÃº ngáº¯n:\nLifecycle chuyá»ƒn objects sang IA/Glacier/Deep Archive lÃ  chÃ¬a khoÃ¡ giáº£m chi phÃ­ dÃ i háº¡n. Data events vÃ  Insights tÄƒng theo sá»‘ events â€” tá»‘i Æ°u sampling / chá»‰ log cáº§n thiáº¿t Ä‘á»ƒ tiáº¿t kiá»‡m. Kiá»ƒm tra thá»±c táº¿ báº±ng billing/Cost Explorer Ä‘á»ƒ hiá»‡u chá»‰nh cÃ¡c giáº£ Ä‘á»‹nh trÃªn. 5. Clean Up Resources (HÆ°á»›ng dáº«n xoÃ¡ tÃ i nguyÃªn) Cáº£nh bÃ¡o: CÃ¡c lá»‡nh bÃªn dÆ°á»›i sáº½ xÃ³a tÃ i nguyÃªn thá»±c táº¿. Kiá»ƒm tra tÃªn tÃ i nguyÃªn (bucket, role, trail, key) trÆ°á»›c khi cháº¡y.\n5.1 XÃ³a CloudTrail PowerShell (AWS CLI):\n# XÃ³a trail (náº¿u tÃªn chÃ­nh xÃ¡c) aws cloudtrail delete-trail --name mlops-retail-prediction-audit-trail # Náº¿u muá»‘n táº¯t ghi sang CloudWatch Logs trÆ°á»›c aws cloudtrail update-trail --name mlops-retail-prediction-audit-trail --cloud-watch-logs-log-group-arn \u0026#34;\u0026#34; --cloud-watch-logs-role-arn \u0026#34;\u0026#34; 5.2 XÃ³a S3 CloudTrail Bucket vÃ  ná»™i dung LÆ°u Ã½: Bucket cÃ³ thá»ƒ náº±m á»Ÿ us-east-1 theo cáº¥u hÃ¬nh trÃªn. Kiá»ƒm tra aws s3 ls/console trÆ°á»›c khi xÃ³a.\n# XÃ³a táº¥t cáº£ objects (recursive) aws s3 rm s3://mlops-cloudtrail-logs-ap-southeast-1 --recursive # XÃ³a bucket aws s3api delete-bucket --bucket mlops-cloudtrail-logs-ap-southeast-1 --region us-east-1 5.3 Há»§y KMS Key (schedule delete) KMS keys khÃ´ng thá»ƒ bá»‹ xÃ³a ngay láº­p tá»©c náº¿u Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng. Ta nÃªn lÃªn lá»‹ch xÃ³a an toÃ n (vÃ­ dá»¥ 7 ngÃ y):\n# TÃ¬m KeyId tá»« alias $keyId = aws kms list-aliases --query \u0026#34;Aliases[?AliasName==\u0026#39;alias/mlops-retail-prediction-dev-cloudtrail-key\u0026#39;].TargetKeyId\u0026#34; --output text # LÃªn lá»‹ch xÃ³a key (pending days: 7 - 30) aws kms schedule-key-deletion --key-id $keyId --pending-window-in-days 7 5.4 Gá»¡ IAM Roles \u0026amp; Policies (EKS / SageMaker / CloudTrail) Quy trÃ¬nh an toÃ n: 1) Detach managed policies 2) XÃ³a inline policies 3) XÃ³a role.\n# VÃ­ dá»¥: xÃ³a SageMaker execution role $roleName = \u0026#39;mlops-retail-prediction-dev-sagemaker-execution\u0026#39; # 1) Liá»‡t kÃª vÃ  detach managed policies aws iam list-attached-role-policies --role-name $roleName --query \u0026#39;AttachedPolicies[].PolicyArn\u0026#39; --output text | ForEach-Object { aws iam detach-role-policy --role-name $roleName --policy-arn $_ } # 2) XÃ³a inline policies aws iam list-role-policies --role-name $roleName --query \u0026#39;PolicyNames\u0026#39; --output text | ForEach-Object { aws iam delete-role-policy --role-name $roleName --policy-name $_ } # 3) XÃ³a role aws iam delete-role --role-name $roleName # Láº·p láº¡i cho cÃ¡c role khÃ¡c (EKS cluster/nodegroup, CloudTrail_CloudWatchLogs_Role, GitHub/CI roles, v.v.) 5.5 Gá»¡ Container Insights / CloudWatch integration # XÃ³a CloudWatch log group (náº¿u cÃ³) aws logs delete-log-group --log-group-name \u0026#34;/aws/containerinsights/mlops-retail-cluster/application\u0026#34; || Write-Host \u0026#39;Log group not found\u0026#39; # XÃ³a CloudWatch log group cho CloudTrail integration aws logs delete-log-group --log-group-name \u0026#34;mlops-cloudtrail-log-group\u0026#34; || Write-Host \u0026#39;Log group not found\u0026#39; # Disable Container Insights addon from EKS (náº¿u Ã¡p dá»¥ng) aws eks delete-addon --cluster-name mlops-retail-cluster --addon-name amazon-cloudwatch-observability 5.6 XÃ³a ECR images (náº¿u muá»‘n dá»n sáº¡ch images dev/staging) # XÃ³a images theo tag aws ecr batch-delete-image --repository-name mlops/retail-api --image-ids imageTag=dev,imageTag=staging || Write-Host \u0026#39;No matching images or already deleted\u0026#39; # XÃ³a untagged images (tháº­n trá»ng) aws ecr describe-images --repository-name mlops/retail-api --filter tagStatus=UNTAGGED --query \u0026#39;imageDetails[].imageDigest\u0026#39; --output text | ForEach-Object { aws ecr batch-delete-image --repository-name mlops/retail-api --image-ids imageDigest=$_ } 5.7 Dá»«ng / XÃ³a SageMaker training jobs, endpoints, model packages # Stop in-progress training jobs with name pattern aws sagemaker list-training-jobs --name-contains \u0026#34;retail-\u0026#34; --status-equals InProgress --query \u0026#39;TrainingJobSummaries[].TrainingJobName\u0026#39; --output text | ForEach-Object { aws sagemaker stop-training-job --training-job-name $_ } # Delete failed endpoints aws sagemaker list-endpoints --name-contains \u0026#34;retail-\u0026#34; --query \u0026#39;Endpoints[?EndpointStatus==`Failed`].EndpointName\u0026#39; --output text | ForEach-Object { aws sagemaker delete-endpoint --endpoint-name $_ } # Delete pending model packages in model group (tháº­n trá»ng: giá»¯ cÃ¡c approved) aws sagemaker list-model-packages --model-package-group-name \u0026#34;retail-forecast-models\u0026#34; --model-approval-status PendingManualApproval --query \u0026#39;ModelPackageSummaryList[].ModelPackageArn\u0026#39; --output text | ForEach-Object { aws sagemaker delete-model-package --model-package-name $_ } 5.8 Kiá»ƒm tra vÃ  xÃ¡c nháº­n (Verification) # Kiá»ƒm tra trail Ä‘Ã£ bá»‹ xÃ³a aws cloudtrail describe-trails --query \u0026#39;trailList[?Name==`mlops-retail-prediction-audit-trail`]\u0026#39; || Write-Host \u0026#39;Trail removed or not found\u0026#39; # Kiá»ƒm tra bucket aws s3 ls s3://mlops-cloudtrail-logs-ap-southeast-1 2\u0026gt;$null || Write-Host \u0026#39;Bucket removed or empty\u0026#39; # Kiá»ƒm tra IAM role aws iam get-role --role-name mlops-retail-prediction-dev-sagemaker-execution 2\u0026gt;$null || Write-Host \u0026#39;Role removed\u0026#39; # Kiá»ƒm tra KMS key scheduled deletion aws kms list-keys --query \u0026#39;Keys[?KeyId==`\u0026#39;$keyId\u0026#39;`]\u0026#39; || Write-Host \u0026#39;Check key deletion schedule manually in KMS console\u0026#39; Náº¿u báº¡n muá»‘n, tÃ´i cÃ³ thá»ƒ:\nthÃªm phiÃªn báº£n PowerShell script tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ bÆ°á»›c cleanup (cáº§n confirm tÃªn tÃ i nguyÃªn) hoáº·c thay tháº¿ cÃ¡c lá»‡nh ForEach-Object báº±ng cÃ¡c script an toÃ n hÆ¡n Ä‘á»ƒ preview danh sÃ¡ch tÃ i nguyÃªn trÆ°á»›c khi xÃ³a. ğŸ“¹ Video thá»±c hiá»‡n Task 2 ğŸ‘‰ Káº¿t quáº£ Task 2 âœ… CloudTrail Multi-Region - Báº£n ghi kiá»ƒm toÃ¡n toÃ n diá»‡n cho táº¥t cáº£ hoáº¡t Ä‘á»™ng AWS\nâœ… LÆ°u trá»¯ Audit S3 - LÆ°u giá»¯ log tá»‘i Æ°u chi phÃ­ vá»›i lifecycle policies\nâœ… Role Báº£o máº­t EKS - Quyá»n cho cluster vÃ  node group Ä‘Ã£ sáºµn sÃ ng\nâœ… Role Thá»±c thi SageMaker - Training jobs + Projects ready (EC2 permissions Báº®T BUá»˜C)\nâœ… Ná»n táº£ng Báº£o máº­t - Kiáº¿n trÃºc least-privilege chuáº©n doanh nghiá»‡p\nâœ… Sáºµn sÃ ng TuÃ¢n thá»§ - Audit trail phÃ¹ há»£p vá»›i yÃªu cáº§u phÃ¡p lÃ½\nğŸ’° Chi phÃ­ hÃ ng thÃ¡ng: ~0.05 USD (CloudTrail + lÆ°u trá»¯ S3)\nğŸ” Phá»§ sÃ³ng kiá»ƒm toÃ¡n: 100% cÃ¡c API call vÃ  hoáº¡t Ä‘á»™ng ngÆ°á»i dÃ¹ng\nğŸ›¡ï¸ TÆ° tháº¿ báº£o máº­t: Quyá»n truy cáº­p theo nguyÃªn táº¯c least-privilege, sáºµn sÃ ng cho production\nğŸš€ BÆ°á»›c tiáº¿p theo:\nTask 3: Thiáº¿t láº­p S3 data lake vá»›i tÃ­ch há»£p báº£o máº­t Task 4: VPC networking vá»›i security groups Task 5: Triá»ƒn khai EKS cluster vá»›i IAM roles Ä‘Ã£ cáº¥u hÃ¬nh Task 6: Thiáº¿t láº­p IRSA cho quyá»n á»Ÿ má»©c Pod ğŸ” LÆ°u Ã½ báº£o máº­t:\nCloudTrail logs chá»©a thÃ´ng tin nháº¡y cáº£m - Ä‘áº£m báº£o báº£o máº­t bucket S3 SageMaker role cáº§n EC2 permissions (Projects báº¯t buá»™c tá»« 2024) Pháº£i thÃªm inline policy EC2 Ä‘á»ƒ táº¡o Ä‘Æ°á»£c Projects TÃªn role sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng chÃ­nh xÃ¡c trong cÃ¡c task tiáº¿p theo IRSA yÃªu cáº§u OIDC provider cho EKS (Task 5) GiÃ¡m sÃ¡t chi phÃ­ CloudTrail báº±ng AWS Cost Explorer RÃ  soÃ¡t logs Ä‘á»‹nh ká»³ Ä‘á»ƒ phÃ¡t hiá»‡n hoáº¡t Ä‘á»™ng báº¥t thÆ°á»ng "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Ná»n táº£ng MLOps Dá»± Ä‘oÃ¡n Price Sensitivity trÃªn AWS End-to-End MLOps: Data Lake â†’ Training â†’ Model Registry â†’ Container â†’ EKS API â†’ Monitoring â†’ CI/CD â†’ Tá»‘i Æ°u chi phÃ­ 1. TÃ³m táº¯t (Executive Summary) Workshop nÃ y xÃ¢y dá»±ng má»™t quy trÃ¬nh MLOps hoÃ n chá»‰nh trÃªn AWS cho Retail Prediction API, dá»± Ä‘oÃ¡n nhÃ£n BASKET_PRICE_SENSITIVITY (Low/Medium/High) tá»« dá»¯ liá»‡u bÃ¡n láº» Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hoÃ¡. Há»‡ thá»‘ng sá»­ dá»¥ng S3 lÃ m data lake (raw/silver/gold), pipeline training trÃªn Amazon SageMaker (RandomForest) cÃ³ Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng (Accuracy â‰¥ 0.8, F1 â‰¥ 0.7), quáº£n lÃ½ phiÃªn báº£n qua SageMaker Model Registry, Ä‘Ã³ng gÃ³i inference báº±ng FastAPI Ä‘áº©y lÃªn Amazon ECR, triá»ƒn khai production-like trÃªn Amazon EKS cÃ³ autoscaling vÃ  public endpoint phá»¥c vá»¥ demo. Quan sÃ¡t há»‡ thá»‘ng báº±ng Amazon CloudWatch, tá»± Ä‘á»™ng hoÃ¡ buildâ€“deploy báº±ng CI/CD (GitHub Actions hoáº·c Jenkins), vÃ  káº¿t thÃºc báº±ng chiáº¿n lÆ°á»£c tá»‘i Æ°u chi phÃ­ + teardown Ä‘á»ƒ khÃ´ng phÃ¡t sinh phÃ­ ngoÃ i Ã½ muá»‘n.\n2. BÃ i toÃ¡n (Problem Statement) 2.1 Váº¥n Ä‘á» lÃ  gÃ¬? Khi Ä‘Æ°a ML vÃ o production, team thÆ°á»ng gáº·p cÃ¡c khÃ³ khÄƒn:\nPipeline dá»¯ liá»‡u khÃ´ng láº·p láº¡i Ä‘Æ°á»£c (ETL thá»§ cÃ´ng, chia táº­p khÃ´ng nháº¥t quÃ¡n) Thiáº¿u governance cho model (khÃ´ng registry, khÃ´ng approve version, khÃ³ trace) Inference khÃ³ triá»ƒn khai á»•n Ä‘á»‹nh (khÃ´ng chuáº©n hoÃ¡ container) EKS phá»©c táº¡p (networking, IAM/IRSA, image pull ECR) Thiáº¿u monitoring/logging â†’ khÃ³ debug lá»—i vÃ  khÃ³ kiá»ƒm soÃ¡t chi phÃ­ Chi phÃ­ tÄƒng nhanh náº¿u tÃ i nguyÃªn cháº¡y 24/7 (EKS nodes, ALB, logs, storage) 2.2 Giáº£i phÃ¡p Workshop triá»ƒn khai má»™t há»‡ thá»‘ng thá»‘ng nháº¥t:\nChuáº©n hoÃ¡ data lake trÃªn S3: silver/ â†’ gold/ (táº¡o train/val/test deterministically) Train RandomForest classifier trÃªn SageMaker, Ä‘áº¡t KPI (Accuracy â‰¥ 0.8, F1 â‰¥ 0.7) LÆ°u artifacts vÃ o S3 artifacts/ vÃ  quáº£n lÃ½ version qua SageMaker Model Registry (register + approve) Build container inference FastAPI theo best-practice, push lÃªn ECR (scan-on-push + lifecycle) Deploy API lÃªn EKS (multi-replica + health check + HPA), public endpoint cho demo (/health, /docs, /predict) ThÃªm monitoring báº±ng CloudWatch (Container Insights, log retention, alarms) ThÃªm CI/CD tá»± Ä‘á»™ng buildâ€“testâ€“scanâ€“pushâ€“deploy vÃ  (tuá»³ chá»n) retrain+register Tá»‘i Æ°u chi phÃ­ báº±ng Spot/schedule/lifecycle/budgets vÃ  teardown Ä‘áº§y Ä‘á»§ 2.3 Lá»£i Ã­ch / GiÃ¡ trá»‹ Triá»ƒn khai ML nhanh, láº·p láº¡i Ä‘Æ°á»£c, cÃ³ kiá»ƒm soÃ¡t phiÃªn báº£n vÃ  audit trail Äá»™ tin cáº­y cao hÆ¡n nhá» health check + autoscaling + monitoring Chi phÃ­ tháº¥p hÆ¡n nhá» Spot/schedule/lifecycle vÃ  táº¯t/bá» tÃ i nguyÃªn khi khÃ´ng dÃ¹ng Kiáº¿n trÃºc máº«u tÃ¡i sá»­ dá»¥ng cho cÃ¡c ML APIs khÃ¡c 3. Kiáº¿n trÃºc giáº£i phÃ¡p (Solution Architecture) 3.1 Kiáº¿n trÃºc tá»•ng quan (mÃ´ táº£) Táº§ng dá»¯ liá»‡u\nAmazon S3: raw/, silver/, gold/, artifacts/, logs/, tmp/ ETL tá»± Ä‘á»™ng tá»« silver partitions â†’ gold splits (train/val/test) Táº§ng training \u0026amp; governance\nAmazon SageMaker Training (RandomForest) Log metric/training qua CloudWatch Model artifacts lÆ°u S3 artifacts/ SageMaker Model Registry: Model Package Group + approve phiÃªn báº£n Táº§ng container \u0026amp; triá»ƒn khai\nAmazon ECR: mlops/retail-api (immutability/scan/lifecycle) Amazon EKS: mlops-retail-cluster (VPC production, private subnets cho nodes/pods) Public demo endpoint qua LoadBalancer/ALB (báº­t khi demo, táº¯t khi khÃ´ng dÃ¹ng) Táº§ng quan sÃ¡t \u0026amp; tá»± Ä‘á»™ng hoÃ¡\nCloudWatch: Container Insights, log groups retention, alarms CI/CD: GitHub Actions (OIDC) hoáº·c Jenkins (EC2) cho build/test/scan/push/deploy + optional retrain Cost: S3/ECR lifecycle, log retention, schedule start/stop, teardown scripts YÃªu cáº§u báº¯t buá»™c vá» region: Ä‘á»“ng bá»™ toÃ n bá»™ tÃ i nguyÃªn á»Ÿ ap-southeast-1 Ä‘á»ƒ trÃ¡nh lá»—i/redirect vÃ  phÃ¡t sinh phá»©c táº¡p (vÃ­ dá»¥ S3 301).\n3.2 AWS services sá»­ dá»¥ng Amazon S3: data lake + artifacts + logs Amazon SageMaker: training jobs + Model Registry Amazon ECR: private image registry cho inference API Amazon EKS: ná»n táº£ng cháº¡y API production-like Elastic Load Balancing (ALB/NLB): public endpoint cho demo /predict vÃ  /docs Amazon CloudWatch: logs, metrics, Container Insights, alarms AWS IAM + IRSA: cáº¥p quyá»n an toÃ n cho pod truy cáº­p S3/SageMaker/CloudWatch Amazon VPC + VPC Endpoints: private access Ä‘áº¿n S3/ECR/CloudWatch Logs (khÃ´ng cáº§n NAT) CI/CD: GitHub Actions hoáº·c Jenkins 3.3 Thiáº¿t káº¿ thÃ nh pháº§n ETL \u0026amp; Dataset Prep: táº¡o gold splits tá»« silver partitions; lÆ°u dÆ°á»›i gold/ Model Training: training job ghi artifact vÃ o artifacts/ vÃ  xuáº¥t metrics Model Governance: register model package vÃ  chá»‰ dÃ¹ng version Ä‘Ã£ approve Inference Service: FastAPI gá»i model approved tá»« registry; endpoint /predict Äá»™ tin cáº­y: probes, replicas, autoscaling HPA Báº£o máº­t: private subnets, endpoints, least-privilege IAM, scan image 4. Triá»ƒn khai ká»¹ thuáº­t (Technical Implementation) 4.1 CÃ¡c giai Ä‘oáº¡n triá»ƒn khai (mapping theo workshop tasks) Task 4 â€“ Training Pipeline (SageMaker): ETL silverâ†’gold, train RF, evaluate, artifacts S3, register/approve Model Registry Task 5 â€“ Production Networking: táº¡o Production VPC (10.0.0.0/16), private subnets cho EKS, public subnets cho ALB demo-only, VPC endpoints, khÃ´ng NAT Task 6 â€“ ECR: build FastAPI container multi-stage, non-root, healthcheck, scan-on-push, lifecycle, push scripts Task 7 â€“ EKS Setup: táº¡o cluster + add-ons + IRSA + nodegroup, deploy app máº«u vÃ  verify ECR pull Task 8 â€“ API Deployment: deploy retail API (namespace mlops), ServiceAccount gáº¯n IRSA role, LB endpoint + HPA + test Task 9 â€“ Load Balancing: nÃ¢ng cáº¥p demo endpoint báº±ng ALB/Ingress + controller (tuá»³ chá»n TLS/DNS) Task 10 â€“ Monitoring: Container Insights, log retention, alarms, Logs Insights Task 11 â€“ CI/CD: build/test/scan/push/deploy + optional retrain+register + rollback hooks Task 12 â€“ Cost \u0026amp; Teardown: Spot/schedule/lifecycle/budget + teardown script vÃ  verify xÃ³a sáº¡ch 4.2 YÃªu cáº§u ká»¹ thuáº­t AWS account cÃ³ quyá»n EKS, SageMaker, ECR, S3, VPC, CloudWatch, IAM Docker + AWS CLI (hoáº·c CloudShell) kubectl (vÃ  Helm náº¿u dÃ¹ng AWS Load Balancer Controller) Repo structure theo workshop: aws/scripts/, aws/k8s/, aws/infra/, \u0026hellip; 5. Timeline \u0026amp; Milestones Milestone 1: Data pipeline + SageMaker training + Model Registry hoáº¡t Ä‘á»™ng (model version cÃ³ approve) Milestone 2: Container FastAPI lÃªn ECR (scan/lifecycle báº­t Ä‘áº§y Ä‘á»§) Milestone 3: EKS + IRSA á»•n; API cháº¡y cÃ³ health check + HPA Milestone 4: Public demo endpoint hoáº¡t Ä‘á»™ng; monitoring/alarms xong Milestone 5: CI/CD cháº¡y end-to-end; cost controls + teardown verify ok 6. Æ¯á»›c tÃ­nh chi phÃ­ (Budget Estimation) Workshop hÆ°á»›ng tá»›i mÃ´ hÃ¬nh production-like nhÆ°ng tiáº¿t kiá»‡m:\nEKS node Æ°u tiÃªn nhá»/Spot cho demo Load balancer chá»‰ báº­t lÃºc demo SageMaker training tá»‘i Æ°u (Managed Spot Training náº¿u phÃ¹ há»£p) CloudWatch log retention 7â€“30 ngÃ y S3/ECR lifecycle Ä‘á»ƒ giáº£m chi phÃ­ storage Budget alerts Ä‘á»ƒ trÃ¡nh vÆ°á»£t ngÃ¢n sÃ¡ch (Tuá»³ chá»n: Ä‘Ã­nh kÃ¨m báº£ng chi phÃ­/áº£nh AWS Pricing Calculator hoáº·c trÃ­ch báº£ng tá»« Task 12.)\n7. ÄÃ¡nh giÃ¡ rá»§i ro (Risk Assessment) 7.1 Risk Matrix (rá»§i ro phá»• biáº¿n) VÆ°á»£t chi phÃ­: áº£nh hÆ°á»Ÿng vá»«a, xÃ¡c suáº¥t vá»«a Lá»—i networking (private subnets/endpoints): áº£nh hÆ°á»Ÿng cao, xÃ¡c suáº¥t vá»«a Sai IAM/IRSA: áº£nh hÆ°á»Ÿng cao, xÃ¡c suáº¥t vá»«a Image pull lá»—i (ECR auth/endpoint): áº£nh hÆ°á»Ÿng vá»«a, xÃ¡c suáº¥t vá»«a DÃ¹ng nháº§m version model / chÆ°a approve: áº£nh hÆ°á»Ÿng vá»«a, xÃ¡c suáº¥t tháº¥p 7.2 Giáº£m thiá»ƒu rá»§i ro Äá»“ng bá»™ region vÃ  Ä‘áº·t naming convention rÃµ rÃ ng DÃ¹ng VPC endpoints (S3 gateway, ECR API/DKR, CloudWatch Logs) Ä‘á»ƒ giáº£m phá»¥ thuá»™c NAT Test IRSA sá»›m báº±ng pod smoke-test Health checks + rollback trong CI/CD Budget alerts + teardown sau demo 8. Káº¿t quáº£ ká»³ vá»ng (Expected Outcomes) 8.1 Káº¿t quáº£ ká»¹ thuáº­t Pipeline MLOps hoÃ n chá»‰nh: S3 â†’ SageMaker Training â†’ Model Registry â†’ ECR â†’ EKS API Public demo endpoint hoáº¡t Ä‘á»™ng vá»›i autoscaling vÃ  health checks Monitoring/logs Ä‘áº§y Ä‘á»§ Ä‘á»ƒ debug vÃ  theo dÃµi váº­n hÃ nh CI/CD giÃºp triá»ƒn khai láº·p láº¡i nhanh vÃ  chuáº©n 8.2 GiÃ¡ trá»‹ dÃ i háº¡n Blueprint tÃ¡i sá»­ dá»¥ng cho cÃ¡c ML services khÃ¡c Governance rÃµ rÃ ng cho model version + artifacts Thá»±c hÃ nh váº­n hÃ nh tiáº¿t kiá»‡m chi phÃ­, phÃ¹ há»£p Ä‘á»“ Ã¡n/thá»­ nghiá»‡m production "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.1-week1/",
	"title": "Tuáº§n 1 - Giá»›i thiá»‡u vá» AWS",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 1: LÃ m quen vá»›i Háº¡ táº§ng ToÃ n cáº§u cá»§a AWS (Regions, AZs) vÃ  mÃ´ hÃ¬nh Chia sáº» trÃ¡ch nhiá»‡m (Shared Responsibility Model). ThÃ nh tháº¡o cáº¥u hÃ¬nh AWS Console \u0026amp; AWS CLI. Thá»±c hÃ nh cÃ¡c nguyÃªn táº¯c báº£o máº­t cÆ¡ báº£n (IAM User/Role, MFA). CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - Thiáº¿t láº­p: Táº¡o tÃ i khoáº£n AWS, báº­t MFA cho root user.\n- IAM: Táº¡o IAM Groups/Users theo nguyÃªn táº¯c Least Privilege. AWS IAM Docs 3 - CLI: CÃ i AWS CLI v2, cáº¥u hÃ¬nh profiles (~/.aws/config, credentials).\n- Kiá»ƒm tra: Cháº¡y aws sts get-caller-identity Ä‘á»ƒ xÃ¡c thá»±c Ä‘Äƒng nháº­p. AWS CLI Guide 4 - S3 cÆ¡ báº£n: Táº¡o bucket vá»›i Block Public Access Ä‘Æ°á»£c báº­t.\n- Thá»±c hÃ nh upload/download object qua Console \u0026amp; CLI. AWS S3 Docs 5 - Báº£o máº­t: Há»c mÃ´ hÃ¬nh Shared Responsibility Model.\n- Äá»‹nh nghÄ©a quy Æ°á»›c tagging (Owner, Project, Env). FCJ Materials 6 - Ã”n táº­p: Ghi láº¡i quy trÃ¬nh setup vÃ  kiá»ƒm tra cáº£nh bÃ¡o billing.\n- BÃ¡o cÃ¡o tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 1: Báº£o vá»‡ tÃ i khoáº£n AWS báº±ng MFA vÃ  khÃ´ng dÃ¹ng root cho cÃ¡c tÃ¡c vá»¥ háº±ng ngÃ y. Cáº¥u hÃ¬nh AWS CLI profiles cho nhiá»u mÃ´i trÆ°á»ng. Táº¡o S3 bucket an toÃ n, báº­t mÃ£ hoÃ¡ vÃ  versioning. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/3-s3-data-storage/",
	"title": "Data Pipeline Optimization",
	"tags": [],
	"description": "",
	"content": "ğŸ¯ Má»¥c tiÃªu Task 3 Táº¡o S3 bucket vÃ  tá»• chá»©c dá»¯ liá»‡u cho pipeline MLOps theo chuáº©n raw â†’ silver â†’ gold â†’ artifacts, chuyá»ƒn Ä‘á»•i CSV â†’ Parquet báº±ng AWS Glue Studio (Visual ETL) vÃ  Ä‘o benchmark hiá»‡u nÄƒng Ä‘á»c/ghi:\nÄo trÃªn AWS CloudShell. Äo trÃªn mÃ¡y local (Windows, 16GB RAM). Táº­p trung vÃ o:\nHiá»‡u nÄƒng Ä‘á»c/ghi: CSV vs Parquet. Dung lÆ°á»£ng lÆ°u trá»¯: trÆ°á»›c/sau khi nÃ©n. CÃ¡ch lÃ m: tá»«ng bÆ°á»›c cá»¥ thá»ƒ, cÃ³ thá»ƒ tÃ¡i hiá»‡n. ğŸ’¡ Task 3 â€“ S3 Storage Optimization\nâœ… Tá»‘i Æ°u format: Parquet + Snappy thay vÃ¬ CSV thuáº§n. âœ… Tá»‘i Æ°u hiá»‡u nÄƒng Ä‘á»c/ghi cho ETL \u0026amp; training. âœ… Tá»‘i Æ°u dung lÆ°á»£ng lÆ°u trá»¯ (giáº£m Ä‘Ã¡ng ká»ƒ GB). âœ… Bá»• sung benchmark thá»±c táº¿: CloudShell + local. ğŸ“¥ Input tá»« Task 2: IAM Roles \u0026amp; Audit â€” account ID, IAM roles/policies and CloudTrail/audit setup required to create buckets, Glue roles and permissions.\nğŸ”§ MÃ´i trÆ°á»ng lab thá»±c táº¿ Account ID: 842676018087 Region lab: us-east-1 Bucket: mlops-retail-prediction-dev-842676018087 Dataset chÃ­nh:\nraw/transactions.csv\nâ‰ˆ 4,593.65 MB, 33,850,823 dÃ²ng VÃ­ dá»¥ 1 file Parquet sau ETL:\nsilver/shop_week=200607/run-1761638745394-part-block-0-0-r-00000-snappy.parquet\nâ‰ˆ 458.45 MB, 33,850,823 dÃ²ng 1. Cáº¥u trÃºc \u0026amp; tá»• chá»©c S3 bucket 1.1. Cáº¥u trÃºc lÆ°u trá»¯ tá»•ng quÃ¡t Ãp dá»¥ng cho má»i account, dÃ¹ng {account-id} lÃ m placeholder:\ns3://mlops-retail-prediction-dev-{account-id}/ â”œâ”€â”€ raw/ # dá»¯ liá»‡u CSV gá»‘c, immutable â”œâ”€â”€ silver/ # dá»¯ liá»‡u Parquet Ä‘Ã£ lÃ m sáº¡ch / chuáº©n hÃ³a â”œâ”€â”€ gold/ # features, aggregated datasets cho training/serving â””â”€â”€ artifacts/ # model, metadata, logs, reports Ã nghÄ©a:\nraw/: chá»‰ append, khÃ´ng sá»­a/xÃ³a â†’ phá»¥c vá»¥ audit \u0026amp; reprocessing. silver/: nÆ¡i lÆ°u Parquet tá»‘i Æ°u (schema chuáº©n, sáº¡ch). gold/: dataset cuá»‘i cÃ¹ng cho training/inference. artifacts/: model.tar.gz, notebook export, log, benchmark CSV,â€¦ Tip: Sá»­ dá»¥ng prefix rÃµ rÃ ng vÃ  consistent (vÃ­ dá»¥ raw/, silver/, gold/) giÃºp dá»… thiáº¿t láº­p lifecycle rules, IAM policies vÃ  S3 analytics. ThÃªm ingest-date=YYYY-MM-DD/ náº¿u cáº§n track theo ngÃ y upload.\n1.2. Cáº¥u trÃºc thá»±c táº¿ trong lab Vá»›i account ID cá»§a báº¡n:\nS3 Bucket: mlops-retail-prediction-dev-842676018087 â”œâ”€â”€ raw/ â”‚ â””â”€â”€ transactions.csv # file gá»‘c ~4.59GB â”œâ”€â”€ silver/ â”‚ â”œâ”€â”€ transactions/ # output tá»« Glue ETL (náº¿u khÃ´ng partition theo week) â”‚ â””â”€â”€ shop_week=200607/ â”‚ â””â”€â”€ run-1761638745394-part-block-0-0-r-00000-snappy.parquet # ~458MB â”œâ”€â”€ gold/ â”‚ â””â”€â”€ (dÃ nh cho feature store / aggregated tables) â””â”€â”€ artifacts/ â””â”€â”€ (lÆ°u wyniki benchmark, model, logs,â€¦) Báº¡n cÃ³ thá»ƒ má»Ÿ S3 Console Ä‘á»ƒ xÃ¡c nháº­n Ä‘Ãºng Ä‘Æ°á»ng dáº«n, nháº¥t lÃ :\nraw/transactions.csv Má»™t file Parquet tiÃªu biá»ƒu trong silver/shop_week=.../. 2. Táº¡o bucket \u0026amp; thÆ° má»¥c trÃªn AWS Console 2.1. Táº¡o S3 Bucket VÃ o AWS Console â†’ S3 â†’ Create bucket. Cáº¥u hÃ¬nh: Bucket name: mlops-retail-prediction-dev-842676018087 Region: us-east-1 Block all public access: âœ… Enabled Versioning: (khuyáº¿n nghá»‹) Enabled Default encryption: âœ… SSE-S3 2.2. Táº¡o 4 thÆ° má»¥c chÃ­nh Trong S3 Console:\nMá»Ÿ bucket mlops-retail-prediction-dev-842676018087. Create folder láº§n lÆ°á»£t: raw/ silver/ gold/ artifacts/ Warning: TrÃ¡nh Ä‘á»•i tÃªn bucket sau khi deploy â€” tÃªn bucket lÃ  global vÃ  thay Ä‘á»•i sáº½ áº£nh hÆ°á»Ÿng tá»›i má»i pipeline, IAM policy, vÃ  config. LuÃ´n báº­t Block all public access trá»« khi cÃ³ lÃ½ do cá»¥ thá»ƒ vÃ  Ä‘Æ°á»£c kiá»ƒm duyá»‡t.\n3. Báº­t Intelligent-Tiering (tá»‘i Æ°u chi phÃ­) Má»¥c Ä‘Ã­ch: dá»¯ liá»‡u Ã­t truy cáº­p (vÃ­ dá»¥ raw/ cÅ©, artifacts/ log cÅ©) Ä‘Æ°á»£c chuyá»ƒn tá»± Ä‘á»™ng sang lá»›p lÆ°u trá»¯ ráº» hÆ¡n, khÃ´ng Ä‘á»•i URL.\nCÃ¡c bÆ°á»›c:\nVÃ o bucket â†’ tab Properties. TÃ¬m pháº§n Intelligent-Tiering archive configurations â†’ Edit. ThÃªm cáº¥u hÃ¬nh: Configuration name: storage-optimization Status: Enabled Scope: Entire bucket (hoáº·c prefix cá»¥ thá»ƒ: raw/, silver/, gold/, artifacts/) 4. Chuyá»ƒn CSV â†’ Parquet báº±ng AWS Glue Studio (Visual ETL) 4.1. Upload transactions.csv vÃ o raw/ TrÃªn S3 Console:\nMá»Ÿ bucket â†’ folder raw/. Upload â†’ Add files â†’ chá»n file transactions.csv trÃªn mÃ¡y. Upload. 4.2. Táº¡o Glue Job (Visual ETL) VÃ o AWS Glue Studio â†’ Jobs â†’ Create job â†’ Visual with a blank canvas. Äáº·t tÃªn: Job name: csv-to-parquet-converter Chá»n/ táº¡o IAM Role cÃ³ quyá»n: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::mlops-retail-prediction-dev-842676018087/raw/*\u0026#34;, \u0026#34;arn:aws:s3:::mlops-retail-prediction-dev-842676018087/silver/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:*\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 4.3. Source node â€“ Ä‘á»c CSV tá»« S3 Trong canvas Glue Studio:\nThÃªm S3 Source. Cáº¥u hÃ¬nh: Data source: S3 Format: CSV S3 URL: s3://mlops-retail-prediction-dev-842676018087/raw/transactions.csv First row as header: Enabled Delimiter: , TÃ³m táº¯t:\nField Value S3 bucket mlops-retail-prediction-dev-842676018087 Path raw/transactions.csv Format CSV Header Yes Delimiter , 4.4. Transform â€“ ApplyMapping (tá»‘i Æ°u schema) ThÃªm node ApplyMapping. Káº¿t ná»‘i Source â†’ ApplyMapping. Mapping kiá»ƒu dá»¯ liá»‡u (vÃ­ dá»¥): Column Source type Target type Ghi chÃº SHOP_WEEK long int int32 lÃ  Ä‘á»§ SHOP_HOUR long tinyint 0â€“23 QUANTITY long smallint Sá»‘ lÆ°á»£ng STORE_CODE string string Giá»¯ nguyÃªn SPEND decimal decimal(10,2) Tiá»n tá»‡, 2 sá»‘ láº» BASKET_TYPE string string Categorical Lá»£i Ã­ch:\nGiáº£m kÃ­ch thÆ°á»›c file Parquet. Tá»‘i Æ°u scan \u0026amp; aggregation. Giáº£m RAM khi Ä‘á»c dá»¯ liá»‡u. 4.5. Target â€“ ghi Parquet (Snappy) ra silver/ ThÃªm node S3 Target. Káº¿t ná»‘i ApplyMapping â†’ Target. Cáº¥u hÃ¬nh: Data target: S3 Format: Parquet Compression: Snappy S3 path: s3://mlops-retail-prediction-dev-842676018087/silver/transactions/ Partition keys: SHOP_WEEK (khuyáº¿n nghá»‹) Minh há»a:\n\u0026ndash; Target config: /imagess3-data-storage/target-config.png \u0026ndash; ToÃ n pipeline: /imagess3-data-storage/04-glue-etl.png\nSave \u0026amp; Run job â†’ theo dÃµi Job run details â†’ kiá»ƒm tra output trong silver/. Info: Khi partitioning, cÃ¢n báº±ng giá»¯a sá»‘ lÆ°á»£ng partition vÃ  kÃ­ch thÆ°á»›c file â€” quÃ¡ nhiá»u file nhá» (small files) sáº½ lÃ m cháº­m query; cÃ¢n nháº¯c cháº¡y bÆ°á»›c compaction (Glue/Athena/EMR) Ä‘á»ƒ gá»™p files lá»›n hÆ¡n (e.g., 128â€“512 MB má»—i file) náº¿u cáº§n.\nTip: Chá»n Snappy cho balance giá»¯a compression ratio vÃ  CPU usage. Náº¿u cáº§n tiáº¿t kiá»‡m I/O nhiá»u hÆ¡n, thá»­ ZSTD (náº¿u stack cá»§a báº¡n há»— trá»£) cho ratio tá»‘t hÆ¡n nhÆ°ng cáº§n kiá»ƒm tra chi phÃ­ CPU.\n5. Benchmark thá»±c táº¿ trÃªn AWS CloudShell (Ä‘á»c trá»±c tiáº¿p tá»« S3) 5.1. ThÃ´ng tin dataset \u0026amp; cÃ¡ch cháº¡y Cháº¡y trÃªn AWS CloudShell.\nÄá»c trá»±c tiáº¿p:\nraw/transactions.csv (~4,593.65 MB, 33,850,823 rows). 1 file Parquet (~458.45 MB, 33,850,823 rows). Báº¡n Ä‘Ã£ dÃ¹ng script kiá»ƒu:\nread_csv_s3(...) Ä‘á»ƒ Ä‘o Ä‘á»c CSV. read_parquet_s3(...) Ä‘á»ƒ Ä‘o Ä‘á»c Parquet. Log chi tiáº¿t Ä‘Ã£ hiá»‡n trong CloudShell.)\nInfo: CloudShell cÃ³ giá»›i háº¡n tÃ i nguyÃªn (vCPU/RAM/IO) â€” measured timings cÃ³ thá»ƒ khÃ¡c vá»›i EC2/Glue worker sizes. Khi so sÃ¡nh, ghi rÃµ kÃ­ch thÆ°á»›c instance/worker Ä‘á»ƒ tÃ¡i láº­p káº¿t quáº£ chÃ­nh xÃ¡c.\nKáº¿t quáº£ Ä‘o Ä‘á»c CSV: Káº¿t quáº£ Ä‘o Ä‘á»c Parquet: 5.2. Káº¿t quáº£ Ä‘o (CloudShell) CSV â€“ Ä‘á»c toÃ n bá»™ raw/transactions.csv tá»« S3\n5 láº§n Ä‘o:\n151.91s, 146.34s, 141.52s, 126.03s, 115.95s TÃ­nh trung bÃ¬nh (xáº¥p xá»‰):\nAvg time â‰ˆ 136.35 s Size = 4,593.65 MB Avg throughput â‰ˆ 33.7 MB/s Rows/s â‰ˆ ~248k rows/s Parquet â€“ Ä‘á»c 1 file ~458.45 MB tá»« S3\n5 láº§n Ä‘o:\n61.37s, 53.65s, 52.51s, 49.66s, 49.55s TÃ­nh trung bÃ¬nh (xáº¥p xá»‰):\nAvg time â‰ˆ 53.35 s Size = 458.45 MB Avg throughput â‰ˆ 8.6 MB/s Rows/s â‰ˆ ~635k rows/s 5.3. Báº£ng so sÃ¡nh (CloudShell) Loáº¡i Size trÃªn S3 Avg time (s) Avg throughput (MB/s) Rows Rows/s (xáº¥p xá»‰) Relative rows/s CSV 4,593.65 MB 136.35 33.7 33,850,823 ~248k 1Ã— Parquet 458.45 MB 53.35 8.6 33,850,823 ~635k ~2.6Ã— Giáº£i thÃ­ch:\nTheo MB/s, CSV cÃ³ váº» â€œnhanhâ€ hÆ¡n vÃ¬ má»—i run xá»­ lÃ½ nhiá»u MB hÆ¡n (4.59 GB). NhÆ°ng xÃ©t sá»‘ dÃ²ng/giÃ¢y (rows/s), Parquet nhanh hÆ¡n ~2.6Ã—, phÃ¹ há»£p cho ETL / training. Káº¿t luáº­n CloudShell\nParquet (Snappy) giáº£m máº¡nh dung lÆ°á»£ng: 4.59 GB â†’ ~0.46 GB. Vá»›i cÃ¹ng 33.85M dÃ²ng, Parquet xá»­ lÃ½ nhanh hÆ¡n ~2.6Ã— vá» rows/s. 6. Benchmark trÃªn mÃ¡y local 6.1. Chuáº©n bá»‹ thÆ° má»¥c \u0026amp; táº£i dá»¯ liá»‡u TrÃªn Windows:\nmkdir s3-local-benchmark cd s3-local-benchmark Táº£i 2 file:\naws s3 cp s3://mlops-retail-prediction-dev-842676018087/raw/transactions.csv ./transactions.csv aws s3 cp s3://mlops-retail-prediction-dev-842676018087/silver/shop_week=200607/run-1761638745394-part-block-0-0-r-00000-snappy.parquet ./transactions_200607.parquet 6.2. Script benchmark Táº¡o file local_benchmark.py:\nimport time import os import pandas as pd def bench_csv_stream(path: str, runs: int = 3): print(f\u0026#34;=== Benchmark CSV (streaming): {path} ===\u0026#34;) size_mb = os.path.getsize(path) / (1024 * 1024) for i in range(1, runs + 1): t0 = time.time() rows = 0 # Äá»c theo chunks Ä‘á»ƒ trÃ¡nh trÃ n RAM for chunk in pd.read_csv(path, chunksize=500_000): rows += len(chunk) t1 = time.time() elapsed = t1 - t0 throughput = size_mb / elapsed print(f\u0026#34;[local_csv_stream] run={i} time={elapsed:.2f}s, \u0026#34; f\u0026#34;size={size_mb:.2f} MB, throughput={throughput:.2f} MB/s, rows={rows}\u0026#34;) def bench_parquet_stream(path: str, runs: int = 3): print(f\u0026#34; === Benchmark Parquet (streaming): {path} ===\u0026#34;) size_mb = os.path.getsize(path) / (1024 * 1024) for i in range(1, runs + 1): t0 = time.time() df = pd.read_parquet(path) rows = len(df) t1 = time.time() elapsed = t1 - t0 throughput = size_mb / elapsed print(f\u0026#34;[local_parquet_stream] run={i} time={elapsed:.2f}s, \u0026#34; f\u0026#34;size={size_mb:.2f} MB, throughput={throughput:.2f} MB/s, rows={rows}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: bench_csv_stream(\u0026#34;transactions.csv\u0026#34;, runs=3) bench_parquet_stream(\u0026#34;transactions_200607.parquet\u0026#34;, runs=3) Cháº¡y:\npython local_benchmark.py 6.3. Log thá»±c táº¿ Nháº­n xÃ©t:\nCSV full 4.59 GB: váº«n xá»­ lÃ½ Ä‘Æ°á»£c nhá» Ä‘á»c theo chunks, throughput ~90â€“95 MB/s. Parquet (sample 1 tuáº§n, 6.48 MB): thá»i gian Ä‘á»c ~0.05â€“0.09s â†’ latency cá»±c tháº¥p. Vá»›i nhiá»u file Parquet nhá» (partition theo shop_week), query theo tuáº§n/thÃ¡ng sáº½ ráº¥t nhanh. Warning (Local): TrÃªn mÃ¡y local, viá»‡c Ä‘á»c Parquet toÃ n bá»™ dataset vÃ o memory (pd.read_parquet) cÃ³ thá»ƒ gÃ¢y thiáº¿u RAM. Sá»­ dá»¥ng Ä‘á»c theo pieces (pyarrow dataset, iter_batches) hoáº·c tÄƒng chunksize khi dÃ¹ng CSV Ä‘á»ƒ trÃ¡nh OOM.\nTip (Local): Khi benchmark trÃªn Windows, táº¯t cÃ¡c chÆ°Æ¡ng trÃ¬nh náº·ng khÃ¡c vÃ  káº¿t quáº£ IO cÃ³ thá»ƒ khÃ¡c giá»¯a á»• HDD vÃ  SSD â€” lÆ°u Ã½ nÃªu loáº¡i á»• (SSD/HDD) trong bÃ¡o cÃ¡o benchmark.\n7. IAM â€“ Quyá»n tá»‘i thiá»ƒu cho Glue Job (tÃ³m táº¯t) Tá»‘i thiá»ƒu cáº§n:\nS3: s3:GetObject cho raw/* s3:PutObject cho silver/* Glue: Quyá»n táº¡o/cháº¡y job, Ä‘á»c metadata (tÃ¹y mÃ´i trÆ°á»ng). CloudWatch Logs: ghi log job. VÃ­ dá»¥ policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::mlops-retail-prediction-dev-842676018087/raw/*\u0026#34;, \u0026#34;arn:aws:s3:::mlops-retail-prediction-dev-842676018087/silver/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;glue:*\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 8. Tá»•ng káº¿t Task 3 â€“ S3 Data Storage Vá» kiáº¿n trÃºc:\nThiáº¿t káº¿ bucket theo chuáº©n MLOps:\nraw/ â†’ silver/ â†’ gold/ â†’ artifacts/ Duy trÃ¬ raw/ immutable.\nChuáº©n hÃ³a dá»¯ liá»‡u vÃ o Parquet (Snappy) trong silver/.\nVá» hiá»‡u nÄƒng (tá»« sá»‘ Ä‘o thá»±c táº¿ cá»§a báº¡n):\n4.59 GB CSV â†’ ~0.46 GB Parquet cho cÃ¹ng 33.85M dÃ²ng.\nTrÃªn CloudShell:\nCSV: ~136s, ~248k rows/s. Parquet: ~53s, ~635k rows/s â†’ ~2.6Ã— rows/s. TrÃªn mÃ¡y local (16GB RAM):\nCSV 4.59 GB váº«n xá»­ lÃ½ Ä‘Æ°á»£c vá»›i chunk 500k rows. Parquet sample 1 tuáº§n (~6.48 MB) Ä‘á»c trong ~0.05â€“0.09s. Vá» cost \u0026amp; váº­n hÃ nh:\nParquet + Snappy giáº£m Ä‘Ã¡ng ká»ƒ dung lÆ°á»£ng â†’ giáº£m tiá»n S3. Intelligent-Tiering giÃºp tá»± Ä‘á»™ng háº¡ táº§ng lá»›p lÆ°u trá»¯ cho dá»¯ liá»‡u cÅ©. Glue Visual ETL giÃºp khÃ´ng cáº§n code nhiá»u, dá»… show trong bÃ¡o cÃ¡o. 9. Clean Up Resources (AWS CLI) 9.1. XÃ³a táº¥t cáº£ objects trong S3 bucket # XÃ³a táº¥t cáº£ files trong bucket aws s3 rm s3://mlops-retail-prediction-dev-842676018087 --recursive # Kiá»ƒm tra bucket Ä‘Ã£ trá»‘ng aws s3 ls s3://mlops-retail-prediction-dev-842676018087 --recursive 9.2. XÃ³a S3 bucket # XÃ³a bucket (chá»‰ khi Ä‘Ã£ trá»‘ng) aws s3 rb s3://mlops-retail-prediction-dev-842676018087 # Kiá»ƒm tra bucket Ä‘Ã£ bá»‹ xÃ³a aws s3 ls | grep mlops-retail-prediction-dev 9.3. XÃ³a Glue Job # Liá»‡t kÃª Glue jobs aws glue get-jobs --query \u0026#39;Jobs[?contains(Name, `csv-to-parquet`)].Name\u0026#39; # XÃ³a Glue job aws glue delete-job --job-name csv-to-parquet-converter # Kiá»ƒm tra job Ä‘Ã£ bá»‹ xÃ³a aws glue get-job --job-name csv-to-parquet-converter 9.4. XÃ³a IAM Role (náº¿u táº¡o riÃªng cho Glue) # Detach policies khá»i role aws iam detach-role-policy --role-name GlueETLRole --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole # XÃ³a inline policies (náº¿u cÃ³) aws iam delete-role-policy --role-name GlueETLRole --policy-name S3AccessPolicy # XÃ³a role aws iam delete-role --role-name GlueETLRole Success tip: Náº¿u báº¡n xÃ³a tÃ i nguyÃªn Ä‘á»ƒ trÃ¡nh chi phÃ­, kiá»ƒm tra CloudWatch log groups vÃ  Athena query history â€” má»™t sá»‘ log hoáº·c query history cÃ³ thá»ƒ váº«n lÆ°u trá»¯ metadata vÃ  gÃ¢y chi phÃ­ nhá» náº¿u khÃ´ng dá»n dáº¹p.\n10. Báº£ng giÃ¡ S3 Storage (ap-southeast-1) 10.1. Chi phÃ­ lÆ°u trá»¯ theo class Storage Class GiÃ¡ (USD/GB/thÃ¡ng) Minimum Duration Ghi chÃº S3 Standard $0.025 None Frequent access S3 Standard-IA $0.0138 30 days Infrequent access S3 One Zone-IA $0.011 30 days Single AZ S3 Glacier Instant $0.005 90 days Archive, instant retrieval S3 Glacier Flexible $0.0045 90 days Archive, 1-12 hours retrieval S3 Deep Archive $0.002 180 days Long-term archive, 12+ hours 10.2. Chi phÃ­ requests Request Type GiÃ¡ (USD/1000 requests) Ghi chÃº PUT/POST/LIST $0.0055 Write operations GET/SELECT $0.00044 Read operations Data Transfer OUT $0.12/GB First 1GB free/month 10.3. Æ¯á»›c tÃ­nh chi phÃ­ cho project Dá»¯ liá»‡u hiá»‡n táº¡i:\nRaw CSV: 4.59 GB Silver Parquet: 0.46 GB Tá»•ng: ~5 GB Chi phÃ­ hÃ ng thÃ¡ng (S3 Standard):\nComponent Size Price/GB Monthly Cost Raw data (CSV) 4.59 GB $0.025 $0.11 Silver data (Parquet) 0.46 GB $0.025 $0.01 Gold + artifacts ~0.5 GB $0.025 $0.01 Total Storage ~5.5 GB $0.14 Requests (Æ°á»›c tÃ­nh) ~1000 req $0.0055 $0.006 Grand Total â‰ˆ $0.15/month Vá»›i Intelligent Tiering:\nSau 30 ngÃ y: Raw data chuyá»ƒn Standard-IA â†’ tiáº¿t kiá»‡m ~45% Sau 90 ngÃ y: Old artifacts chuyá»ƒn Glacier â†’ tiáº¿t kiá»‡m ~80% Æ¯á»›c tÃ­nh tiáº¿t kiá»‡m: ~$0.05-0.08/month ğŸ’° Chi phÃ­ Storage tá»‘i Æ°u\nHiá»‡n táº¡i: ~$0.15/month cho 5.5GB Vá»›i Intelligent Tiering: ~$0.07-0.10/month Parquet format: Giáº£m 90% dung lÆ°á»£ng so vá»›i CSV ğŸ¯ Task 3 hoÃ n thÃ nh\nKiáº¿n trÃºc S3 rÃµ rÃ ng, chuáº©n MLOps. CSV â†’ Parquet báº±ng Glue Studio (Visual, cÃ³ hÃ¬nh minh há»a). CÃ³ benchmark thá»±c táº¿ trÃªn CloudShell vÃ  local, cÃ³ sá»‘ liá»‡u cá»¥ thá»ƒ. Clean up commands vÃ  pricing breakdown chi tiáº¿t. Dá»… trÃ¬nh bÃ y trong bÃ¡o cÃ¡o \u0026amp; demo cho GV. ğŸ“¹ Video thá»±c hiá»‡n Task 3 Next Step: Task 4: SageMaker Training\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/3-blogstranslated/",
	"title": "CÃ¡c bÃ i blogs Ä‘Ã£ dá»‹ch",
	"tags": [],
	"description": "",
	"content": "Blog 1 - CÃ´ng bá»‘ cÃ¡c Äá»‘i tÃ¡c AWS cá»§a NÄƒm 2025 khu vá»±c chÃ¢u Ã‚u, Trung ÄÃ´ng vÃ  chÃ¢u Phi Blog cÃ´ng bá»‘ cÃ¡c Äá»‘i tÃ¡c AWS cá»§a NÄƒm 2025 cho khu vá»±c chÃ¢u Ã‚u, Trung ÄÃ´ng vÃ  chÃ¢u Phi (EMEA). Nhá»¯ng giáº£i thÆ°á»Ÿng hÃ ng nÄƒm nÃ y ghi nháº­n cÃ¡c thÃ nh viÃªn cá»§a Máº¡ng lÆ°á»›i Äá»‘i tÃ¡c AWS (APN) luÃ´n mang láº¡i káº¿t quáº£ xuáº¥t sáº¯c vÃ  thÃºc Ä‘áº©y Ä‘á»•i má»›i cho khÃ¡ch hÃ ng.\nBlog 2 - CÃ¡ch tÃ¹y chá»‰nh pháº£n há»“i Ä‘á»‘i vá»›i cÃ¡c cuá»™c táº¥n cÃ´ng DDoS táº§ng 7 báº±ng AWS WAF Anti-DDoS AMR Blog giáº£i thÃ­ch cÃ¡ch AWS WAF Anti-DDoS AWS Managed Rules (Anti-DDoS AMR) hoáº¡t Ä‘á»™ng vÃ  cÃ¡ch báº¡n cÃ³ thá»ƒ tÃ¹y chá»‰nh hÃ nh vi cá»§a nÃ³ báº±ng nhÃ£n vÃ  cÃ¡c quy táº¯c AWS WAF bá»• sung. HÆ°á»›ng dáº«n qua ba tÃ¬nh huá»‘ng thá»±c táº¿ vá»›i cÃ¡c ká»¹ thuáº­t tÃ¹y chá»‰nh khÃ¡c nhau.\nBlog 3 - XÃ¢y dá»±ng AI agents cho Amazon Bedrock sá»­ dá»¥ng AWS CDK vÃ  triá»ƒn khai vá»›i GitHub Actions Blog hÆ°á»›ng dáº«n cÃ¡ch xÃ¢y dá»±ng AI agents cho Amazon Bedrock sá»­ dá»¥ng AWS Cloud Development Kit (CDK) vÃ  triá»ƒn khai báº±ng GitHub Actions. Cung cáº¥p giáº£i phÃ¡p end-to-end hoÃ n chá»‰nh Ä‘á»ƒ táº¡o, test vÃ  deploy AI agents trong mÃ´i trÆ°á»ng production.\nBlog 4 - TÃ­ch há»£p dá»¯ liá»‡u S\u0026amp;P Global má»Ÿ rá»™ng kháº£ nÄƒng Amazon QuickSight Research Blog thÃ´ng bÃ¡o tÃ­ch há»£p má»›i giá»¯a Amazon QuickSight Research vÃ  S\u0026amp;P Global, káº¿t há»£p tin tá»©c nÄƒng lÆ°á»£ng toÃ n cáº§u, nghiÃªn cá»©u vÃ  thÃ´ng tin chi tiáº¿t vá»›i kháº£ nÄƒng thá»‹ trÆ°á»ng toÃ n cáº§u Ä‘á»ƒ cung cáº¥p cho khÃ¡ch hÃ ng QuickSight Research má»™t Ä‘áº¡i lÃ½ nghiÃªn cá»©u sÃ¢u.\nBlog 5 - CÃ¡ch VÄƒn phÃ²ng Tá»•ng chÆ°á»Ÿng lÃ½ Quáº­n Contra Costa hiá»‡n Ä‘áº¡i hÃ³a quy trÃ¬nh xá»­ lÃ½ trÃ¡t Ä‘Ã²i háº§u tÃ²a vá»›i AWS vÃ  CC Tech Digital Blog mÃ´ táº£ cÃ¡ch VÄƒn phÃ²ng Tá»•ng chÆ°á»Ÿng lÃ½ Quáº­n Contra Costa há»£p tÃ¡c vá»›i AWS vÃ  CC Tech Digital Ä‘á»ƒ hiá»‡n Ä‘áº¡i hÃ³a quy trÃ¬nh xá»­ lÃ½ trÃ¡t Ä‘Ã²i háº§u tÃ²a báº±ng giáº£i phÃ¡p serverless, cloud-native, quáº£n lÃ½ hÆ¡n 17.000 trÃ¡t Ä‘Ã²i háº§u tÃ²a hÃ ng nÄƒm vá»›i tá»‘c Ä‘á»™, Ä‘á»™ chÃ­nh xÃ¡c vÃ  tuÃ¢n thá»§ Ä‘Æ°á»£c cáº£i thiá»‡n.\nBlog 6 - Há»£p lÃ½ hÃ³a sá»± há»£p tÃ¡c giá»¯a cÃ¡c Sá»Ÿ Giao thÃ´ng Váº­n táº£i vá»›i Private Blockchain Blog tháº£o luáº­n vá» cÃ¡ch blockchain cÃ³ thá»ƒ há»£p lÃ½ hÃ³a viá»‡c cáº¥p giáº¥y phÃ©p lÃ¡i xe vÃ  thÃºc Ä‘áº©y sá»± há»£p tÃ¡c sÃ¢u sáº¯c hÆ¡n giá»¯a cÃ¡c DMV á»Ÿ táº¥t cáº£ 50 bang, giáº£i thÃ­ch táº¡i sao blockchain lÃ  lá»±a chá»n cÃ´ng nghá»‡ háº¥p dáº«n Ä‘á»ƒ cáº£i thiá»‡n mÃ´ hÃ¬nh tÆ°Æ¡ng há»— hiá»‡n táº¡i.\nBlog 7 - Giá»›i thiá»‡u AWS Infrastructure as Code MCP Server: Há»— trá»£ CDK vÃ  CloudFormation Ä‘Æ°á»£c há»— trá»£ bá»Ÿi AI Blog giá»›i thiá»‡u AWS IaC MCP Server (chuáº©n MCP) giÃºp AI assistant há»— trá»£ tra cá»©u tÃ i liá»‡u CDK/CloudFormation, validate template, kiá»ƒm tra tuÃ¢n thá»§, vÃ  troubleshoot lá»—i triá»ƒn khai (kÃ¨m CloudTrail), vá»›i trá»ng tÃ¢m cháº¡y cá»¥c bá»™ Ä‘á»ƒ Ä‘áº£m báº£o báº£o máº­t.\nBlog 8 - AWS Clean Rooms ra máº¯t tÃ­nh nÄƒng táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh ML Blog cÃ´ng bá»‘ kháº£ nÄƒng táº¡o synthetic dataset trong AWS Clean Rooms Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh ML (há»“i quy/phÃ¢n loáº¡i) mÃ  váº«n báº£o vá»‡ quyá»n riÃªng tÆ°: giá»¯ Ä‘áº·c trÆ°ng thá»‘ng kÃª, giáº£m rá»§i ro tÃ¡i nháº­n diá»‡n, cÃ³ tham sá»‘ quyá»n riÃªng tÆ° vÃ  cÃ¡c chá»‰ sá»‘ cháº¥t lÆ°á»£ng (fidelity/privacy).\nBlog 9 - AWS Partner Central nay Ä‘Ã£ cÃ³ sáºµn trong AWS Management Console Blog trÃ¬nh bÃ y tráº£i nghiá»‡m Partner Central tÃ­ch há»£p trá»±c tiáº¿p trong AWS Console, táº¡o hÃ nh trÃ¬nh thá»‘ng nháº¥t tá»« khÃ¡ch hÃ ng AWS â†’ Partner â†’ Marketplace Seller, kÃ¨m cÃ¡c khu vá»±c chÃ­nh (Solutions, Profile, Opportunities, Partner connections), ná»n táº£ng Ä‘á»‹nh danh dá»±a trÃªn IAM/IAM Identity Center vÃ  vai trÃ² cá»§a APIs.\nBlog 10 - CÃ´ng bá»‘ AWS Partner Central trong AWS Management Console Blog thÃ´ng bÃ¡o Partner Central cÃ³ sáºµn trong AWS Console, nháº¥n máº¡nh kháº£ nÄƒng tÃ­ch há»£p \u0026amp; tá»± Ä‘á»™ng hÃ³a qua APIs (Account/Connections, Solution, Benefits, Selling/Leads), hÆ°á»›ng dáº«n dÃ¹ng Amazon Q chat, quáº£n lÃ½ truy cáº­p qua IAM/SSO, vÃ  má»Ÿ rá»™ng khÃ¡m phÃ¡ giáº£i phÃ¡p trong AWS Marketplace; kÃ¨m cÃ¡c bÆ°á»›c báº¯t Ä‘áº§u/migration.\nBlog 11 - CÃ´ng bá»‘ Amazon EKS Capabilities cho Ä‘iá»u phá»‘i workload vÃ  quáº£n lÃ½ tÃ i nguyÃªn Ä‘Ã¡m mÃ¢y Blog giá»›i thiá»‡u Amazon EKS Capabilities (fully managed) giÃºp Ä‘Æ¡n giáº£n hÃ³a Ä‘iá»u phá»‘i workload vÃ  quáº£n lÃ½ tÃ i nguyÃªn AWS/Kubernetes, vá»›i cÃ¡c capability táº¡i thá»i Ä‘iá»ƒm ra máº¯t nhÆ° Argo CD, ACK, vÃ  KRO, cÃ¹ng cÃ¡ch báº­t/tÃ¹y chá»‰nh vÃ  cÃ¡c lÆ°u Ã½ vá» quyá»n, nÃ¢ng cáº¥p, adoption.\nBlog 12 - ÄÆ¡n giáº£n hÃ³a viá»‡c táº¡o chÃ­nh sÃ¡ch IAM vá»›i IAM Policy Autopilot, má»™t MCP server mÃ£ nguá»“n má»Ÿ má»›i dÃ nh cho builders Blog giá»›i thiá»‡u IAM Policy Autopilot (MCP server mÃ£ nguá»“n má»Ÿ) phÃ¢n tÃ­ch code Ä‘á»ƒ táº¡o IAM identity-based policies há»£p lá»‡, há»— trá»£ tÃ­ch há»£p vá»›i cÃ¡c AI coding assistants, cáº­p nháº­t quyá»n theo thay Ä‘á»•i code, vÃ  nháº¥n máº¡nh viá»‡c review/tinh chá»‰nh theo nguyÃªn táº¯c least privilege trÆ°á»›c khi triá»ƒn khai.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.2-week2/",
	"title": "Tuáº§n 2 - Máº¡ng (Networking) trÃªn AWS",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 2: Thiáº¿t káº¿ vÃ  triá»ƒn khai Virtual Private Cloud (VPC). Cáº¥u hÃ¬nh Ä‘á»‹nh tuyáº¿n (routing), cÃ¡c gateway (IGW, NAT) vÃ  lá»›p báº£o máº­t (Security Group, NACL). CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - Thiáº¿t káº¿ VPC: Láº­p káº¿ hoáº¡ch CIDR cho 2 AZ (Public/Private Subnets).\n- Táº¡o VPC, Subnets vÃ  Route Tables. AWS VPC Docs 3 - Káº¿t ná»‘i: Gáº¯n Internet Gateway (IGW) cho public subnets.\n- Triá»ƒn khai NAT Gateway Ä‘á»ƒ private subnet truy cáº­p Internet (egress). AWS Workshop 4 - Báº£o máº­t: Cáº¥u hÃ¬nh Security Groups (stateful) vs NACLs (stateless).\n- Ãp dá»¥ng mÃ´ hÃ¬nh Bastion Host. FCJ Materials 5 - Truy cáº­p: SSH vÃ o private EC2 thÃ´ng qua Bastion hoáº·c Session Manager.\n- Tá»‘i Æ°u: Thiáº¿t láº­p VPC Endpoints cho S3/DynamoDB. AWS Systems Manager 6 - XÃ¡c minh: Kiá»ƒm tra káº¿t ná»‘i (Public -\u0026gt; Internet, Private -\u0026gt; NAT).\n- Tá»•ng káº¿t tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 2: Triá»ƒn kha "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/4-eventparticipated/",
	"title": "CÃ¡c events Ä‘Ã£ tham gia",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/4-sagemaker-training/",
	"title": "End-to-End Model Training Pipeline",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Task 4 Huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»± bÃ¡o BASKET_PRICE_SENSITIVITY (Low/Medium/High) báº±ng Amazon SageMaker vá»›i pipeline tá»± Ä‘á»™ng ETL â†’ Training â†’ Model Registry.\nâ†’ TrÃ¡i tim cá»§a MLOps pipeline - tá»« dá»¯ liá»‡u thÃ´ Ä‘áº¿n model production-ready.\nInput\nAWS Account vá»›i quyá»n SageMaker/S3/CloudWatch S3 bucket vá»›i dá»¯ liá»‡u (tá»« Task 3) IAM Role SageMaker (tá»« Task 2) Káº¿t quáº£\nModel Ä‘áº¡t accuracy â‰¥ 80%, F1 â‰¥ 0.7 Model Ä‘Æ°á»£c Ä‘Äƒng kÃ½ trong Model Registry Artifacts lÆ°u trá»¯ trong S3 Chi phÃ­: ~$0.3-0.5/job (ml.m5.large, 10-15 phÃºt)\nğŸ’¡ Task 4 - MLOps Core Pipeline:\nETL tá»± Ä‘á»™ng - Raw data â†’ Features Model training - Random Forest classifier Model evaluation - Accuracy, F1, Confusion Matrix Model Registry - Version control vÃ  approval 1. Chuáº©n bá»‹ mÃ´i trÆ°á»ng vÃ  kiá»ƒm tra prerequisites 1.1. Kiá»ƒm tra S3 bucket (tá»« Task 3) AWS Console â†’ S3:\nTÃ¬m bucket: mlops-retail-prediction-dev-[account-id]\nKiá»ƒm tra cáº¥u trÃºc thá»±c táº¿:\nraw/ # transactions.csv + _select/ folder\rsilver/ # shop_week partitions (200607-200619) gold/ # features Ä‘Ã£ xá»­ lÃ½ (sáº½ táº¡o tá»« silver/)\rartifacts/ # model outputs (sáº½ táº¡o) 1.2. XÃ¡c minh IAM Role (tá»« Task 2) AWS Console â†’ IAM â†’ Roles:\nTÃ¬m role: mlops-retail-prediction-dev-sagemaker-execution Kiá»ƒm tra permissions: âœ… AmazonSageMakerFullAccess âœ… AmazonS3FullAccess âœ… CloudWatchLogsFullAccess Warning: Náº¿u bucket cá»§a báº¡n dÃ¹ng SSE-KMS, role cáº§n cÃ³ quyá»n decrypt/encrypt vá»›i KMS key; náº¿u dÃ¹ng cross-account S3, kiá»ƒm tra thÃªm trust policy.\n2. Táº¡o SageMaker Domain vÃ  Project 2.1. Truy cáº­p SageMaker Unified Studio AWS Console â†’ SageMaker:\nTruy cáº­p URL: https://[domain-id].studio.sagemaker.[region].amazonaws.com Hoáº·c tá»« SageMaker Console â†’ Studio â†’ Open Studio Chá»n authentication method: Sign in with SSO (náº¿u cÃ³ setup SSO) Sign in with AWS IAM (dÃ¹ng IAM user/role) Sau khi Ä‘Äƒng nháº­p, báº¡n sáº½ tháº¥y giao diá»‡n SageMaker Unified Studio Dashboard hiá»ƒn thá»‹: \u0026ldquo;Good morning\u0026rdquo; greeting Search bar: \u0026ldquo;Search for data products, assets, and projects\u0026rdquo; Discover section: Catalog, Generative AI playground, Shared generative AI assets Build section: ML and generative AI model development, Generative AI app development Browse all projects vÃ  Create project buttons ğŸ’¡ SageMaker Unified Studio:\nUnified interface cho data analytics, ML, vÃ  generative AI Project-based workspace vá»›i shared resources Built-in collaboration vá»›i team members vÃ  approval workflows Integrated tools: Notebooks, Visual ETL, Workflows, Chat agents 2.2. Táº¡o Project trong Unified Studio Trong SageMaker Unified Studio dashboard:\nBÆ°á»›c 1: Truy cáº­p Create Project\nTrong Build section, click \u0026ldquo;Create project\u0026rdquo; (nÃºt xanh) Hoáº·c click \u0026ldquo;Browse all projects\u0026rdquo; â†’ \u0026ldquo;Create project\u0026rdquo; BÆ°á»›c 2: Äiá»n thÃ´ng tin Project (Step 1)\nProject name: retail-ml-training Description: Retail price sensitivity model training Click Next Ä‘á»ƒ chuyá»ƒn tá»›i Step 2 BÆ°á»›c 2.5: Chá»n Project Profile (Step 2)\nProject profile: Chá»n \u0026ldquo;All capabilities\u0026rdquo; (highlighted in blue) Description: \u0026ldquo;Analyze data and build machine learning and generative AI models and applications powered by Amazon Bedrock, Amazon EMR, AWS Glue, Amazon Athena, Amazon SageMaker AI and Amazon SageMaker Lakehouse\u0026rdquo; Tooling: LakeHouse Database, Workflows + 12 more capabilities CÃ¡c options khÃ¡c: Generative AI application development, SQL analytics BÆ°á»›c 3: Blueprint Parameters\nS3 location: s3://amazon-sagemaker-[account-id]-ap-southeast-1-[random-id] Retention: 731 days Enable Project Repository Auto Sync: false Lakehouse Database: glue_db BÆ°á»›c 4: Create Project\nReview cÃ¡c settings vÃ  click \u0026ldquo;Create project\u0026rdquo; Äá»£i 2-3 phÃºt Ä‘á»ƒ Project Ä‘Æ°á»£c provisioned 2.3. Truy cáº­p Project Workspace Sau khi Project retail-ml-training táº¡o thÃ nh cÃ´ng:\nBÆ°á»›c 1: VÃ o Project Overview\nProject sáº½ hiá»ƒn thá»‹ trong danh sÃ¡ch vá»›i status \u0026ldquo;Created\u0026rdquo; Click vÃ o project name retail-ml-training Ä‘á»ƒ vÃ o Project overview Project overview hiá»ƒn thá»‹: Project title: retail-ml-training Description: \u0026ldquo;Retail price sensitivity model training\u0026rdquo; Project files (6): .ipynb_checkpoints, workflows, .libs.json, .temp_sagemaker_unified_studio_debugging_info, README.md, getting_started.ipynb S3 path: /dzd-5kultpj28sm31d/cu2gr2js1w1wv (project workspace path) Actions vÃ  New dropdown buttons Project overview (active) Data - data assets vÃ  connections Compute - compute resources vÃ  environments Members - team collaboration Project catalog (expandable) Assets, Subscription requests, Data sources, Metadata entities BÆ°á»›c 2: Táº¡o Notebook\nClick \u0026ldquo;New\u0026rdquo; dropdown (nÃºt xanh) â†’ chá»n \u0026ldquo;Notebook\u0026rdquo; New dropdown hiá»ƒn thá»‹ cÃ¡c options (theo thá»© tá»± trong hÃ¬nh): Notebook (chá»n option nÃ y) BÆ°á»›c 3: Project Welcome Trong project overview, báº¡n cÅ©ng tháº¥y Readme section hiá»ƒn thá»‹ \u0026ldquo;Welcome\u0026rdquo; vá»›i hÆ°á»›ng dáº«n báº¯t Ä‘áº§u sá»­ dá»¥ng project.\n2.4. XÃ¡c minh EC2 Permissions (Ä‘Ã£ cÃ³ tá»« Task 2) EC2 permissions Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘áº§y Ä‘á»§ trong Task 2, bao gá»“m inline policy SageMakerEC2Access trong role mlops-retail-prediction-dev-sagemaker-execution.\nXÃ¡c minh EC2 permissions Ä‘Ã£ cÃ³:\n# Kiá»ƒm tra inline policy Ä‘Ã£ Ä‘Æ°á»£c thÃªm tá»« Task 2 aws iam get-role-policy --role-name mlops-retail-prediction-dev-sagemaker-execution --policy-name SageMakerEC2Access # Test quyá»n EC2 describe aws ec2 describe-vpcs --region us-east-1 Role Ä‘Ã£ cÃ³ Ä‘á»§ 4 policies tá»« Task 2:\nâœ… AmazonSageMakerFullAccess (AWS managed) âœ… AmazonS3FullAccess (AWS managed) âœ… CloudWatchLogsFullAccess (AWS managed) âœ… SageMakerEC2Access (inline policy cho Project creation) Project creation ready: Role Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘áº§y Ä‘á»§ tá»« Task 2, cÃ³ thá»ƒ táº¡o Project ngay láº­p tá»©c.\n2.5. Khuyáº¿n nghá»‹ Region cho Task 4 TÃ³m táº¯t: Náº¿u dá»¯ liá»‡u gold/ vÃ  artifacts/ hiá»‡n Ä‘ang náº±m trong bucket mlops-retail-prediction-dev-842676018087 (region us-east-1), khuyáº¿n nghá»‹ lÃ  táº¡o SageMaker Domain / Project á»Ÿ cÃ¹ng us-east-1 Ä‘á»ƒ trÃ¡nh lá»—i cross-region (S3 301), phá»©c táº¡p vá»›i KMS vÃ  endpoint.\nLá»£i Ã­ch khi táº¡o Project á»Ÿ us-east-1:\nLoáº¡i bá» lá»—i \u0026lsquo;bucket must be addressed using the specified endpoint\u0026rsquo; khi SageMaker táº£i dá»¯ liá»‡u tá»« S3. KhÃ´ng cáº§n duy trÃ¬ KMS keys hoáº·c IAM policies á»Ÿ nhiá»u region. Ãt rá»§i ro khi cháº¡y training jobs tá»« Studio/Project. Khi cáº§n táº¡o Project á»Ÿ ap-southeast-1 (hoáº·c region khÃ¡c):\nPháº£i chuyá»ƒn hoáº·c sao chÃ©p dá»¯ liá»‡u gold/ vÃ  artifacts/ sang bucket á»Ÿ region Ä‘Ã³ hoáº·c cáº¥u hÃ¬nh Cross-Region Replication (CRR). Táº¡o KMS keys tÆ°Æ¡ng á»©ng vÃ  cáº­p nháº­t policies/roles cho bucket má»›i. Náº¿u báº¡n muá»‘n giá»¯ Project trong ap-southeast-1, Ä‘Ã¢y lÃ  vÃ­ dá»¥ lá»‡nh Ä‘á»ƒ táº¡o bucket vÃ  sao chÃ©p dá»¯ liá»‡u (PowerShell / CloudShell):\n# Táº¡o bucket má»›i á»Ÿ ap-southeast-1 aws s3 mb s3://mlops-retail-prediction-dev-842676018087-apse1 --region ap-southeast-1 # Äá»“ng bá»™ gold/ vÃ  artifacts/ sang bucket má»›i aws s3 sync s3://mlops-retail-prediction-dev-842676018087/gold/ s3://mlops-retail-prediction-dev-842676018087-apse1/gold/ --acl bucket-owner-full-control --exact-timestamps aws s3 sync s3://mlops-retail-prediction-dev-842676018087/artifacts/ s3://mlops-retail-prediction-dev-842676018087-apse1/artifacts/ --acl bucket-owner-full-control --exact-timestamps # (Optional) Táº¡o KMS key á»Ÿ ap-southeast-1 vÃ  cáº­p nháº­t bucket policy / IAM role # aws kms create-key --region ap-southeast-1 --description \u0026#34;KMS for mlops retail ap-southeast-1\u0026#34; Ghi chÃº:\nNáº¿u bucket nguá»“n dÃ¹ng SSE-KMS, Ä‘áº£m báº£o báº¡n táº¡o tÆ°Æ¡ng á»©ng KMS key á»Ÿ region Ä‘Ã­ch vÃ  cáº­p nháº­t cáº£ bucket policy vÃ  role mlops-retail-prediction-dev-sagemaker-execution. Náº¿u muá»‘n resolution nhanh vÃ  Ã­t thay Ä‘á»•i IAM, chá»n táº¡o Project/Domain á»Ÿ us-east-1 (nÆ¡i bucket hiá»‡n cÃ³) â€” Ä‘Ã¢y lÃ  phÆ°Æ¡ng Ã¡n khuyáº¿n nghá»‹ cho lab vÃ  cháº¡y training nhanh. 3. ETL - Chuáº©n bá»‹ dá»¯ liá»‡u trong SageMaker Studio ğŸ¯ Má»¥c tiÃªu: Äá»c Táº¤T Cáº¢ shop_week partitions tá»« S3 silver/ vÃ  táº¡o train/test/validation splits\nInput: silver/shop_week=200607/ Ä‘áº¿n silver/shop_week=200619/ (14 partitions)\nOutput: gold/train.parquet, gold/test.parquet, gold/validation.parquet\nBÆ°á»›c 1: Táº¡o ETL Notebook trong Project Tá»« Project overview:\nClick \u0026ldquo;New\u0026rdquo; dropdown â†’ chá»n \u0026ldquo;Notebook\u0026rdquo; Notebook sáº½ má»Ÿ trong browser tab má»›i Chá»n kernel: Python 3 (Data Science 3.0) Äáº·t tÃªn notebook: File â†’ Rename â†’ retail-etl-pipeline.ipynb Notebook sáº½ tá»± Ä‘á»™ng lÆ°u vÃ o S3 project path LÆ°u Ã½:\nNotebook cháº¡y trÃªn managed compute instance cá»§a SageMaker Files tá»± Ä‘á»™ng sync vá»›i S3 project storage CÃ³ thá»ƒ chia sáº» vá»›i team members trong project BÆ°á»›c 2: Thá»±c hiá»‡n ETL Pipeline Táº¡o vÃ  cháº¡y cÃ¡c cells sau theo thá»© tá»±:\nCell 1: CÃ i Ä‘áº·t dependencies\n# Install all required packages pip install pandas pyarrow s3fs scikit-learn xgboost sagemaker boto3 joblib Cell 2: Thiáº¿t láº­p cáº¥u hÃ¬nh\nimport boto3 import pandas as pd import numpy as np from sklearn.model_selection import train_test_split import json from datetime import datetime # Configuration - update bucket name theo project cá»§a báº¡n # Náº¿u dÃ¹ng project S3: amazon-sagemaker-[account-id]-ap-southeast-1-[random-id] # Hoáº·c bucket tá»« Task 3: mlops-retail-prediction-dev-[account-id] bucket_name = \u0026#39;amazon-sagemaker-842676018087-ap-southeast-1-f6cd5056a835\u0026#39; # \u0026lt;-- Sá»¬A theo project cá»§a báº¡n raw_prefix = \u0026#39;silver/\u0026#39; gold_prefix = \u0026#39;gold/\u0026#39; # Initialize AWS clients s3 = boto3.client(\u0026#39;s3\u0026#39;) print(f\u0026#39;âœ… AWS clients initialized. Bucket: {bucket_name}\u0026#39;) Cell 3: Load dá»¯ liá»‡u tá»« táº¥t cáº£ partitions\nprint(f\u0026#39;ğŸ“Š Loading all partitioned data from s3://{bucket_name}/{raw_prefix}...\u0026#39;) # Discover all parquet files in silver/ response = s3.list_objects_v2(Bucket=bucket_name, Prefix=raw_prefix) parquet_files = [obj[\u0026#39;Key\u0026#39;] for obj in response.get(\u0026#39;Contents\u0026#39;, []) if obj[\u0026#39;Key\u0026#39;].endswith(\u0026#39;.parquet\u0026#39;) and obj[\u0026#39;Size\u0026#39;] \u0026gt; 0] print(f\u0026#39;ğŸ“ Found {len(parquet_files)} parquet files\u0026#39;) # Load all files into one DataFrame all_dataframes = [] total_rows = 0 for i, key in enumerate(parquet_files[:10]): # Limit to first 10 files for demo s3_path = f\u0026#39;s3://{bucket_name}/{key}\u0026#39; try: df = pd.read_parquet(s3_path) all_dataframes.append(df) total_rows += len(df) print(f\u0026#39; âœ… File {i+1}: {len(df):,} rows from {key.split(\u0026#34;/\u0026#34;)[-1]}\u0026#39;) except Exception as e: print(f\u0026#39; âŒ Failed to load {key}: {e}\u0026#39;) # Combine all data combined_data = pd.concat(all_dataframes, ignore_index=True) print(f\u0026#39;\\nğŸ¯ Combined dataset: {combined_data.shape}\u0026#39;) print(f\u0026#39;ğŸ“‹ Columns: {list(combined_data.columns)}\u0026#39;) Cell 4: Táº¡o features vÃ  target variable\nprint(\u0026#34;ğŸ“Œ STEP 1 â€” Columns in combined_data:\u0026#34;) print(list(combined_data.columns)) # Force lowercase column names for safety combined_data.columns = [c.lower() for c in combined_data.columns] print(\u0026#34;\\nğŸ“Œ STEP 2 â€” Columns after lowercase normalization:\u0026#34;) print(list(combined_data.columns)) print(\u0026#34;\\nğŸ“Œ STEP 3 â€” Checking required columns...\u0026#34;) if \u0026#39;basket_id\u0026#39; in combined_data.columns and \u0026#39;spend\u0026#39; in combined_data.columns: print(\u0026#34;âœ… Found basket_id and spend â€” proceeding with basket-level feature engineering.\u0026#34;) print(\u0026#34;\\nğŸ“Œ STEP 4 â€” Converting numeric columns...\u0026#34;) for col in [\u0026#39;spend\u0026#39;, \u0026#39;quantity\u0026#39;]: if col in combined_data.columns: combined_data[col] = pd.to_numeric(combined_data[col], errors=\u0026#39;coerce\u0026#39;) print(f\u0026#34; - Converted column \u0026#39;{col}\u0026#39; to numeric.\u0026#34;) print(\u0026#34;\\nğŸ“Œ STEP 5 â€” Starting groupby aggregation...\u0026#34;) print(\u0026#34;âš™ï¸ Aggregating, this may take a moment...\u0026#34;) features = combined_data.groupby(\u0026#39;basket_id\u0026#39;).agg({ \u0026#39;spend\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;, \u0026#39;count\u0026#39;], \u0026#39;quantity\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;] if \u0026#39;quantity\u0026#39; in combined_data.columns else [] }).reset_index() print(\u0026#34;âœ… Aggregation complete.\u0026#34;) print(f\u0026#34;ğŸ“Š Features raw shape: {features.shape}\u0026#34;) # Flatten column names print(\u0026#34;\\nğŸ“Œ STEP 6 â€” Flattening column names...\u0026#34;) features.columns = ( [\u0026#39;basket_id\u0026#39;, \u0026#39;spend_sum\u0026#39;, \u0026#39;spend_mean\u0026#39;, \u0026#39;spend_count\u0026#39;] + ([\u0026#39;quantity_sum\u0026#39;, \u0026#39;quantity_mean\u0026#39;] if \u0026#39;quantity\u0026#39; in combined_data.columns else []) ) print(\u0026#34;ğŸ“‹ New feature columns:\u0026#34;, list(features.columns)) print(\u0026#34;\\nğŸ“Œ STEP 7 â€” Creating price_sensitivity target variable...\u0026#34;) median_spend = features[\u0026#39;spend_sum\u0026#39;].median() print(f\u0026#34;Median spend = {median_spend}\u0026#34;) features[\u0026#39;price_sensitivity\u0026#39;] = (features[\u0026#39;spend_sum\u0026#39;] \u0026gt; median_spend).astype(int) else: print(\u0026#34;âŒ Could NOT find required columns for basket-level engineering.\u0026#34;) print(\u0026#34;Available columns:\u0026#34;, list(combined_data.columns)) print(\u0026#34;\\nğŸ“Œ Fallback: Using transaction-level feature engineering...\u0026#34;) features = combined_data.copy() if \u0026#39;spend\u0026#39; in features.columns: features[\u0026#39;price_sensitivity\u0026#39;] = ( features[\u0026#39;spend\u0026#39;] \u0026gt; features[\u0026#39;spend\u0026#39;].median() ).astype(int) print(\u0026#34;âœ… Created price_sensitivity for transaction-level data.\u0026#34;) else: raise RuntimeError(\u0026#34;âŒ spend column not found â€” cannot create price_sensitivity.\u0026#34;) print(\u0026#34;\\nğŸ“Œ STEP 8 â€” Final feature table shape:\u0026#34;) print(features.shape) print(\u0026#34;\\nğŸ“Œ STEP 9 â€” Target distribution:\u0026#34;) print(features[\u0026#39;price_sensitivity\u0026#39;].value_counts(dropna=False)) print(\u0026#34;\\nğŸ“Œ STEP 10 â€” Sample output:\u0026#34;) print(features.head()) Cell 5: Táº¡o train/test/validation splits vÃ  lÆ°u vÃ o S3\nprint(\u0026#39;ğŸ“‹ Creating train/test/validation splits...\u0026#39;) # Create stratified splits train_data, temp_data = train_test_split( features, test_size=0.3, random_state=42, stratify=features[\u0026#39;price_sensitivity\u0026#39;] ) test_data, val_data = train_test_split( temp_data, test_size=0.33, random_state=42, stratify=temp_data[\u0026#39;price_sensitivity\u0026#39;] ) splits = { \u0026#39;train\u0026#39;: train_data, \u0026#39;test\u0026#39;: test_data, \u0026#39;validation\u0026#39;: val_data } # Save to S3 print(f\u0026#39;ğŸ’¾ Saving splits to s3://{bucket_name}/{gold_prefix}...\u0026#39;) for split_name, split_data in splits.items(): s3_path = f\u0026#39;s3://{bucket_name}/{gold_prefix}{split_name}.parquet\u0026#39; split_data.to_parquet(s3_path, index=False) print(f\u0026#39; âœ… {split_name}: {len(split_data):,} rows saved to {split_name}.parquet\u0026#39;) print(\u0026#39;\\nğŸ‰ ETL Complete! Data ready for training.\u0026#39;) # Verify files created response = s3.list_objects_v2(Bucket=bucket_name, Prefix=gold_prefix) if \u0026#39;Contents\u0026#39; in response: print(f\u0026#39;\\nğŸ“ Files in gold/:\u0026#39;) for obj in response[\u0026#39;Contents\u0026#39;]: size_mb = obj[\u0026#39;Size\u0026#39;] / (1024*1024) print(f\u0026#39; ğŸ“„ {obj[\u0026#34;Key\u0026#34;]}: {size_mb:.2f} MB\u0026#39;) 4. Training - Huáº¥n luyá»‡n Model ğŸ¯ Má»¥c tiÃªu: Huáº¥n luyá»‡n mÃ´ hÃ¬nh Random Forest Ä‘á»ƒ phÃ¢n loáº¡i Ä‘á»™ nháº¡y giÃ¡ khÃ¡ch hÃ ng\nInput: S3 gold/train.parquet, gold/test.parquet, gold/validation.parquet\nOutput: Trained Random Forest model trong S3 artifacts/ vá»›i performance metrics\nBÆ°á»›c 1: Táº¡o Training Notebook trong Project Trong Studio interface, click File â†’ New â†’ Notebook Chá»n conda_python3 kernel (hoáº·c Python 3 (Data Science)) Äáº·t tÃªn notebook: notebooks/retail-model-training.ipynb Click Create ğŸ’¡ LÆ°u Ã½: Notebook sáº½ Ä‘Æ°á»£c lÆ°u trong Project repository vÃ  cÃ³ thá»ƒ commit vÃ o CodeCommit.\nBÆ°á»›c 2: Thá»±c hiá»‡n Model Training Táº¡o vÃ  cháº¡y cÃ¡c cells sau theo thá»© tá»±:\nCell 1: Setup SageMaker Configuration\nimport sagemaker import boto3 import os from sagemaker.sklearn.estimator import SKLearn # Initialize SageMaker session and get execution role sagemaker_session = sagemaker.Session() role = sagemaker.get_execution_role() # Configuration - update bucket name if different bucket_name = \u0026#39;mlops-retail-prediction-dev-842676018087\u0026#39; gold_prefix = \u0026#39;gold/\u0026#39; artifacts_prefix = \u0026#39;artifacts/\u0026#39; print(f\u0026#39;âœ… SageMaker Role: {role}\u0026#39;) print(f\u0026#39;ğŸ“Š Training Data: s3://{bucket_name}/{gold_prefix}\u0026#39;) print(f\u0026#39;ğŸ“¦ Model Artifacts: s3://{bucket_name}/{artifacts_prefix}\u0026#39;) Cell 2: Táº¡o Training Script\n%%writefile train_retail_model.py import pandas as pd import joblib import os import json from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report def main(): # ÄÆ°á»ng dáº«n chuáº©n cá»§a SageMaker train_dir = os.environ.get(\u0026#34;SM_CHANNEL_TRAIN\u0026#34;, \u0026#34;/opt/ml/input/data/train\u0026#34;) model_dir = os.environ.get(\u0026#34;SM_MODEL_DIR\u0026#34;, \u0026#34;/opt/ml/model\u0026#34;) # 1. Load data train_path = os.path.join(train_dir, \u0026#34;train.parquet\u0026#34;) print(f\u0026#34;ğŸ“– Loading training data from: {train_path}\u0026#34;) df = pd.read_parquet(train_path) print(f\u0026#34;ğŸ“Š Dataset shape: {df.shape}\u0026#34;) print(f\u0026#34;ğŸ“‹ Columns: {list(df.columns)}\u0026#34;) # 2. Chuáº©n bá»‹ features \u0026amp; target target_col = \u0026#34;price_sensitivity\u0026#34; if \u0026#34;price_sensitivity\u0026#34; in df.columns else df.columns[-1] feature_cols = [c for c in df.columns if c not in [target_col, \u0026#34;basket_id\u0026#34;]] X = df[feature_cols] y = df[target_col] print(f\u0026#34;ğŸ”¢ Features: {len(feature_cols)} columns\u0026#34;) print(f\u0026#34;ğŸ¯ Target: {target_col}\u0026#34;) print(f\u0026#34;ğŸ“ˆ Target distribution: {dict(y.value_counts())}\u0026#34;) # 3. Train/validation split (stratified) X_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y ) # 4. Train Random Forest model print(\u0026#34;\\nğŸŒ² Training Random Forest model...\u0026#34;) model = RandomForestClassifier( n_estimators=100, max_depth=10, min_samples_split=5, min_samples_leaf=2, random_state=42 ) model.fit(X_train, y_train) # 5. Evaluate model y_pred = model.predict(X_val) accuracy = accuracy_score(y_val, y_pred) f1 = f1_score(y_val, y_pred, average=\u0026#34;macro\u0026#34;) precision = precision_score(y_val, y_pred, average=\u0026#34;macro\u0026#34;) recall = recall_score(y_val, y_pred, average=\u0026#34;macro\u0026#34;) print(f\u0026#34;\\nğŸ“Š Model Performance:\u0026#34;) print(f\u0026#34; âœ… Accuracy: {accuracy:.4f}\u0026#34;) print(f\u0026#34; âœ… Precision: {precision:.4f}\u0026#34;) print(f\u0026#34; âœ… Recall: {recall:.4f}\u0026#34;) print(f\u0026#34; âœ… F1-Score: {f1:.4f}\u0026#34;) # 6. Save model and results os.makedirs(model_dir, exist_ok=True) # Save Random Forest model model_path = os.path.join(model_dir, \u0026#39;model.joblib\u0026#39;) joblib.dump(model, model_path) print(f\u0026#39;ğŸŒ² Random Forest model saved: {model_path}\u0026#39;) # Save training results results_path = os.path.join(model_dir, \u0026#39;training_results.json\u0026#39;) training_summary = { \u0026#39;model_name\u0026#39;: \u0026#39;RandomForest\u0026#39;, \u0026#39;accuracy\u0026#39;: float(accuracy), \u0026#39;precision\u0026#39;: float(precision), \u0026#39;recall\u0026#39;: float(recall), \u0026#39;f1_score\u0026#39;: float(f1), \u0026#39;feature_count\u0026#39;: len(feature_cols), \u0026#39;training_samples\u0026#39;: len(X_train), \u0026#39;validation_samples\u0026#39;: len(X_val), \u0026#39;feature_names\u0026#39;: feature_cols, \u0026#39;classification_report\u0026#39;: classification_report(y_val, y_pred, output_dict=True) with open(results_path, \u0026#39;w\u0026#39;) as f: json.dump(training_summary, f, indent=2) print(f\u0026#39;ğŸ“‹ Results saved: {results_path}\u0026#39;) print(f\u0026#39;ğŸ“¦ Model artifacts: 1 model + 1 results file\u0026#39;) # Validation checks target_accuracy = 0.80 target_f1 = 0.70 print(f\u0026#39;\\nğŸ¯ Performance Validation:\u0026#39;) print(f\u0026#39; ğŸ“Š Accuracy â‰¥ {target_accuracy}: {\u0026#34;âœ…\u0026#34; if accuracy \u0026gt;= target_accuracy else \u0026#34;âŒ\u0026#34;} ({accuracy:.3f})\u0026#39;) print(f\u0026#39; ğŸ“Š F1-Score â‰¥ {target_f1}: {\u0026#34;âœ…\u0026#34; if f1 \u0026gt;= target_f1 else \u0026#34;âŒ\u0026#34;} ({f1:.3f})\u0026#39;) if __name__ == \u0026#39;__main__\u0026#39;: main() \u0026#39;\u0026#39;\u0026#39; # Write training script to file with open(\u0026#39;train_retail_model.py\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(train_script) Cell 3: Submit SageMaker Training Job\nprint(\u0026#34;ğŸš€ Submitting SageMaker Training Job...\u0026#34;) # Pre-flight: kiá»ƒm tra region + data trong gold/ s3_client = boto3.client(\u0026#34;s3\u0026#34;) bucket_location = s3_client.get_bucket_location(Bucket=bucket_name) bucket_region = bucket_location[\u0026#34;LocationConstraint\u0026#34;] or \u0026#34;us-east-1\u0026#34; current_region = sagemaker_session.boto_region_name print(f\u0026#34;ğŸ“ Bucket region: {bucket_region}\u0026#34;) print(f\u0026#34;ğŸ“ SageMaker region: {current_region}\u0026#34;) # Kiá»ƒm tra cross-region issue if bucket_region != current_region: print(f\u0026#34;âš ï¸ Region mismatch detected!\u0026#34;) print(f\u0026#34; Bucket: {bucket_name} in {bucket_region}\u0026#34;) print(f\u0026#34; SageMaker: {current_region}\u0026#34;) print(f\u0026#34; This may cause S3 301 redirect errors during training\u0026#34;) print(f\u0026#34; Consider using project bucket in same region or configure cross-region access\u0026#34;) # Option 1: Use project bucket in same region print(f\u0026#34;\\nğŸ’¡ Option 1: Use project bucket (same region):\u0026#34;) print(f\u0026#34; bucket_name = \u0026#39;{project_bucket}\u0026#39;\u0026#34;) print(f\u0026#34; (But need to copy gold/ data to this bucket first)\u0026#34;) # Option 2: Continue with cross-region print(f\u0026#34;\\nğŸ’¡ Option 2: Continue with cross-region (may need additional config)\u0026#34;) # For demo, we\u0026#39;ll continue but warn user import warnings warnings.warn(f\u0026#34;Cross-region S3 access: {bucket_region} -\u0026gt; {current_region}\u0026#34;) else: print(f\u0026#34;âœ… Same region - no cross-region issues\u0026#34;) train_s3_uri = f\u0026#34;s3://{bucket_name}/{gold_prefix}\u0026#34; resp = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=gold_prefix) data_files = [o[\u0026#34;Key\u0026#34;] for o in resp.get(\u0026#34;Contents\u0026#34;, []) if o[\u0026#34;Key\u0026#34;].endswith(\u0026#34;.parquet\u0026#34;)] if not data_files: raise ValueError(f\u0026#34;âŒ No .parquet files found in {train_s3_uri}. Run ETL trÆ°á»›c Ä‘Ã£.\u0026#34;) print(f\u0026#34;âœ… Found {len(data_files)} training files in {train_s3_uri}\u0026#34;) # Cáº¥u hÃ¬nh estimator estimator = SKLearn( entry_point=\u0026#34;train_retail_model.py\u0026#34;, role=role, instance_type=\u0026#34;ml.m5.xlarge\u0026#34;, instance_count=1, framework_version=\u0026#34;1.0-1\u0026#34;, py_version=\u0026#34;py3\u0026#34;, base_job_name=\u0026#34;retail-prediction-training\u0026#34;, sagemaker_session=sagemaker_session, ) print(f\u0026#34;ğŸ“Š Training data location: {train_s3_uri}\u0026#34;) # Cháº¡y training job estimator.fit({\u0026#34;train\u0026#34;: train_s3_uri}, wait=True) job_name = estimator.latest_training_job.name model_artifacts = estimator.model_data print(\u0026#34;\\nğŸ‰ Training job completed!\u0026#34;) print(\u0026#34;ğŸ“ Job name: \u0026#34;, job_name) print(\u0026#34;ğŸ’¾ Model artifacts:\u0026#34;, model_artifacts) except Exception as e: print(f\u0026#39;âŒ Pre-flight check failed: {e}\u0026#39;) print(\u0026#39;ğŸ’¡ Solutions:\u0026#39;) print(\u0026#39; 1. Run ETL notebook first to create gold/ data\u0026#39;) print(\u0026#39; 2. Make sure you ran debug cell (BÆ°á»›c 4) to fix regions\u0026#39;) print(\u0026#39; 3. Check IAM role has S3 access\u0026#39;) raise # Configure estimator for Unified Studio project estimator = SKLearn( entry_point=\u0026#39;train_retail_model.py\u0026#39;, role=role, instance_type=\u0026#39;ml.m5.xlarge\u0026#39;, # Larger instance for better performance instance_count=1, framework_version=\u0026#39;1.0-1\u0026#39;, py_version=\u0026#39;py3\u0026#39;, base_job_name=\u0026#39;retail-prediction-training\u0026#39;, sagemaker_session=sagemaker_session, # Output path sáº½ Ä‘Æ°á»£c set tá»± Ä‘á»™ng vÃ o project S3 location output_path=f\u0026#39;s3://{bucket_name}/{artifacts_prefix}\u0026#39; ) print(f\u0026#39;ğŸ“Š Training data location: {train_s3_uri}\u0026#39;) # Start training job with error handling try: print(\u0026#39;â³ Starting training job (this will take 5-10 minutes)...\u0026#39;) estimator.fit({\u0026#39;train\u0026#39;: train_s3_uri}, wait=True) # Get job results job_name = estimator.latest_training_job.name model_artifacts = estimator.model_data print(f\u0026#39;\\\\nğŸ‰ Training job completed!\u0026#39;) print(f\u0026#39;ğŸ“ Job name: {job_name}\u0026#39;) print(f\u0026#39;ğŸ’¾ Model artifacts: {model_artifacts}\u0026#39;) except Exception as e: print(f\u0026#39;âŒ Training job failed: {e}\u0026#39;) print(\u0026#39;ğŸ’¡ Troubleshooting:\u0026#39;) print(\u0026#39; - Check CloudWatch logs for detailed error\u0026#39;) print(\u0026#39; - Verify IAM role permissions\u0026#39;) print(\u0026#39; - Ensure training data format is correct\u0026#39;) raise Cell 4: Download \u0026amp; Äá»c Training Results\nimport os import tarfile import json import boto3 print(\u0026#34;ğŸ“Š Analyzing training results...\u0026#34;) # model_artifacts láº¥y tá»« Cell 3 (estimator.model_data) print(\u0026#34;ğŸ“¦ Artifact path:\u0026#34;, model_artifacts) # TÃ¡ch bucket + key art_parts = model_artifacts.replace(\u0026#34;s3://\u0026#34;, \u0026#34;\u0026#34;).split(\u0026#34;/\u0026#34;, 1) art_bucket = art_parts[0] art_key = art_parts[1] s3 = boto3.client(\u0026#34;s3\u0026#34;) # Táº£i file model.tar.gz vá» local local_tar = \u0026#34;model.tar.gz\u0026#34; s3.download_file(art_bucket, art_key, local_tar) print(\u0026#34;ğŸ“¥ Downloaded\u0026#34;, local_tar) # Giáº£i nÃ©n vÃ o thÆ° má»¥c model_artifacts/ extract_dir = \u0026#34;model_artifacts\u0026#34; os.makedirs(extract_dir, exist_ok=True) with tarfile.open(local_tar, \u0026#34;r:gz\u0026#34;) as tar: tar.extractall(extract_dir) print(\u0026#34;\\nğŸ“‚ Files inside model:\u0026#34;) print(os.listdir(extract_dir)) # Äá»c training_results.json results_path = os.path.join(extract_dir, \u0026#34;training_results.json\u0026#34;) with open(results_path, \u0026#34;r\u0026#34;) as f: results = json.load(f) print(\u0026#34;\\nğŸŒ² RANDOM FOREST TRAINING RESULTS:\u0026#34;) print(f\u0026#34; ğŸ¤– Model: {results[\u0026#39;model_name\u0026#39;]}\u0026#34;) print(f\u0026#34; ğŸ“Š Accuracy: {results[\u0026#39;accuracy\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34; ğŸ“Š Precision: {results[\u0026#39;precision\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34; ğŸ“Š Recall: {results[\u0026#39;recall\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34; ğŸ“Š F1-Score: {results[\u0026#39;f1_score\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34; ğŸ”¢ Features: {results[\u0026#39;feature_count\u0026#39;]}\u0026#34;) print(f\u0026#34; ğŸ“š Train rows: {results[\u0026#39;training_samples\u0026#39;]:,}\u0026#34;) print(f\u0026#34; ğŸ§ª Val rows: {results[\u0026#39;validation_samples\u0026#39;]:,}\u0026#34;) print(\u0026#34;\\nğŸ“‹ Per-class Performance:\u0026#34;) class_report = results[\u0026#39;classification_report\u0026#39;] for class_name, metrics in class_report.items(): if isinstance(metrics, dict) and \u0026#39;f1-score\u0026#39; in metrics: print(f\u0026#34; {class_name:\u0026gt;12}: Precision={metrics[\u0026#39;precision\u0026#39;]:.3f}, Recall={metrics[\u0026#39;recall\u0026#39;]:.3f}, F1={metrics[\u0026#39;f1-score\u0026#39;]:.3f}\u0026#34;) # Validate target acc_target = 0.80 f1_target = 0.70 print(\u0026#34;\\nğŸ¯ Performance validation:\u0026#34;) print(f\u0026#34; ğŸ“Š Accuracy â‰¥ {acc_target}: {\u0026#39;âœ…\u0026#39; if results[\u0026#39;accuracy\u0026#39;] \u0026gt;= acc_target else \u0026#39;âŒ\u0026#39;} ({results[\u0026#39;accuracy\u0026#39;]:.3f})\u0026#34;) print(f\u0026#34; ğŸ“Š F1-score â‰¥ {f1_target}: {\u0026#39;âœ…\u0026#39; if results[\u0026#39;f1_score\u0026#39;] \u0026gt;= f1_target else \u0026#39;âŒ\u0026#39;} ({results[\u0026#39;f1_score\u0026#39;]:.3f})\u0026#34;) Káº¿t quáº£ âœ… Training HoÃ n thÃ nh! Model Ä‘áº¡t target performance vÃ  sáºµn sÃ ng cho Model Registry.\n5. Monitoring \u0026amp; Results 5.1. Theo dÃµi Training Job trong Studio Trong SageMaker Studio (Unified Studio):\nMá»Ÿ má»¥c Build á»Ÿ thanh bÃªn trÃ¡i\nChá»n Training jobs\nTÃ¬m job cÃ³ tÃªn báº¯t Ä‘áº§u báº±ng: retail-prediction-training-\nNháº¥p vÃ o tÃªn Training Job Ä‘á»ƒ má»Ÿ chi tiáº¿t\nChá»n tab Logs Ä‘á»ƒ xem log real-time\n(TÃ¹y chá»n) Báº¥m â€œOpen in CloudWatchâ€ Ä‘á»ƒ xem log Ä‘áº§y Ä‘á»§\nInfo:\nCloudWatch logs cho Training Job Ä‘Æ°á»£c lÆ°u dÆ°á»›i dáº¡ng: /aws/sagemaker/TrainingJobs/ Náº¿u job bá»‹ lá»—i, lá»—i Python sáº½ náº±m á»Ÿ cuá»‘i log.\n6. Model Registry (Giao diá»‡n má»›i trong Project) Sau khi training job hoÃ n thÃ nh vÃ  táº¡o ra model.tar.gz, bÆ°á»›c tiáº¿p theo lÃ  Ä‘Äƒng kÃ½ mÃ´ hÃ¬nh trong Model Registry.\ná» giao diá»‡n SageMaker Unified Studio má»›i, Model Registry náº±m bÃªn trong tá»«ng Project, khÃ´ng cÃ²n tÃ¡ch riÃªng nhÆ° trÆ°á»›c.\n6.1. Má»Ÿ Model Registry trong Project SageMaker Studio â†’ Projects â†’ mlops-retail-prediction â†’ Models â†’ Registered models\n6.2. Táº¡o Model Group Báº¥m Register model group Äiá»n: Name: retail-price-sensitivity-models Description: Model group for retail price sensitivity prediction Nháº¥n Register model group Model Group sáº½ xuáº¥t hiá»‡n trong danh sÃ¡ch.\n6.3. ÄÄƒng kÃ½ Model Version sau khi Training VÃ o Models â†’ Registered models versions â†’ Model groups Chá»n nhÃ³m: retail-price-sensitivity-models Nháº¥n Register model Nháº­p thÃ´ng tin: Model artifact location (S3)\nVÃ­ dá»¥: s3://amazon-sagemaker-842676018087-us-east-1-xxxx/output/model.tar.gz Version description: Retail prediction model v1.0 Approval status: Pending manual approval Nháº¥n Register Má»™t Model Version má»›i sáº½ Ä‘Æ°á»£c táº¡o.\n6.4. Approve Model Version Má»Ÿ Model group â†’ retail-price-sensitivity-models Chá»n Version 1 Nháº¥n Update status Äáº·t: Approved Save HoÃ n thÃ nh Task 4 ğŸ“ Notebook thá»±c thi: notebooks/sagemaker-retail-etl-training.ipynb\nÄÃ£ thÃ nh cÃ´ng:\nâœ… Táº¡o SageMaker Domain vÃ  cáº¥u hÃ¬nh âœ… Táº¡o Project vÃ  má»Ÿ Studio workspace âœ… ETL toÃ n bá»™ dataset - All shop_week partitions â†’ Gold Parquet âœ… Auto-detect partitions - Scan táº¥t cáº£ shop_week cÃ³ sáºµn âœ… Train Random Forest vá»›i optimal hyperparameters âœ… Chá»n single model focus Ä‘á»ƒ tá»‘i Æ°u performance âœ… Spot instances - Cost optimization vá»›i auto-scaling âœ… Complete notebook - 4 cells vá»›i detailed logging 7. Clean Up Resources (AWS CLI) 7.1. XÃ³a SageMaker Training Jobs # Liá»‡t kÃª training jobs aws sagemaker list-training-jobs --name-contains \u0026#34;retail-prediction-training\u0026#34; --query \u0026#39;TrainingJobSummaries[*].[TrainingJobName,TrainingJobStatus]\u0026#39; --output table # Dá»«ng training job Ä‘ang cháº¡y (náº¿u cÃ³) aws sagemaker stop-training-job --training-job-name \u0026lt;job-name\u0026gt; # Training jobs tá»± Ä‘á»™ng cleanup sau khi hoÃ n thÃ nh (khÃ´ng cáº§n xÃ³a manual) 7.2. XÃ³a Model Registry # Liá»‡t kÃª model packages aws sagemaker list-model-packages --model-package-group-name retail-price-sensitivity-models --query \u0026#39;ModelPackageSummaryList[*].[ModelPackageArn,ModelPackageStatus]\u0026#39; --output table # XÃ³a tá»«ng model package version aws sagemaker delete-model-package --model-package-name \u0026lt;model-package-arn\u0026gt; # XÃ³a model package group aws sagemaker delete-model-package-group --model-package-group-name retail-price-sensitivity-models 7.3. XÃ³a SageMaker Domain vÃ  Project # Liá»‡t kÃª domains aws sagemaker list-domains --query \u0026#39;Domains[*].[DomainId,DomainName,Status]\u0026#39; --output table # XÃ³a user profiles trÆ°á»›c aws sagemaker list-user-profiles --domain-id \u0026lt;domain-id\u0026gt; --query \u0026#39;UserProfiles[*].UserProfileName\u0026#39; --output text # XÃ³a tá»«ng user profile aws sagemaker delete-user-profile --domain-id \u0026lt;domain-id\u0026gt; --user-profile-name \u0026lt;user-profile-name\u0026gt; # XÃ³a domain (sau khi xÃ³a háº¿t user profiles) aws sagemaker delete-domain --domain-id \u0026lt;domain-id\u0026gt; 7.4. Clean Up S3 Artifacts # XÃ³a model artifacts aws s3 rm s3://amazon-sagemaker-\u0026lt;account-id\u0026gt;-\u0026lt;region\u0026gt;-\u0026lt;random-id\u0026gt;/artifacts/ --recursive # XÃ³a gold datasets aws s3 rm s3://amazon-sagemaker-\u0026lt;account-id\u0026gt;-\u0026lt;region\u0026gt;-\u0026lt;random-id\u0026gt;/gold/ --recursive # Kiá»ƒm tra project bucket cÃ²n gÃ¬ aws s3 ls s3://amazon-sagemaker-\u0026lt;account-id\u0026gt;-\u0026lt;region\u0026gt;-\u0026lt;random-id\u0026gt;/ --recursive 7.5. XÃ³a CloudWatch Logs # Liá»‡t kÃª log groups cá»§a SageMaker aws logs describe-log-groups --log-group-name-prefix \u0026#34;/aws/sagemaker/TrainingJobs\u0026#34; --query \u0026#39;logGroups[*].logGroupName\u0026#39; # XÃ³a training job logs aws logs delete-log-group --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training-\u0026lt;timestamp\u0026gt;\u0026#34; 8. Báº£ng giÃ¡ SageMaker 8.1. Chi phÃ­ Training Instances Instance Type vCPU RAM GiÃ¡ (USD/hour) PhÃ¹ há»£p cho ml.m5.large 2 8 GB $0.138 Small datasets, prototyping ml.m5.xlarge 4 16 GB $0.276 Medium datasets (Ä‘Ã£ dÃ¹ng) ml.m5.2xlarge 8 32 GB $0.552 Large datasets ml.c5.xlarge 4 8 GB $0.238 CPU-intensive training ml.p3.2xlarge 8 61 GB $4.284 GPU deep learning 8.2. Chi phÃ­ SageMaker Studio Component GiÃ¡ (USD) Ghi chÃº Studio Notebooks $0.0582/hour ml.t3.medium default Domain Setup Free One-time setup Data Wrangler $0.42/hour Visual data prep Processing Jobs Instance pricing Same as training 8.3. Chi phÃ­ Model Registry \u0026amp; Endpoints Service GiÃ¡ (USD) Ghi chÃº Model Registry Free Model versioning Real-time Endpoint $0.076/hour ml.t2.medium Batch Transform Instance pricing Pay per job Multi-model Endpoint $0.076/hour + storage Cost optimization 8.4. Æ¯á»›c tÃ­nh chi phÃ­ cho Task 4 Training Job thá»±c táº¿:\nInstance: ml.m5.xlarge Duration: ~10-15 minutes Training cost: $0.276 Ã— 0.25h = $0.07 SageMaker Studio:\nNotebook development: ~2 hours Instance: ml.t3.medium Studio cost: $0.0582 Ã— 2h = $0.12 Storage \u0026amp; Model Registry:\nModel artifacts: ~50MB S3 storage: ~$0.001 Model Registry: Free Total chi phÃ­ Task 4:\nComponent Duration Cost Training (ml.m5.xlarge) 15 mins $0.07 Studio Notebooks 2 hours $0.12 S3 Storage Monthly $0.001 Total â‰ˆ $0.19 So sÃ¡nh vá»›i cÃ¡c options:\nApproach Instance Duration Cost Performance Current (ml.m5.xlarge) 4 vCPU, 16GB 15 min $0.07 âœ… Balanced Smaller (ml.m5.large) 2 vCPU, 8GB 25 min $0.06 Slower Larger (ml.m5.2xlarge) 8 vCPU, 32GB 8 min $0.07 Faster, same cost Spot instance Same specs 15 min $0.02-0.05 60-70% savings ğŸ’° Cost Optimization Tips:\nSpot instances: 60-70% cheaper cho non-critical training Smaller instances: OK cho datasets \u0026lt; 1GB Studio auto-shutdown: Tá»± Ä‘á»™ng táº¯t notebooks sau 1h idle Batch jobs: Thay vÃ¬ real-time endpoints cho inference ğŸ“Š SageMaker Unified Studio Benefits:\nIntegrated Workspace: Project-based collaboration vá»›i shared resources Managed Infrastructure: Auto-provisioned compute cho notebooks vÃ  training Cross-Region Support: Built-in handling cá»§a S3 cross-region access Asset Catalog: Automatic registration cá»§a models vÃ  datasets Team Collaboration: Shared notebooks, workflows, vÃ  approval processes Cost Optimization: Managed compute vá»›i automatic scaling Unified Interface: Single pane for data, ML, vÃ  generative AI workflows ğŸ“¹ Video thá»±c hiá»‡n Task 4 Next Step: Task 05: Production VPC\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.3-week3/",
	"title": "Tuáº§n 3 - Dá»‹ch vá»¥ mÃ¡y áº£o/Compute (VM) trÃªn AWS",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 3: ThÃ nh tháº¡o triá»ƒn khai vÃ  quáº£n lÃ½ EC2. Triá»ƒn khai tÃ­nh sáºµn sÃ ng cao (High Availability) vá»›i Auto Scaling (ASG) vÃ  Load Balancing (ALB). CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - EC2: Táº¡o Launch Templates kÃ¨m User Data scripts.\n- Cáº¥u hÃ¬nh IAM Roles cho EC2 (truy cáº­p S3). AWS EC2 Docs 3 - CÃ¢n báº±ng táº£i (Load Balancing): Triá»ƒn khai Application Load Balancer (ALB).\n- Cáº¥u hÃ¬nh Target Groups vÃ  Health Checks (/health). AWS ELB Docs 4 - Auto Scaling: Táº¡o Auto Scaling Group (ASG) trÃªn Multi-AZ.\n- Thiáº¿t láº­p scaling policies (vÃ­ dá»¥: CPU \u0026gt; 50%). AWS ASG Docs 5 - GiÃ¡m sÃ¡t (Monitoring): CÃ i CloudWatch Agent qua User Data.\n- Test cÃ¡c sá»± kiá»‡n scaling (stress test). Hands-on Lab 6 - Ã”n táº­p: XÃ¡c minh ALB routing vÃ  ASG auto scaling Ä‘á»™ng.\n- BÃ¡o cÃ¡o tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 3: Tá»± Ä‘á»™ng hoÃ¡ provisioning EC2 báº±ng Launch "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "AWS MLOps Retail Prediction Platform\rğŸ—ï¸ Infrastructure\rğŸ¤– ML Training\rğŸš€ Deployment\rğŸ“Š Monitoring\rğŸ”„ CI/CD\rğŸ’° Cost Optimization\rEnd-to-end MLOps pipeline cho Retail Prediction vá»›i Infrastructure as Code vÃ  Model Deployment\nğŸ‘¨â€ğŸ’» TÃ¡c giáº£ 1\rNguyá»…n Thanh NhÃ¢n\nCloud Engineer\nnpthanhnhan2003@gmail.com\rnguyen-nhan-732a66247\ruit-ntn\rUIT.NTN.13\rğŸ‘©â€ğŸ’» TÃ¡c giáº£ 2\rHÃ  Kháº£ NguyÃªn\nData Engineer\nhakhanguyen09052004@gmail.com\rLinkedIn\rNguyenHK64\rFacebook\rğŸ›’ Retail Prediction MLOps Technology Stack\rï¿½ï¸ Infrastructure\rTerraform + EKS + VPC + IAM\rğŸ¤– ML Training\rSageMaker + Model Registry + S3\rğŸ³ Containers\rEKS + ECR + Docker + HPA\rğŸ’¾ Data \u0026 Storage\rS3 Data Lake + Versioning + Lifecycle\rğŸš€ CI/CD \u0026 Automation\rJenkins + Pipeline Automation\rğŸ“Š Monitoring \u0026 Security\rCloudWatch + KMS + CloudTrail\rğŸ“š Retail Prediction MLOps Workshop\r12 Tasks MLOps hoÃ n chá»‰nh cho Retail Prediction\rEnd-to-end MLOps pipeline tá»« Infrastructure as Code Ä‘áº¿n Model Deployment vá»›i Monitoring vÃ  Cost Optimization cho Retail Prediction\rğŸ—ï¸ Infrastructure\rğŸ¤– ML Training\rï¿½ Deployment\rï¿½ Monitoring\rğŸš€ CI/CD\rï¿½ Cost Optimization\rğŸ—ï¸\rInfrastructure Foundation\râ–¸Retail prediction architecture \u0026 objectives\râ–¸VPC, subnets, NAT, security groups\râ–¸IAM roles \u0026 IRSA configuration\râ–¸EKS control plane setup\râ–¸EC2 managed node groups\rğŸ¤–\rML Training \u0026 Registry\râ–¸S3 data lake \u0026 model artifacts\râ–¸SageMaker retail prediction training \u0026 model registry\rï¿½\rContainer Deployment\râ–¸ECR container registry\râ–¸EKS Cluster setup\râ–¸API Containerization\râ–¸Deploy to Kubernetes\râ–¸Load Balancing\rï¿½\rSecurity \u0026 Audit\râ–¸CloudWatch Monitoring \u0026 Logs\râ–¸CI/CD Pipeline\rï¿½\rCost \u0026 Optimization\râ–¸Cost optimization \u0026 teardown\rğŸš€\rBáº¯t Ä‘áº§u Retail Prediction MLOps Workshop\rğŸ“‹ Prerequisites\rAWS Account, Terraform, kubectl, Docker\râ±ï¸ Thá»i gian\r12-15 giá» (MLOps end-to-end)\rğŸ“ˆ Level\rIntermediate to Advanced\rğŸ¯ Báº¯t Ä‘áº§u vá»›i Task 1: Retail Prediction Architecture Overview\râ†’\râœ¨ Äiá»ƒm ná»•i báº­t cá»§a Retail Prediction MLOps Workshop\rğŸ—ï¸\rInfrastructure as Code\rTerraform automation cho toÃ n bá»™ AWS resources\rğŸ¤–\rSageMaker Training\rDistributed ML training cho retail prediction\rï¿½\rEKS Deployment\rKubernetes cho retail prediction API\rğŸ”’\rSecurity-first\rKMS encryption + CloudTrail audit\rğŸ”„\rCI/CD Pipeline\rAutomated build â†’ train â†’ deploy\rğŸ’°\rCost Optimized\rAuto-scaling + spot instances\râ¬†ï¸\r"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/5-vpc/",
	"title": "Production Networking",
	"tags": [],
	"description": "",
	"content": "ğŸ¯ Má»¥c tiÃªu Task 5 Thiáº¿t láº­p Production VPC cho EKS deployment vÃ  public API demo (riÃªng biá»‡t vá»›i SageMaker training VPC):\nProduction EKS Infrastructure - EKS Cluster vÃ  Pods trong private subnets Public API Access - ALB trong public subnets cho demo endpoint /predict High-Performance Internal Networking - VPC Endpoints cho S3/ECR access \u0026lt; 50ms latency Cost Optimization - Bá» NAT Gateway, chá»‰ báº­t ALB khi demo âš ï¸ VPC Separation Strategy:\nTask 4: SageMaker training dÃ¹ng VPC máº·c Ä‘á»‹nh (Quick setup) - Ä‘Æ¡n giáº£n, tiáº¿t kiá»‡m Task 5: EKS production dÃ¹ng VPC riÃªng - báº£o máº­t, kiá»ƒm soÃ¡t tá»‘t hÆ¡n KhÃ´ng conflict: 2 VPC Ä‘á»™c láº­p, cÃ³ thá»ƒ káº¿t ná»‘i qua VPC Peering náº¿u cáº§n ğŸ¯ Production VPC Architecture:\nâœ… Private Subnets: EKS Pods (secure, no direct Internet) âœ… Public Subnets: ALB only (public API demo access) âœ… Internal Communication: EKS â†” S3 qua VPC Endpoints âœ… Demo Ready: Public API endpoint qua ALB vá»›i SSL/health checks ğŸ“¥ Input\nAWS Account vá»›i VPC permissions CIDR planning: 10.0.0.0/16 (production EKS VPC) Demo requirements: Public API access qua ALB Task 4 completed: SageMaker training cháº¡y trong VPC máº·c Ä‘á»‹nh Input tá»« Task 4: Task 4 (SageMaker training) â€” training VPC choices and requirements Input tá»« Task 2: Task 2 (IAM Roles \u0026amp; Audit) â€” roles and policies required for VPC, EKS and ECR access 1. Hybrid VPC Infrastructure Setup 1.1. Táº¡o VPC Truy cáº­p VPC Dashboard: AWS Console â†’ VPC service â†’ \u0026ldquo;Create VPC\u0026rdquo; VPC Configuration: VPC Name: mlops-retail-forecast-hybrid-vpc\rIPv4 CIDR: 10.0.0.0/16\rIPv6 CIDR: No IPv6 CIDR block\rTenancy: Default\rEnable DNS hostnames: âœ… (Required for VPC Endpoints)\rEnable DNS resolution: âœ… (Required for VPC Endpoints) 1.2. Táº¡o Subnets Public Subnets (ALB Only):\nNavigate to \u0026ldquo;Subnets\u0026rdquo; â†’ \u0026ldquo;Create subnet\u0026rdquo; Public Subnet 1 (ap-southeast-1a):\nName: mlops-hybrid-public-alb-ap-southeast-1a\rAvailability Zone: ap-southeast-1a\rIPv4 CIDR: 10.0.1.0/24\rPurpose: ALB + Internet Gateway access Public Subnet 2 (ap-southeast-1b):\nName: mlops-hybrid-public-alb-ap-southeast-1b\rAvailability Zone: ap-southeast-1b\rIPv4 CIDR: 10.0.2.0/24\rPurpose: ALB + Internet Gateway access Private Subnets (EKS Production): Private Subnet 1 (ap-southeast-1a):\nName: mlops-eks-private-workloads-ap-southeast-1a\rAvailability Zone: ap-southeast-1a\rIPv4 CIDR: 10.0.101.0/24\rPurpose: EKS API Pods Private Subnet 2 (ap-southeast-1b):\nName: mlops-eks-private-workloads-ap-southeast-1b\rAvailability Zone: ap-southeast-1b\rIPv4 CIDR: 10.0.102.0/24\rPurpose: EKS API Pods 1.3. Internet Gateway Setup Táº¡o Internet Gateway: \u0026ldquo;Internet Gateways\u0026rdquo; â†’ \u0026ldquo;Create internet gateway\u0026rdquo; Name: mlops-hybrid-igw\rPurpose: Public access for ALB demo endpoint Attach to VPC: Select Internet Gateway â†’ \u0026ldquo;Actions\u0026rdquo; â†’ \u0026ldquo;Attach to VPC\u0026rdquo; Select: mlops-retail-forecast-hybrid-vpc 1.4. Route Tables Configuration 1.4.1. Public Route Table (ALB Traffic) Create Public Route Table: Name: mlops-hybrid-public-alb-rt\rVPC: mlops-retail-forecast-hybrid-vpc\rPurpose: Route Internet traffic to ALB Add Internet Gateway Route: Add route: 0.0.0.0/0 â†’ Internet Gateway (mlops-hybrid-igw) Associate Public Subnets: Associate both ALB public subnets 1.4.2. Private Route Table (Secure Workloads) Create Private Route Table:\nName: mlops-hybrid-private-workloads-rt\rVPC: mlops-retail-forecast-hybrid-vpc\rPurpose: VPC Endpoints access only (no Internet) Keep Default Local Routes:\nOnly local VPC route: 10.0.0.0/16 â†’ local NO Internet Gateway route (security) VPC Endpoints will provide AWS services access Associate Private Subnets:\nAssociate both workload private subnets 1.5. Security Groups Setup (Layered Security) 1.5.1. ALB Security Group (Public Internet Access) Basic Details:\nSecurity group name: mlops-hybrid-alb-sg\rDescription: Security group for ALB public API demo access\rVPC: mlops-retail-forecast-hybrid-vpc Inbound Rules (Public Demo Access):\nRule 1: HTTP (80) from 0.0.0.0/0 (redirect to HTTPS) Rule 2: HTTPS (443) from 0.0.0.0/0 (secure API demo) Outbound Rules:\nRule: All traffic to EKS Security Group (ALB â†’ EKS communication) 1.5.2. EKS Worker Nodes Security Group (Private Workloads) Basic Details:\nSecurity group name: mlops-hybrid-eks-nodes-sg\rDescription: Security group for EKS worker nodes in private subnets\rVPC: mlops-retail-forecast-hybrid-vpc Inbound Rules (Controlled Access):\nRule 1: HTTP (80) from ALB Security Group (API traffic) Rule 2: HTTPS (443) from ALB Security Group (secure API traffic) Rule 3: All Traffic from self (inter-node communication) Rule 4: All Traffic from EKS Control Plane SG (cluster management) Outbound Rules:\nRule 1: HTTPS (443) to VPC Endpoints SG (AWS services access) Rule 2: All Traffic to self (inter-node communication) 1.5.3. EKS Control Plane Security Group Basic Details:\nSecurity group name: mlops-hybrid-eks-control-plane-sg\rDescription: Security group for EKS control plane\rVPC: mlops-retail-forecast-hybrid-vpc Inbound Rules:\nRule: HTTPS (443) from EKS Nodes SG (cluster API access) Outbound Rules:\nRule: All Traffic to EKS Nodes SG (cluster management) 1.5.4. VPC Endpoints Security Group (AWS Services Access) Basic Details:\nSecurity group name: mlops-hybrid-vpc-endpoints-sg\rDescription: Security group for VPC endpoints (S3, ECR, SageMaker)\rVPC: mlops-retail-forecast-hybrid-vpc Inbound Rules (Internal Access Only):\nRule 1: HTTPS (443) from EKS Nodes SG (EKS â†’ AWS services) Rule 2: HTTPS (443) from SageMaker SG (SageMaker â†’ S3/ECR) Rule 3: HTTPS (443) from VPC CIDR (10.0.0.0/16) - fallback ğŸ¯ Security Groups Complete!\n4 Security Groups Created:\nALB SG: Public Internet access (80/443) EKS Nodes SG: Private workloads EKS Control Plane SG: Cluster management VPC Endpoints SG: AWS services access Note: SageMaker sáº½ dÃ¹ng default SG trong VPC máº·c Ä‘á»‹nh (Task 4)\n1.6. Enable Auto-assign Public IP for ALB Subnets Important for ALB functionality:\nNavigate to Public Subnets:\nVPC Dashboard â†’ Subnets Enable Auto-assign Public IP:\nSelect each public subnet Actions â†’ \u0026ldquo;Modify auto-assign IP settings\u0026rdquo; âœ… Enable auto-assign public IPv4 address âš ï¸ Critical for ALB: Public subnets must have auto-assign public IP enabled, otherwise ALB creation will fail.\n1.7. Console Setup Complete ğŸ¯ Hybrid VPC Console Setup Complete!\nSecurity Architecture:\nLayer 1: Internet â†’ ALB (80/443 from 0.0.0.0/0) Layer 2: ALB â†’ EKS Nodes (controlled access) Layer 3: EKS â†’ VPC Endpoints (AWS services only) Layer 4: Private subnets completely isolated from Internet Demo Ready: ALB can accept public traffic and route to private EKS API pods\n2. VPC Endpoints for High-Performance Internal Networking BÆ°á»›c nÃ y Báº®T BUá»˜C pháº£i lÃ m Ä‘á»ƒ Ä‘áº£m báº£o EKS â†” S3 â†” SageMaker latency \u0026lt; 50ms:\n2.1. S3 Gateway Endpoint (FREE - Model/Data Access) Create S3 Gateway Endpoint: VPC Dashboard â†’ \u0026ldquo;Endpoints\u0026rdquo; â†’ \u0026ldquo;Create endpoint\u0026rdquo; Endpoint name: mlops-hybrid-s3-gateway-endpoint\rService: com.amazonaws.ap-southeast-1.s3\rType: Gateway\rVPC: mlops-retail-forecast-hybrid-vpc\rRoute Tables: mlops-hybrid-private-workloads-rt\rPolicy: Full Access (demo purposes) Purpose: EKS Pods load model artifacts tá»« S3 \u0026lt; 50ms latency\n2.2. ECR API Interface Endpoint (Container Images) Create ECR API Endpoint: Endpoint name: mlops-hybrid-ecr-api-endpoint\rService: com.amazonaws.ap-southeast-1.ecr.api\rType: Interface\rVPC: mlops-retail-forecast-hybrid-vpc\rSubnets: Both private workload subnets\rSecurity Groups: mlops-hybrid-vpc-endpoints-sg\rPrivate DNS: âœ… Enabled Purpose: EKS pull container images tá»« ECR repository\n2.3. ECR DKR Interface Endpoint (Docker Registry) Create ECR DKR Endpoint: Endpoint name: mlops-hybrid-ecr-dkr-endpoint\rService: com.amazonaws.ap-southeast-1.ecr.dkr\rType: Interface\rVPC: mlops-retail-forecast-hybrid-vpc\rSubnets: Both private workload subnets\rSecurity Groups: mlops-hybrid-vpc-endpoints-sg\rPrivate DNS: âœ… Enabled Purpose: Docker layer downloads cho EKS container runtime\n2.4. CloudWatch Logs Interface Endpoint Create CloudWatch Logs Endpoint: Endpoint name: mlops-hybrid-logs-endpoint\rService: com.amazonaws.ap-southeast-1.logs\rType: Interface\rVPC: mlops-retail-forecast-hybrid-vpc\rSubnets: Both private workload subnets\rSecurity Groups: mlops-hybrid-vpc-endpoints-sg\rPrivate DNS: âœ… Enabled Purpose: Monitoring and logging cho EKS/SageMaker workloads\n2.5. VPC Endpoints Verification Expected Results:\nS3 Gateway: Route added to private route table automatically 3x Interface Endpoints: ENI created in each private subnet Private DNS: All endpoints resolvable via internal DNS ğŸ¯ VPC Endpoints Complete!\nHigh-Performance Internal Network:\nEKS â†” S3: \u0026lt; 50ms (Gateway Endpoint) EKS â†” ECR: \u0026lt; 50ms (Interface Endpoints) EKS â†” CloudWatch: \u0026lt; 50ms (Logs Endpoint) No Internet dependency for AWS services Cost Optimized: ~$7.2/month tiáº¿t kiá»‡m (khÃ´ng cáº§n SageMaker VPC Endpoint)\n3. Application Load Balancer Setup 3.1. Create Application Load Balancer Navigate to Load Balancers:\nEC2 Dashboard â†’ Load Balancers â†’ \u0026ldquo;Create load balancer\u0026rdquo; ALB Configuration:\nName: mlops-hybrid-api-demo-alb\rScheme: Internet-facing\rIP address type: IPv4\rVPC: mlops-retail-forecast-hybrid-vpc\rMappings: Both public ALB subnets\rSecurity groups: mlops-hybrid-alb-sg 3.2. Create Target Group for EKS API Target Group Configuration: Target type: IP addresses\rTarget group name: mlops-hybrid-eks-api-tg\rProtocol: HTTP\rPort: 80\rVPC: mlops-retail-forecast-hybrid-vpc\rHealth check path: /health\rHealth check port: 80 Health Check Settings: Health check protocol: HTTP\rHealth check path: /health\rHealth check port: 80\rHealthy threshold: 2\rUnhealthy threshold: 2\rTimeout: 5 seconds\rInterval: 30 seconds\rSuccess codes: 200 3.3. Configure ALB Listeners HTTP Listener (Redirect to HTTPS):\nProtocol: HTTP\rPort: 80\rDefault action: Redirect to HTTPS HTTPS Listener (API Traffic):\nProtocol: HTTPS\rPort: 443\rDefault action: Forward to target group\rSSL certificate: ACM certificate (hoáº·c self-signed for demo) ğŸ’¡ SSL Certificate Options:\nProduction: Use AWS Certificate Manager (ACM) vá»›i domain Demo: Create self-signed certificate or use HTTP only Development: Skip SSL, use HTTP listener only 3.4. ALB Creation Complete ALB DNS Name: Will be used for public API demo access\nExample: mlops-hybrid-api-demo-alb-1234567890.ap-southeast-1.elb.amazonaws.com ğŸ¯ ALB Setup Complete!\nPublic Demo Access Ready:\nHTTP/HTTPS endpoints configured Target group ready for EKS API pods Health checks configured Multi-AZ availability 4. Advanced Configuration \u0026amp; Integration 4.1. VPC Flow Logs # Enable VPC Flow Logs for security monitoring aws ec2 create-flow-logs \\ --resource-type VPC \\ --resource-ids vpc-xxxxxxxxx \\ --traffic-type ALL \\ --log-destination-type cloud-watch-logs \\ --log-group-name VPCFlowLogs \\ --deliver-logs-permission-arn arn:aws:iam::ACCOUNT:role/flowlogsRole 4.2. Network Performance Testing Test EKS â†” S3 Latency:\n# From EKS Pod (after cluster setup) kubectl run test-pod --image=amazonlinux --restart=Never -- sleep 3600 kubectl exec -it test-pod -- bash # Inside pod yum install -y awscli time aws s3 ls s3://your-model-bucket/ # Expected: \u0026lt; 50ms for VPC Endpoint Test ALB â†” EKS Connectivity:\n# Test from outside VPC curl -I http://your-alb-dns-name/health # Expected: HTTP 200 OK when EKS API is running 4.3. Cost Monitoring Setup CloudWatch Cost Alarms:\n# Create cost alarm for VPC Endpoints aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;VPC-Endpoints-Cost-Alert\u0026#34; \\ --alarm-description \u0026#34;Alert when VPC Endpoints cost \u0026gt; $30/month\u0026#34; \\ --metric-name EstimatedCharges \\ --namespace AWS/Billing \\ --statistic Maximum \\ --period 86400 \\ --threshold 30 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=Currency,Value=USD Name=ServiceName,Value=AmazonVPC 5. Terraform Outputs ğŸ’¡ Khi nÃ o cáº§n Terraform outputs:\nâœ… Task 7-10 sáº½ dÃ¹ng Terraform (EKS cluster, ALB controller) âœ… Cáº§n automated deployment across environments âœ… Want to reference infrastructure programmatically Náº¿u Task 7-10 dÃ¹ng Console: Skip pháº§n nÃ y hoÃ n toÃ n!\n5.1. Data Sources (Reference Console-created Resources) File: aws/infra/vpc-data-sources.tf\n# Reference VPC infrastructure tá»« Console data \u0026#34;aws_vpc\u0026#34; \u0026#34;hybrid\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;mlops-retail-forecast-hybrid-vpc\u0026#34;] } } # Public subnets (ALB) data \u0026#34;aws_subnets\u0026#34; \u0026#34;public_alb\u0026#34; { filter { name = \u0026#34;vpc-id\u0026#34; values = [data.aws_vpc.hybrid.id] } filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;*public-alb*\u0026#34;] } } # Private subnets (EKS + SageMaker) data \u0026#34;aws_subnets\u0026#34; \u0026#34;private_workloads\u0026#34; { filter { name = \u0026#34;vpc-id\u0026#34; values = [data.aws_vpc.hybrid.id] } filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;*private-workloads*\u0026#34;] } } # Security Groups data \u0026#34;aws_security_group\u0026#34; \u0026#34;alb\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;mlops-hybrid-alb-sg\u0026#34;] } } data \u0026#34;aws_security_group\u0026#34; \u0026#34;eks_nodes\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;mlops-hybrid-eks-nodes-sg\u0026#34;] } } data \u0026#34;aws_security_group\u0026#34; \u0026#34;eks_control_plane\u0026#34; { filter { name = \u0026#34;tag:Name\u0026#34; values = [\u0026#34;mlops-hybrid-eks-control-plane-sg\u0026#34;] } } # ALB data \u0026#34;aws_lb\u0026#34; \u0026#34;api_demo\u0026#34; { name = \u0026#34;mlops-hybrid-api-demo-alb\u0026#34; } data \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;eks_api\u0026#34; { name = \u0026#34;mlops-hybrid-eks-api-tg\u0026#34; } # VPC Endpoints data \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;s3\u0026#34; { vpc_id = data.aws_vpc.hybrid.id service_name = \u0026#34;com.amazonaws.ap-southeast-1.s3\u0026#34; } data \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;ecr_api\u0026#34; { vpc_id = data.aws_vpc.hybrid.id service_name = \u0026#34;com.amazonaws.ap-southeast-1.ecr.api\u0026#34; } data \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;ecr_dkr\u0026#34; { vpc_id = data.aws_vpc.hybrid.id service_name = \u0026#34;com.amazonaws.ap-southeast-1.ecr.dkr\u0026#34; } data \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;sagemaker_runtime\u0026#34; { vpc_id = data.aws_vpc.hybrid.id service_name = \u0026#34;com.amazonaws.ap-southeast-1.sagemaker.runtime\u0026#34; } 5.2. Outputs for EKS/ALB Integration File: aws/infra/networking-outputs.tf\n# VPC Information output \u0026#34;vpc_id\u0026#34; { description = \u0026#34;Hybrid VPC ID\u0026#34; value = data.aws_vpc.hybrid.id } output \u0026#34;vpc_cidr_block\u0026#34; { description = \u0026#34;VPC CIDR block\u0026#34; value = data.aws_vpc.hybrid.cidr_block } # Subnet Information output \u0026#34;public_alb_subnet_ids\u0026#34; { description = \u0026#34;Public subnet IDs for ALB\u0026#34; value = data.aws_subnets.public_alb.ids } output \u0026#34;private_workload_subnet_ids\u0026#34; { description = \u0026#34;Private subnet IDs for EKS and SageMaker\u0026#34; value = data.aws_subnets.private_workloads.ids } # Security Group Information output \u0026#34;alb_security_group_id\u0026#34; { description = \u0026#34;ALB Security Group ID\u0026#34; value = data.aws_security_group.alb.id } output \u0026#34;eks_nodes_security_group_id\u0026#34; { description = \u0026#34;EKS Nodes Security Group ID\u0026#34; value = data.aws_security_group.eks_nodes.id } output \u0026#34;eks_control_plane_security_group_id\u0026#34; { description = \u0026#34;EKS Control Plane Security Group ID\u0026#34; value = data.aws_security_group.eks_control_plane.id } # ALB Information output \u0026#34;alb_arn\u0026#34; { description = \u0026#34;ALB ARN for API demo\u0026#34; value = data.aws_lb.api_demo.arn } output \u0026#34;alb_dns_name\u0026#34; { description = \u0026#34;ALB DNS name for public API access\u0026#34; value = data.aws_lb.api_demo.dns_name } output \u0026#34;alb_zone_id\u0026#34; { description = \u0026#34;ALB Zone ID for Route53 integration\u0026#34; value = data.aws_lb.api_demo.zone_id } output \u0026#34;eks_api_target_group_arn\u0026#34; { description = \u0026#34;Target group ARN for EKS API pods\u0026#34; value = data.aws_lb_target_group.eks_api.arn } # VPC Endpoints Information output \u0026#34;s3_vpc_endpoint_id\u0026#34; { description = \u0026#34;S3 VPC Endpoint ID\u0026#34; value = data.aws_vpc_endpoint.s3.id } output \u0026#34;ecr_api_vpc_endpoint_id\u0026#34; { description = \u0026#34;ECR API VPC Endpoint ID\u0026#34; value = data.aws_vpc_endpoint.ecr_api.id } output \u0026#34;sagemaker_runtime_vpc_endpoint_id\u0026#34; { description = \u0026#34;SageMaker Runtime VPC Endpoint ID\u0026#34; value = data.aws_vpc_endpoint.sagemaker_runtime.id } # Demo Configuration output \u0026#34;api_demo_config\u0026#34; { description = \u0026#34;Configuration for API demo\u0026#34; value = { public_endpoint = \u0026#34;https://${data.aws_lb.api_demo.dns_name}\u0026#34; health_check = \u0026#34;https://${data.aws_lb.api_demo.dns_name}/health\u0026#34; predict_endpoint = \u0026#34;https://${data.aws_lb.api_demo.dns_name}/predict\u0026#34; } } 5.3. Deploy Terraform Outputs # Navigate to infrastructure directory cd aws/infra # Initialize Terraform terraform init # Plan (should show 0 resources to create, only data sources) terraform plan # Apply outputs terraform apply -auto-approve # Verify outputs terraform output api_demo_config Expected output:\n{ \u0026#34;health_check\u0026#34; = \u0026#34;https://mlops-hybrid-api-demo-alb-123456789.ap-southeast-1.elb.amazonaws.com/health\u0026#34; \u0026#34;predict_endpoint\u0026#34; = \u0026#34;https://mlops-hybrid-api-demo-alb-123456789.ap-southeast-1.elb.amazonaws.com/predict\u0026#34; \u0026#34;public_endpoint\u0026#34; = \u0026#34;https://mlops-hybrid-api-demo-alb-123456789.ap-southeast-1.elb.amazonaws.com\u0026#34; } 6. Verification \u0026amp; Performance Testing 6.1. Network Architecture Verification Verify Hybrid VPC Setup:\n# Check VPC configuration aws ec2 describe-vpcs \\ --filters \u0026#34;Name=tag:Name,Values=mlops-retail-forecast-hybrid-vpc\u0026#34; \\ --query \u0026#39;Vpcs[0].{VpcId:VpcId,CidrBlock:CidrBlock,State:State}\u0026#39; # Verify subnets aws ec2 describe-subnets \\ --filters \u0026#34;Name=vpc-id,Values=$(aws ec2 describe-vpcs --filters \u0026#39;Name=tag:Name,Values=mlops-retail-forecast-hybrid-vpc\u0026#39; --query \u0026#39;Vpcs[0].VpcId\u0026#39; --output text)\u0026#34; \\ --query \u0026#39;Subnets[*].{Name:Tags[?Key==`Name`].Value|[0],CIDR:CidrBlock,AZ:AvailabilityZone,Type:MapPublicIpOnLaunch}\u0026#39; Verify Security Groups:\n# List all security groups for the VPC aws ec2 describe-security-groups \\ --filters \u0026#34;Name=vpc-id,Values=$(terraform output -raw vpc_id)\u0026#34; \\ --query \u0026#39;SecurityGroups[*].{Name:GroupName,ID:GroupId,Description:Description}\u0026#39; \\ --output table 6.2. VPC Endpoints Testing Test S3 VPC Endpoint (after EKS setup):\n# From EKS pod kubectl run network-test --image=amazonlinux --restart=Never -- sleep 3600 kubectl exec -it network-test -- bash # Test S3 access via VPC endpoint yum update -y \u0026amp;\u0026amp; yum install -y awscli curl time time aws s3 ls s3://your-model-bucket/ --region ap-southeast-1 # Expected: \u0026lt; 50ms response time # Test ECR access time aws ecr get-login-token --region ap-southeast-1 # Expected: \u0026lt; 100ms response time Verify VPC Endpoint DNS Resolution:\n# Inside EKS pod nslookup s3.ap-southeast-1.amazonaws.com nslookup api.ecr.ap-southeast-1.amazonaws.com nslookup runtime.sagemaker.ap-southeast-1.amazonaws.com # Should resolve to private IP addresses (10.0.x.x) 6.3. ALB Demo Testing Test ALB Public Access:\n# Get ALB DNS name ALB_DNS=$(aws elbv2 describe-load-balancers \\ --names mlops-hybrid-api-demo-alb \\ --query \u0026#39;LoadBalancers[0].DNSName\u0026#39; \\ --output text) echo \u0026#34;ALB DNS: $ALB_DNS\u0026#34; # Test connectivity (will show 503 until EKS API is deployed) curl -I http://$ALB_DNS/ # Expected: Connection successful (503 Service Unavailable is normal without backend) Verify Target Group (after EKS deployment):\n# Check target group health aws elbv2 describe-target-health \\ --target-group-arn $(aws elbv2 describe-target-groups \\ --names mlops-hybrid-eks-api-tg \\ --query \u0026#39;TargetGroups[0].TargetGroupArn\u0026#39; --output text) 6.4. Cost Monitoring Monthly Cost Breakdown:\n# Check VPC Endpoints cost (should be ~$21.6/month) aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon Virtual Private Cloud\u0026#34;]}}\u0026#39; 7. Troubleshooting \u0026amp; Common Issues 7.1. VPC Endpoints Issues Problem: EKS pods can\u0026rsquo;t access S3\n# Solution 1: Check route table aws ec2 describe-route-tables \\ --filters \u0026#34;Name=tag:Name,Values=mlops-hybrid-private-workloads-rt\u0026#34; \\ --query \u0026#39;RouteTables[0].Routes\u0026#39; # Should see route to S3 endpoint # Solution 2: Check security group aws ec2 describe-security-groups \\ --group-names mlops-hybrid-vpc-endpoints-sg \\ --query \u0026#39;SecurityGroups[0].IpPermissions\u0026#39; # Should allow HTTPS 443 from 10.0.0.0/16 Problem: Interface endpoint DNS not resolving\n# Check private DNS is enabled aws ec2 describe-vpc-endpoints \\ --filters \u0026#34;Name=vpc-id,Values=$(terraform output -raw vpc_id)\u0026#34; \\ --query \u0026#39;VpcEndpoints[*].{Service:ServiceName,PrivateDnsEnabled:PrivateDnsEnabled}\u0026#39; 7.2. ALB Issues Problem: ALB not accessible from Internet\n# Check Internet Gateway attached aws ec2 describe-internet-gateways \\ --filters \u0026#34;Name=attachment.vpc-id,Values=$(terraform output -raw vpc_id)\u0026#34; # Check public subnet route table aws ec2 describe-route-tables \\ --filters \u0026#34;Name=tag:Name,Values=mlops-hybrid-public-alb-rt\u0026#34; \\ --query \u0026#39;RouteTables[0].Routes\u0026#39; # Should have 0.0.0.0/0 â†’ Internet Gateway Problem: ALB can\u0026rsquo;t reach EKS targets\n# Check security group rules aws ec2 describe-security-groups \\ --group-names mlops-hybrid-eks-nodes-sg \\ --query \u0026#39;SecurityGroups[0].IpPermissions[?IpProtocol==`tcp` \u0026amp;\u0026amp; FromPort==`80`]\u0026#39; # Should allow port 80 from ALB security group 7.3. Performance Issues Problem: High latency EKS â†” S3\n# Verify using VPC endpoint kubectl exec -it network-test -- traceroute s3.ap-southeast-1.amazonaws.com # Should not go through Internet (no public IPs in trace) # Check VPC endpoint policy aws ec2 describe-vpc-endpoints \\ --filters \u0026#34;Name=service-name,Values=com.amazonaws.ap-southeast-1.s3\u0026#34; \\ --query \u0026#39;VpcEndpoints[0].PolicyDocument\u0026#39; ğŸ‘‰ Káº¿t quáº£ Task 5 âœ… Hybrid VPC Architecture - Public ALB + Private workloads security model\nâœ… Public API Demo Ready - ALB configured cho demo endpoint /predict\nâœ… High-Performance Internal - VPC Endpoints \u0026lt; 50ms latency EKS â†” S3\nâœ… Cost Optimized - $21.6/month base + $0.02/hour ALB demo usage\nâœ… Production Security - Layered security groups, no Internet access for workloads\nArchitecture Delivered âœ… Hybrid VPC Foundation:\r- Public Subnets: ALB demo access (Internet facing)\r- Private Subnets: EKS + SageMaker (secure, no Internet)\r- Multi-AZ: High availability for API demo\râœ… Public Demo Capability:\r- ALB: Public endpoint for API demo\r- Target Groups: Ready for EKS API pods\r- SSL/Health checks: Production-ready demo\râœ… High-Performance Internal Network:\r- S3 Gateway Endpoint: FREE, \u0026lt; 50ms model access\r- ECR Interface Endpoints: Fast container pulls\r- SageMaker Runtime Endpoint: Low-latency inference\râœ… Security Architecture:\rInternet â†’ ALB SG â†’ EKS SG â†’ VPC Endpoints SG\r(Layered access control) ğŸ¯ Task 5 Complete - Demo-Ready Hybrid VPC!\nPublic Access: ALB provides secure public API demo endpoint\nPrivate Security: EKS/SageMaker workloads completely isolated\nHigh Performance: \u0026lt; 50ms internal AWS services latency\nCost Efficient: $21.6 base + demo usage only when needed\nProduction Ready: SSL, health checks, multi-AZ availability\nğŸš€ Next Steps:\nTask 6: ECR container registry cho API container images Task 7: EKS cluster deployment trong private subnets Task 8: EKS node groups vá»›i auto-scaling Task 10: Deploy API service vá»›i ALB integration Task 11: ALB ingress controller configuration Demo Commands Ready:\n# Public API demo endpoint (after deployment) curl https://your-alb-dns/predict -d \u0026#39;{\u0026#34;data\u0026#34;: \u0026#34;your-input\u0026#34;}\u0026#39; # Health check endpoint curl https://your-alb-dns/health ğŸ“Š Performance Benchmarks Achieved:\nEKS â†” S3 Latency: \u0026lt; 50ms (VPC Gateway Endpoint) EKS â†” ECR Latency: \u0026lt; 100ms (Interface Endpoints) ALB â†” EKS Latency: \u0026lt; 10ms (same VPC) Internet â†” ALB: Standard Internet latency Cost: $21.6/month base + $0.02/hour demo usage Availability: Multi-AZ (99.99% SLA) Next Step: Task 6: ECR Container Registry\nğŸš€ Next Steps:\nTask 3: IAM Roles \u0026amp; IRSA sá»­ dá»¥ng VPC infrastructure Task 4: EKS cluster deployment vá»›i VPC Endpoints integration Task 5: EKS managed node groups trong cost-optimized private subnets 8. Clean Up Resources (AWS CLI) 8.1. XÃ³a Application Load Balancer vÃ  Target Groups # Liá»‡t kÃª ALB aws elbv2 describe-load-balancers --names mlops-hybrid-api-demo-alb --query \u0026#39;LoadBalancers[*].[LoadBalancerArn,DNSName]\u0026#39; --output table # XÃ³a ALB (tá»± Ä‘á»™ng xÃ³a listeners) aws elbv2 delete-load-balancer --load-balancer-arn \u0026lt;alb-arn\u0026gt; # XÃ³a Target Groups aws elbv2 describe-target-groups --names mlops-hybrid-eks-api-tg --query \u0026#39;TargetGroups[*].TargetGroupArn\u0026#39; --output text | xargs -I {} aws elbv2 delete-target-group --target-group-arn {} 8.2. XÃ³a VPC Endpoints # Liá»‡t kÃª VPC Endpoints aws ec2 describe-vpc-endpoints --filters \u0026#34;Name=vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; --query \u0026#39;VpcEndpoints[*].[VpcEndpointId,ServiceName]\u0026#39; --output table # XÃ³a Interface Endpoints (ECR, CloudWatch Logs) aws ec2 delete-vpc-endpoints --vpc-endpoint-ids \u0026lt;ecr-api-endpoint-id\u0026gt; \u0026lt;ecr-dkr-endpoint-id\u0026gt; \u0026lt;logs-endpoint-id\u0026gt; # XÃ³a Gateway Endpoint (S3) aws ec2 delete-vpc-endpoints --vpc-endpoint-ids \u0026lt;s3-gateway-endpoint-id\u0026gt; 8.3. XÃ³a Security Groups # Liá»‡t kÃª Security Groups (trá»« default) aws ec2 describe-security-groups --filters \u0026#34;Name=vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; --query \u0026#39;SecurityGroups[?GroupName!=`default`].[GroupId,GroupName]\u0026#39; --output table # XÃ³a Security Groups (theo thá»© tá»± ngÆ°á»£c dependency) aws ec2 delete-security-group --group-id \u0026lt;vpc-endpoints-sg-id\u0026gt; aws ec2 delete-security-group --group-id \u0026lt;eks-control-plane-sg-id\u0026gt; aws ec2 delete-security-group --group-id \u0026lt;eks-nodes-sg-id\u0026gt; aws ec2 delete-security-group --group-id \u0026lt;alb-sg-id\u0026gt; 8.4. XÃ³a Subnets vÃ  Route Tables # Liá»‡t kÃª Route Tables (trá»« main) aws ec2 describe-route-tables --filters \u0026#34;Name=vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; --query \u0026#39;RouteTables[?Associations[0].Main!=`true`].[RouteTableId,Tags[0].Value]\u0026#39; --output table # XÃ³a Route Tables aws ec2 delete-route-table --route-table-id \u0026lt;public-rt-id\u0026gt; aws ec2 delete-route-table --route-table-id \u0026lt;private-rt-id\u0026gt; # Liá»‡t kÃª Subnets aws ec2 describe-subnets --filters \u0026#34;Name=vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; --query \u0026#39;Subnets[*].[SubnetId,Tags[0].Value,CidrBlock]\u0026#39; --output table # XÃ³a Subnets aws ec2 delete-subnet --subnet-id \u0026lt;public-subnet-1a-id\u0026gt; aws ec2 delete-subnet --subnet-id \u0026lt;public-subnet-1b-id\u0026gt; aws ec2 delete-subnet --subnet-id \u0026lt;private-subnet-1a-id\u0026gt; aws ec2 delete-subnet --subnet-id \u0026lt;private-subnet-1b-id\u0026gt; 8.5. XÃ³a Internet Gateway vÃ  VPC # Detach vÃ  xÃ³a Internet Gateway aws ec2 describe-internet-gateways --filters \u0026#34;Name=attachment.vpc-id,Values=\u0026lt;vpc-id\u0026gt;\u0026#34; --query \u0026#39;InternetGateways[*].InternetGatewayId\u0026#39; --output text | xargs -I {} aws ec2 detach-internet-gateway --internet-gateway-id {} --vpc-id \u0026lt;vpc-id\u0026gt; aws ec2 delete-internet-gateway --internet-gateway-id \u0026lt;igw-id\u0026gt; # XÃ³a VPC (cuá»‘i cÃ¹ng) aws ec2 delete-vpc --vpc-id \u0026lt;vpc-id\u0026gt; # Verify clean up aws ec2 describe-vpcs --vpc-ids \u0026lt;vpc-id\u0026gt; 8.6. Clean Up Helper Script #!/bin/bash # vpc-cleanup.sh VPC_ID=\u0026#34;vpc-xxxxxxxxx\u0026#34; # Thay báº±ng VPC ID thá»±c táº¿ echo \u0026#34;ğŸ§¹ Cleaning up VPC resources for $VPC_ID...\u0026#34; # 1. XÃ³a ALB vÃ  Target Groups echo \u0026#34;Deleting ALB...\u0026#34; ALB_ARN=$(aws elbv2 describe-load-balancers --names mlops-hybrid-api-demo-alb --query \u0026#39;LoadBalancers[0].LoadBalancerArn\u0026#39; --output text 2\u0026gt;/dev/null) if [ \u0026#34;$ALB_ARN\u0026#34; != \u0026#34;None\u0026#34; ] \u0026amp;\u0026amp; [ ! -z \u0026#34;$ALB_ARN\u0026#34; ]; then aws elbv2 delete-load-balancer --load-balancer-arn $ALB_ARN echo \u0026#34;ALB deleted: $ALB_ARN\u0026#34; fi # 2. XÃ³a VPC Endpoints echo \u0026#34;Deleting VPC Endpoints...\u0026#34; ENDPOINTS=$(aws ec2 describe-vpc-endpoints --filters \u0026#34;Name=vpc-id,Values=$VPC_ID\u0026#34; --query \u0026#39;VpcEndpoints[*].VpcEndpointId\u0026#39; --output text) for endpoint in $ENDPOINTS; do aws ec2 delete-vpc-endpoints --vpc-endpoint-ids $endpoint echo \u0026#34;VPC Endpoint deleted: $endpoint\u0026#34; done # 3. Äá»£i resources Ä‘Æ°á»£c xÃ³a echo \u0026#34;Waiting for resources to be deleted...\u0026#34; sleep 60 # 4. XÃ³a Security Groups echo \u0026#34;Deleting Security Groups...\u0026#34; SECURITY_GROUPS=$(aws ec2 describe-security-groups --filters \u0026#34;Name=vpc-id,Values=$VPC_ID\u0026#34; --query \u0026#39;SecurityGroups[?GroupName!=`default`].GroupId\u0026#39; --output text) for sg in $SECURITY_GROUPS; do aws ec2 delete-security-group --group-id $sg 2\u0026gt;/dev/null echo \u0026#34;Security Group deleted: $sg\u0026#34; done echo \u0026#34;âœ… VPC cleanup completed for $VPC_ID\u0026#34; 9. Báº£ng giÃ¡ VPC vÃ  Networking (ap-southeast-1) 9.1. Chi phÃ­ VPC Core Components Component GiÃ¡ (USD) Ghi chÃº VPC Free Unlimited VPCs Subnets Free Unlimited subnets Route Tables Free Routing configuration Internet Gateway Free One per VPC Security Groups Free Firewall rules 9.2. Chi phÃ­ VPC Endpoints Endpoint Type GiÃ¡ (USD/hour) GiÃ¡ (USD/month) Data Transfer Gateway Endpoint (S3) Free Free Free Interface Endpoint $0.01 $7.2 $0.01/GB PrivateLink Endpoint $0.01 $7.2 $0.01/GB Chi phÃ­ VPC Endpoints cho Task 5:\nS3 Gateway: Free ECR API Interface: $7.2/month ECR DKR Interface: $7.2/month CloudWatch Logs Interface: $7.2/month Tá»•ng: $21.6/month 9.3. Chi phÃ­ Application Load Balancer Component GiÃ¡ (USD/hour) GiÃ¡ (USD/month) Ghi chÃº ALB Fixed Cost $0.0225 $16.2 Always running LCU (Load Balancer Capacity Unit) $0.008 $5.76 Per LCU-hour Rule Evaluations $0.008 $5.76 Per million requests Æ¯á»›c tÃ­nh ALB chi phÃ­:\nBase ALB: $16.2/month 1 LCU (basic usage): $5.76/month Total ALB: ~$22/month continuous 9.4. Chi phÃ­ NAT Gateway (KhÃ´ng dÃ¹ng trong Task 5) Component GiÃ¡ (USD/hour) GiÃ¡ (USD/month) Data Transfer NAT Gateway $0.045 $32.4 $0.045/GB Data Processing $0.045/GB Tiáº¿t kiá»‡m: $32.4/month báº±ng cÃ¡ch dÃ¹ng VPC Endpoints thay NAT Gateway\n9.5. Data Transfer Pricing Transfer Type GiÃ¡ (USD/GB) Ghi chÃº VPC Internal Free Same AZ Cross-AZ $0.01 Different AZ trong region VPC Endpoints $0.01 Interface endpoints Internet OUT $0.12 First 1GB free/month S3 Transfer Free Via Gateway endpoint 9.6. Æ¯á»›c tÃ­nh tá»•ng chi phÃ­ Task 5 Monthly Baseline Cost:\nComponent Monthly Cost Purpose VPC + Subnets + IGW $0 Core networking VPC Endpoints (3x Interface) $21.6 ECR + CloudWatch S3 Gateway Endpoint $0 Model access Subtotal $21.6 Always running Demo Usage Cost:\nUsage Pattern ALB Cost Total Cost Use Case Development (8h/day) $5.4/month $27/month Daily development Demo only (3h/day) $2.0/month $23.6/month Presentation demos Production (24/7) $22/month $43.6/month Live production Testing (1h/day) $0.7/month $22.3/month Occasional testing 9.7. Cost Comparison vá»›i Traditional Setup Task 5 (VPC Endpoints) vs Traditional (NAT Gateway):\nArchitecture Monthly Cost Performance Security VPC Endpoints $21.6 \u0026lt; 50ms latency Private network NAT Gateway $32.4 + data Variable Internet routing Savings -$10.8 Better Higher 9.8. Cost Optimization Tips Immediate Savings:\nâœ… Use S3 Gateway Endpoint (Free thay vÃ¬ $7.2/month Interface) âœ… Skip NAT Gateway (-$32.4/month) âœ… Turn off ALB khi khÃ´ng demo (-$22/month) Long-term Optimization:\nUse Spot instances cho EKS nodes (60-70% savings) S3 Intelligent Tiering cho model storage CloudWatch Logs retention policy (7-30 days) Demo Cost Management:\n# Báº­t ALB chá»‰ khi demo aws elbv2 create-load-balancer --name demo-alb --type application # Táº¯t ALB sau demo aws elbv2 delete-load-balancer --load-balancer-arn \u0026lt;arn\u0026gt; ğŸ’° Cost Summary cho Task 5:\nBaseline: $21.6/month (VPC Endpoints, always on) Demo usage: $0.02/hour ALB (chá»‰ khi cáº§n) Savings: $10.8/month so vá»›i NAT Gateway approach Performance: \u0026lt; 50ms internal latency guaranteed Console-created resources sáºµn sÃ ng cho subsequent tasks:\nVPC ID, subnet IDs cho EKS cluster creation Security Group IDs cho EKS vÃ  ALB configuration VPC Endpoint IDs cho cost-optimized AWS services access ğŸ“¹ Video thá»±c hiá»‡n Task 5 Next Step: Task 06: ERC Registry\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.4-week4/",
	"title": "Tuáº§n 4 - Dá»‹ch vá»¥ lÆ°u trá»¯ trÃªn AWS",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 4: KhÃ¡m phÃ¡ cÃ¡c lá»±a chá»n lÆ°u trá»¯ nÃ¢ng cao: S3, EFS vÃ  EBS. Triá»ƒn khai phÃ¢n phá»‘i ná»™i dung vá»›i CloudFront. CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - S3 nÃ¢ng cao: Cáº¥u hÃ¬nh Versioning, Lifecycle Policies vÃ  Encryption.\n- Thiáº¿t láº­p Pre-signed URL Ä‘á»ƒ upload an toÃ n. AWS S3 Docs 3 - CDN: Thiáº¿t láº­p CloudFront vá»›i Origin Access Control (OAC) cho S3.\n- Kiá»ƒm tra caching vÃ  invalidation. AWS CloudFront 4 - LÆ°u trá»¯ file: Táº¡o vÃ  mount Amazon EFS vÃ o cÃ¡c EC2 instances.\n- Cáº¥u hÃ¬nh Access Points vÃ  Mount Targets. AWS EFS Docs 5 - LÆ°u trá»¯ block: Quáº£n lÃ½ EBS Volumes (Snapshot, Restore, Resize).\n- Triá»ƒn khai káº¿ hoáº¡ch Backup. AWS Backup 6 - Ã”n táº­p: XÃ¡c thá»±c phÃ¢n phá»‘i ná»™i dung an toÃ n vÃ  tÃ­nh bá»n vá»¯ng (persistence) cá»§a lÆ°u trá»¯.\n- BÃ¡o cÃ¡o tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 4: Phá»¥c vá»¥ ná»™i dung tÄ©nh an toÃ n thÃ´ng qua CloudFront + S3 OAC. Triá»ƒn khai shared storage báº±ng EFS cho nhiá»u instances. Tá»± Ä‘á»™ng hoÃ¡ backup EBS báº±ng Data Lifecycle Manager. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/6-ecr-registry/",
	"title": "ECR Container Registry",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Má»¥c tiÃªu Task 6: Thiáº¿t láº­p Amazon Elastic Container Registry (ECR) cho MLOps pipeline:\nTáº¡o ECR Repository: Repository cho API container Cáº¥u hÃ¬nh Security: Image scanning, IAM policy, lifecycle rules Build \u0026amp; Push Image: Upload FastAPI container lÃªn ECR Manual Build \u0026amp; Push: HÆ°á»›ng dáº«n build/push báº±ng script (CLI / PowerShell) ğŸ“¥ Input tá»« cÃ¡c Task trÆ°á»›c:\nTask 2 (IAM Roles \u0026amp; Audit): IAM roles, policies vÃ  permissions cho ECR/EKS/S3 access Task 5 (Production VPC): VPC endpoints, networking vÃ  security groups Ä‘á»ƒ cho phÃ©p EKS pull images tá»« ECR ğŸ“¦ Output:\nInference Container: server/ code â†’ FastAPI API serving predictions trong EKS Tá»•ng quan Amazon ECR (Elastic Container Registry) lÃ  dá»‹ch vá»¥ Docker container registry Ä‘Æ°á»£c quáº£n lÃ½ hoÃ n toÃ n bá»Ÿi AWS, tÃ­ch há»£p sÃ¢u vá»›i EKS vÃ  CI/CD pipeline. ECR cung cáº¥p kháº£ nÄƒng lÆ°u trá»¯, quáº£n lÃ½ vÃ  triá»ƒn khai container images má»™t cÃ¡ch an toÃ n cho MLOps workflow.\n1. ECR Repositories Setup 1.1. Create ECR Repositories Navigate to ECR Console: ÄÄƒng nháº­p AWS Console Navigate to Amazon ECR service Region: ap-southeast-1 Chá»n \u0026ldquo;Create repository\u0026rdquo; API Repository Configuration: Repository Created Successfully:\nSau khi táº¡o repository, báº¡n sáº½ tháº¥y giao diá»‡n nhÆ° hÃ¬nh dÆ°á»›i vá»›i thÃ´ng tin:\nRepository name: mlops/retail-api Repository URI: \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api Status: \u0026ldquo;No active images\u0026rdquo; (chÆ°a cÃ³ image nÃ o Ä‘Æ°á»£c push) CÃ¡c tab: Summary, Images, Permissions, Lifecycle policy, Repository tags Repository Setup Complete:\nAPI repository Ä‘Ã£ sáºµn sÃ ng cho containerized FastAPI application.\nRepository Management Interface:\nTrong giao diá»‡n quáº£n lÃ½ repository, báº¡n cÃ³ thá»ƒ:\nImages tab: Xem danh sÃ¡ch images, filter theo tags View push commands: Lá»‡nh Ä‘á»ƒ push image lÃªn repository Copy URI: Copy repository URI Ä‘á»ƒ sá»­ dá»¥ng Scan: QuÃ©t vulnerability cho images Delete: XÃ³a repository khi khÃ´ng cáº§n Tip: Báº­t tag immutability cho cÃ¡c tag production (vÃ­ dá»¥ v*) Ä‘á»ƒ trÃ¡nh accidental overwrite. Sá»­ dá»¥ng semantic tags (v1.2.3, commit-\u0026lt;sha\u0026gt;) giÃºp dá»… rollback vÃ  audit.\n1.2. Lifecycle Policy Setup API Repository Lifecycle Policy: Chá»n repository mlops/retail-api Click tab \u0026ldquo;Lifecycle policy\u0026rdquo; Click \u0026ldquo;Create rule\u0026rdquo; Ä‘á»ƒ táº¡o lifecycle policy Configure API Lifecycle Rules:\nRule 1 - Keep Latest Production Images:\nRule priority: 1\rDescription: Keep latest 10 production images\rImage status: Tagged (wildcard matching)\rImage tag filters: v*\rMatch criteria:\r- Count type: imageCountMoreThan\r- Count number: 10\rAction: expire Rule 2 - Keep Latest Development Images:\nRule priority: 2\rDescription: Keep latest 5 development images\rImage status: Tagged (wildcard matching)\rImage tag filters: dev*, feature*, main*\rMatch criteria:\r- Count type: imageCountMoreThan\r- Count number: 5\rAction: expire Rule 3 - Remove Old Untagged Images:\nRule priority: 3\rDescription: Delete untagged images after 1 day\rImage status: Untagged\rMatch criteria:\r- Days since image created: 1\rAction: expire Training Repository Lifecycle Policy:\n1.3. Image Scanning \u0026amp; Push Commands Check Scan Settings:\nChá»n repository tá»« danh sÃ¡ch Kiá»ƒm tra \u0026ldquo;Scan on push\u0026rdquo; Ä‘Ã£ Ä‘Æ°á»£c enabled Review enhanced scanning options náº¿u cáº§n View Push Commands:\nClick nÃºt \u0026ldquo;View push commands\u0026rdquo; trong giao diá»‡n repository AWS sáº½ hiá»ƒn thá»‹ cÃ¡c lá»‡nh Ä‘á»ƒ authenticate vÃ  push image Copy cÃ¡c lá»‡nh nÃ y Ä‘á»ƒ sá»­ dá»¥ng tá»« local machine hoáº·c CI/CD pipeline ğŸ¯ ECR Repositories Setup Complete!\nCreated Repository:\nâœ… mlops/retail-api: FastAPI prediction service container âœ… Repository URI: \u0026lt;account-id\u0026gt;.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api âœ… Private repository vá»›i tag immutability enabled âœ… Image scanning enabled on push âœ… Lifecycle policies configured for cost optimization âœ… Push commands available trong console âœ… IAM access policies for EKS integration Tip: Ghi chÃº lifecycle rule priorities trong docs team vÃ  test rules trÃªn non-prod repos trÆ°á»›c khi Ã¡p dá»¥ng production Ä‘á»ƒ trÃ¡nh xÃ³a nháº§m images.\n2. API Containerization Workflow 2.1. Dockerfile Configuration Táº¡o server/Dockerfile - Multi-stage build:\n# Multi-stage build FROM python:3.9-slim as builder WORKDIR /app COPY requirements.txt . RUN pip install --user -r requirements.txt # Production stage FROM python:3.9-slim as production WORKDIR /app # Copy dependencies COPY --from=builder /root/.local /root/.local # Create non-root user RUN useradd --create-home --shell /bin/bash apiuser USER apiuser # Copy application COPY . . # Expose port EXPOSE 8000 # Health check HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \\ CMD curl -f http://localhost:8000/health || exit 1 # Start application CMD [\u0026#34;uvicorn\u0026#34;, \u0026#34;main:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;8000\u0026#34;] Táº¡o server/.dockerignore:\n# Development files\r.git\r.gitignore\r__pycache__/\r*.pyc\r.env\r*.log\r# Editor files .idea/\r.vscode/\r# Large files (downloaded at runtime)\r*.joblib\r*.pkl\rmodel/ 2.2. Local Build \u0026amp; Test # Navigate to server directory cd retail-price-sensitivity-prediction/server # Build Docker image docker build -t mlops/retail-api:latest . # Test locally docker run -d --name test -p 8000:8000 mlops/retail-api:latest curl http://localhost:8000/health docker stop test \u0026amp;\u0026amp; docker rm test Warning: Docker login tokens (ECR auth) cÃ³ thá»i háº¡n; CI agents nÃªn refresh token (aws ecr get-login-password) per job. TrÃ¡nh hardcode credentials in scripts or environment files.\n2.3. View Push Commands tá»« AWS Console Trong ECR Console:\nChá»n repository mlops/retail-api Click nÃºt \u0026ldquo;View push commands\u0026rdquo; AWS sáº½ hiá»ƒn thá»‹ cÃ¡c lá»‡nh Ä‘á»ƒ build vÃ  push CÃ¡c lá»‡nh push commands sáº½ nhÆ° (Windows PowerShell):\n# 1. Retrieve an authentication token and authenticate Docker client (Get-ECRLoginCommand).Password | docker login --username AWS --password-stdin 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com # 2. Build your Docker image docker build -t mlops/retail-api . # 3. Tag your image docker tag mlops/retail-api:latest 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest # 4. Push image to ECR docker push 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest Hoáº·c sá»­ dá»¥ng AWS CLI:\n# 1. Retrieve an authentication token and authenticate Docker client aws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com # 2. Build your Docker image docker build -t mlops/retail-api . # 3. Tag your image docker tag mlops/retail-api:latest 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest # 4. Push image to ECR docker push 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest Info: TrÃªn Windows/PowerShell, Æ°u tiÃªn dÃ¹ng aws ecr get-login-password --region \u0026lt;region\u0026gt; | docker login --username AWS --password-stdin \u0026lt;registry\u0026gt; trong CI Ä‘á»ƒ trÃ¡nh cÃ¡c lá»‡nh Ä‘Ã£ bá»‹ deprecated. Token ECR thÆ°á»ng háº¿t háº¡n sau ~12 giá»; xÃ¡c thá»±c láº¡i cho cÃ¡c phiÃªn cháº¡y dÃ i.\n2.2. Verify ECR Push Success Kiá»ƒm tra trong AWS Console:\nNavigate to ECR Console:\nVÃ o AWS Console â†’ ECR service Chá»n repository mlops/retail-api Check tab \u0026ldquo;Images\u0026rdquo; Ä‘á»ƒ xem image Ä‘Ã£ Ä‘Æ°á»£c push Expected Result:\nImage vá»›i tag latest xuáº¥t hiá»‡n trong danh sÃ¡ch Image size hiá»ƒn thá»‹ (~927MB) Vulnerability scan status (if enabled) Push timestamp Kiá»ƒm tra báº±ng CLI:\nKiá»ƒm tra báº±ng console:\nTip: Giáº£m kÃ­ch thÆ°á»›c image báº±ng multi-stage builds vÃ  base images nháº¹ (vÃ­ dá»¥ python:3.9-slim hoáº·c distroless). Image nhá» giÃºp Ä‘áº©y/kÃ©o nhanh hÆ¡n vÃ  giáº£m chi phÃ­ lÆ°u trá»¯/truyá»n táº£i.\n2.5. Container Environment \u0026amp; Testing Environment Variables:\n# Basic configuration AWS_DEFAULT_REGION=ap-southeast-1 MODEL_BUCKET=mlops-retail-forecast-models LOG_LEVEL=INFO PORT=8000 Test Docker Image Locally:\n# Test API container locally docker run -d \\ --name retail-api-test \\ -p 8000:8000 \\ -e AWS_DEFAULT_REGION=ap-southeast-1 \\ -e MODEL_BUCKET=mlops-retail-prediction-dev-842676018087 \\ 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest # Test health endpoint curl http://localhost:8000/health # Test API documentation open http://localhost:8000/docs # Clean up docker stop retail-api-test \u0026amp;\u0026amp; docker rm retail-api-test Warning (Local): Khi cháº¡y image trÃªn mÃ¡y local, trÃ¡nh mount secrets hoáº·c AWS credentials vÃ o container. DÃ¹ng biáº¿n mÃ´i trÆ°á»ng chá»‰ cho giÃ¡ trá»‹ khÃ´ng nháº¡y cáº£m vÃ  Æ°u tiÃªn IAM roles cho mÃ´i trÆ°á»ng production.\nLocal container test for retail-api : HoÃ n thÃ nh! ğŸ‰\nECR registry Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p vÃ  tÃ­ch há»£p vá»›i EKS cluster mlops-retail-cluster. Docker image cá»§a retail API Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ deploy trÃªn Kubernetes trong Task 10.\nKáº¿t quáº£ Task 6 âœ… ECR Repository - mlops/retail-api repository\nâœ… Container Image - FastAPI prediction service\nâœ… Cost Optimization - Lifecycle policies, multi-stage builds, ~$0.15/month\nğŸ¯ Task 6 Complete - ECR Registry + API Containerization!\nâœ… ECR Setup: Repository vá»›i lifecycle policies \u0026amp; image scanning\nâœ… Dockerfile: Multi-stage build, non-root user, health checks\nâœ… Build \u0026amp; Push: Local build â†’ ECR push workflow\nâœ… Testing: Container verification \u0026amp; API validation\nâœ… Ready: Sáºµn sÃ ng cho EKS deployment trong Task 7\nInfo (QuÃ©t lá»— há»•ng): QuÃ©t image cÆ¡ báº£n miá»…n phÃ­; quÃ©t nÃ¢ng cao (Inspector) cÃ³ thá»ƒ phÃ¡t sinh phÃ­ theo image/thÃ¡ng. CÃ¢n nháº¯c chá»‰ quÃ©t cÃ¡c tag production hoáº·c tÃ­ch há»£p quÃ©t vÃ o CI vá»›i Ä‘iá»u kiá»‡n Ä‘á»ƒ kiá»ƒm soÃ¡t chi phÃ­.\nğŸš€ Next Steps:\nTask 7: EKS cluster deployment vá»›i ECR integration Task 8: Deploy API container lÃªn EKS vá»›i ALB Task 9: Load balancing vÃ  scaling configuration Káº¿t quáº£ benchmark (Production):\nKÃ­ch thÆ°á»›c image: FastAPI ~500MB (Ä‘Ã£ tá»‘i Æ°u multi-stage) Thá»i gian build: ~3-5 phÃºt (vá»›i cache) Chi phÃ­ lÆ°u trá»¯: ~$0.15/thÃ¡ng (tá»•ng ~1.5GB) Báº£o máº­t: Cháº¡y non-root, Ä‘Ã£ quÃ©t lá»— há»•ng Kháº£ dá»¥ng: Multi-tag strategy (latest, commit, branch) CI/CD: Tá»± Ä‘á»™ng trÃªn má»—i commit 3. Clean Up Resources (AWS CLI) 3.1. XÃ³a Images tá»« ECR Repository # Liá»‡t kÃª images trong repository aws ecr describe-images --repository-name mlops/retail-api --region ap-southeast-1 --query \u0026#39;imageDetails[*].[imageDigest,imageTags[0],imagePushedAt]\u0026#39; --output table # XÃ³a specific image tag aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --image-ids imageTag=latest \\ --region ap-southeast-1 # XÃ³a táº¥t cáº£ images trong repository aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --image-ids \u0026#34;$(aws ecr describe-images --repository-name mlops/retail-api --region ap-southeast-1 --query \u0026#39;imageDetails[*].{imageDigest:imageDigest}\u0026#39; --output json)\u0026#34; \\ --region ap-southeast-1 3.2. XÃ³a ECR Repositories # XÃ³a repository (pháº£i trá»‘ng trÆ°á»›c) aws ecr delete-repository --repository-name mlops/retail-api --region ap-southeast-1 --force # Verify repository Ä‘Ã£ bá»‹ xÃ³a aws ecr describe-repositories --region ap-southeast-1 --query \u0026#39;repositories[?repositoryName==`mlops/retail-api`]\u0026#39; 3.3. XÃ³a Lifecycle Policies # XÃ³a lifecycle policy (tá»± Ä‘á»™ng xÃ³a khi xÃ³a repository) aws ecr delete-lifecycle-policy --repository-name mlops/retail-api --region ap-southeast-1 # List remaining repositories aws ecr describe-repositories --region ap-southeast-1 --query \u0026#39;repositories[*].[repositoryName,repositoryUri]\u0026#39; --output table 3.4. Clean Up Local Docker Images # Remove local Docker images docker rmi mlops/retail-api:latest docker rmi 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest # Clean up Docker build cache docker system prune -f # Remove unused images docker image prune -a -f 3.5. ECR Cleanup Helper Script #!/bin/bash # ecr-cleanup.sh REPOSITORY_NAME=\u0026#34;mlops/retail-api\u0026#34; REGION=\u0026#34;ap-southeast-1\u0026#34; echo \u0026#34;ğŸ§¹ Cleaning up ECR repository: $REPOSITORY_NAME...\u0026#34; # 1. Delete all images echo \u0026#34;Deleting all images...\u0026#34; IMAGE_IDS=$(aws ecr describe-images --repository-name $REPOSITORY_NAME --region $REGION --query \u0026#39;imageDetails[*].{imageDigest:imageDigest}\u0026#39; --output json) if [ \u0026#34;$IMAGE_IDS\u0026#34; != \u0026#34;[]\u0026#34; ]; then aws ecr batch-delete-image \\ --repository-name $REPOSITORY_NAME \\ --image-ids \u0026#34;$IMAGE_IDS\u0026#34; \\ --region $REGION echo \u0026#34;Images deleted\u0026#34; else echo \u0026#34;No images to delete\u0026#34; fi # 2. Delete repository echo \u0026#34;Deleting repository...\u0026#34; aws ecr delete-repository \\ --repository-name $REPOSITORY_NAME \\ --region $REGION \\ --force # 3. Clean up local Docker echo \u0026#34;Cleaning up local Docker images...\u0026#34; docker rmi mlops/retail-api:latest 2\u0026gt;/dev/null || true docker rmi 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/$REPOSITORY_NAME:latest 2\u0026gt;/dev/null || true echo \u0026#34;âœ… ECR cleanup completed\u0026#34; 4. Báº£ng giÃ¡ ECR 4.1. Chi phÃ­ ECR Storage Storage Type GiÃ¡ (USD/GB/thÃ¡ng) Ghi chÃº ECR Storage $0.10 Compressed image size Free Tier 500MB free First 12 months Data Transfer IN Free Push images to ECR Data Transfer OUT $0.12/GB Pull tá»« Internet Data Transfer VPC Free Pull qua VPC Endpoints 4.2. Chi phÃ­ Image Scanning Scan Type GiÃ¡ (USD) Ghi chÃº Basic Scanning Free CVE database scanning Enhanced Scanning $0.09/image/month Inspector integration OS Package Scanning Free Basic vulnerability detection Language Package Scanning $0.09/image/month Enhanced scanning only 4.3. Æ¯á»›c tÃ­nh chi phÃ­ cho Task 6 Container Images:\nFastAPI image: ~500MB (compressed) Total storage: ~0.5GB Monthly Costs:\nComponent Size Price Monthly Cost ECR Storage 0.5GB $0.10/GB $0.05 Basic Scanning 1 image Free $0.00 VPC Endpoint Transfer ~1GB/month Free $0.00 Total $0.05 4.4. Cost Comparison vá»›i Alternatives ECR vs Docker Hub:\nFeature ECR Docker Hub Winner Storage (500MB) $0.05/month Free (public) Docker Hub Private repos âœ… Native $5/month ECR AWS Integration âœ… Native Manual setup ECR VPC Endpoints âœ… Free transfer âŒ Internet only ECR IAM Integration âœ… Native âŒ Token-based ECR Vulnerability Scanning âœ… Built-in âŒ Extra cost ECR 4.5. Data Transfer Costs ECR Pull Scenarios:\nPull Location Cost Use Case Same Region (VPC) Free EKS production Same Region (Internet) $0.12/GB CI/CD outside AWS Cross Region $0.12/GB + transfer Multi-region deployment Internet (outside AWS) $0.12/GB Local development 4.6. Lifecycle Policy Cost Savings Without Lifecycle Policies:\n50 images Ã— 500MB = 25GB storage Cost: 25GB Ã— $0.10 = $2.50/month With Lifecycle Policies (Task 6):\nKeep 10 production images = 5GB Keep 5 development images = 2.5GB Total: 7.5GB Ã— $0.10 = $0.75/month Savings: $1.75/month (70%) 4.7. Cost Optimization Tips Storage Optimization:\n# Multi-stage builds giáº£m image size FROM node:16 as builder # ... build steps FROM node:16-alpine as production # Smaller base image COPY --from=builder /app/dist ./dist Registry Management:\n# Automated cleanup with lifecycle policies aws ecr put-lifecycle-policy \\ --repository-name mlops/retail-api \\ --lifecycle-policy-text file://lifecycle-policy.json Free Tier Usage:\nSá»­ dá»¥ng 500MB free tier cho development Production images trong repositories riÃªng biá»‡t VPC Endpoints Ä‘á»ƒ trÃ¡nh data transfer charges ğŸ’° Cost Summary cho Task 6:\nStorage: $0.05/month (500MB images) Scanning: Free (basic vulnerability detection) Data Transfer: Free (VPC Endpoints to EKS) Total: $0.05/month (vs $5/month Docker Hub private) Savings: $4.95/month vá»›i ECR + lifecycle policies Máº¹o thÃ nh cÃ´ng: TrÆ°á»›c khi xÃ³a repos/images Ä‘á»ƒ dá»n dáº¹p, hÃ£y snapshot deployment manifests vÃ  tham chiáº¿u CI náº¿u cáº§n lÆ°u trá»¯. Æ¯u tiÃªn sá»­ dá»¥ng lifecycle policies Ä‘á»ƒ tá»± Ä‘á»™ng quáº£n lÃ½ retention thay vÃ¬ xÃ³a thá»§ cÃ´ng, trÃ¡nh máº¥t mÃ¡t dá»¯ liá»‡u.\nğŸ¬ Video thá»±c hiá»‡n Task 6 Next Step: Task 7: EKS Cluster Setup\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "Trong quÃ¡ trÃ¬nh thá»±c hiá»‡n workshop Retail MLOps trÃªn AWS, tÃ´i cÃ³ cÆ¡ há»™i Ã¡p dá»¥ng kiáº¿n thá»©c Cloud/DevOps/MLOps vÃ o má»™t pipeline thá»±c táº¿ end-to-end: tá»« tá»• chá»©c dá»¯ liá»‡u theo chuáº©n S3, huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn SageMaker, quáº£n lÃ½ phiÃªn báº£n mÃ´ hÃ¬nh báº±ng SageMaker Model Registry, Ä‘Ã³ng gÃ³i dá»‹ch vá»¥ inference FastAPI báº±ng Docker, triá»ƒn khai lÃªn EKS, giÃ¡m sÃ¡t qua CloudWatch, tá»± Ä‘á»™ng hoÃ¡ CI/CD, vÃ  tá»‘i Æ°u chi phÃ­ báº±ng cÃ¡c quy trÃ¬nh teardown sau má»—i buá»•i demo.\nThÃ´ng qua workshop nÃ y, tÃ´i Ä‘Ã£ cáº£i thiá»‡n cÃ¡c ká»¹ nÄƒng sau:\nThiáº¿t káº¿ kiáº¿n trÃºc MLOps theo hÆ°á»›ng production (networking, security, scalability) XÃ¢y dá»±ng pipeline tá»« dá»¯ liá»‡u â†’ mÃ´ hÃ¬nh vá»›i tiÃªu chÃ­ cháº¥t lÆ°á»£ng rÃµ rÃ ng vÃ  bÆ°á»›c kiá»ƒm chá»©ng cá»¥ thá»ƒ LÃ m viá»‡c vá»›i cÃ¡c dá»‹ch vá»¥ AWS: S3, SageMaker, ECR, EKS, IAM/IRSA, CloudWatch, VPC endpoints Container hoÃ¡ dá»‹ch vá»¥ inference (multi-stage build, non-root, healthcheck, scan image) Triá»ƒn khai vÃ  váº­n hÃ nh workload Kubernetes (manifests, Service/Ingress, HPA, debug) Tá»‘i Æ°u chi phÃ­ vÃ  váº­n hÃ nh â€œgá»n sáº¡châ€ (Spot, lifecycle policies, log retention, schedule start/stop, teardown scripts) Äá»ƒ pháº£n Ã¡nh khÃ¡ch quan má»©c Ä‘á»™ tiáº¿n bá»™ sau khi hoÃ n thÃ nh workshop, tÃ´i tá»± Ä‘Ã¡nh giÃ¡ theo cÃ¡c tiÃªu chÃ­ dÆ°á»›i Ä‘Ã¢y:\nSTT TiÃªu chÃ­ MÃ´ táº£ Tá»‘t KhÃ¡ Trung bÃ¬nh 1 Kiáº¿n thá»©c \u0026amp; ká»¹ nÄƒng chuyÃªn mÃ´n Hiá»ƒu khÃ¡i niá»‡m MLOps, triá»ƒn khai pipeline train/deploy, cháº¥t lÆ°á»£ng Ä‘áº§u ra âœ… â˜ â˜ 2 Kháº£ nÄƒng há»c há»i Há»c vÃ  Ã¡p dá»¥ng AWS/EKS/SageMaker/ECR/CloudWatch nhanh âœ… â˜ â˜ 3 TÃ­nh chá»§ Ä‘á»™ng Chá»§ Ä‘á»™ng debug (pods, image pull, IRSA), cáº£i tiáº¿n cáº¥u hÃ¬nh vÃ  cleanup âœ… â˜ â˜ 4 Tinh tháº§n trÃ¡ch nhiá»‡m HoÃ n thÃ nh theo checklist, Ä‘áº¡t tiÃªu chÃ­ metric mÃ´ hÃ¬nh vÃ  yÃªu cáº§u verify âœ… â˜ â˜ 5 TÃ­nh ká»· luáº­t TuÃ¢n thá»§ naming conventions, nháº¥t quÃ¡n region, thá»±c hiá»‡n teardown Ä‘á»ƒ trÃ¡nh phÃ¡t sinh chi phÃ­ â˜ âœ… â˜ 6 Tinh tháº§n cáº§u tiáº¿n Tiáº¿p nháº­n pháº£n há»“i vÃ  cáº£i tiáº¿n (Spot, lifecycle, log retention, scheduling) âœ… â˜ â˜ 7 Giao tiáº¿p TrÃ¬nh bÃ y lá»±a chá»n kiáº¿n trÃºc, bÃ¡o cÃ¡o tiáº¿n Ä‘á»™, ghi chÃº bÆ°á»›c verify rÃµ rÃ ng â˜ âœ… â˜ 8 LÃ m viá»‡c nhÃ³m (náº¿u Ã¡p dá»¥ng) Phá»‘i há»£p cÃ¡c pháº§n (data/train/deploy/monitor/cost) vÃ  tÃ­ch há»£p deliverables â˜ âœ… â˜ 9 TÃ¡c phong chuyÃªn nghiá»‡p LÃ m viá»‡c cÃ³ há»‡ thá»‘ng, tÃ´n trá»ng quy Æ°á»›c, Æ°u tiÃªn least privilege vÃ  báº£o máº­t âœ… â˜ â˜ 10 Ká»¹ nÄƒng giáº£i quyáº¿t váº¥n Ä‘á» TÃ¬m root cause vÃ  Ä‘á» xuáº¥t fix (VPC endpoints, IRSA/RBAC, LB/SG, logs) â˜ âœ… â˜ 11 ÄÃ³ng gÃ³p cho dá»± Ã¡n/nhÃ³m Táº¡o pipeline end-to-end cÃ³ thá»ƒ tÃ¡i sá»­ dá»¥ng kÃ¨m script váº­n hÃ nh/teardown âœ… â˜ â˜ 12 Tá»•ng quan Má»©c Ä‘á»™ hoÃ n thÃ nh tá»•ng thá»ƒ vÃ  kháº£ nÄƒng tá»± tÃ¡i hiá»‡n há»‡ thá»‘ng âœ… â˜ â˜ Cáº§n cáº£i thiá»‡n NÃ¢ng cháº¥t lÆ°á»£ng tÃ i liá»‡u hoÃ¡: mÃ´ táº£ kiáº¿n trÃºc rÃµ hÆ¡n, checklist prerequisite, vÃ  bá»™ hÆ°á»›ng dáº«n troubleshooting cÃ³ cáº¥u trÃºc. TÄƒng Ä‘á»™ â€œcháº·tâ€ cho CI/CD: bá»• sung cÃ¡c cá»•ng kiá»ƒm soÃ¡t (tests, policy checks, rollback rÃµ rÃ ng), tÃ¡ch dev/stage/prod. NÃ¢ng má»©c trÆ°á»Ÿng thÃ nh quan sÃ¡t (observability): dashboard/cáº£nh bÃ¡o theo SLO thá»±c táº¿ (latency, error rate), tá»‘i Æ°u log vÃ  (tuá»³ chá»n) tracing. Kiá»ƒm soÃ¡t chi phÃ­ tá»‘t hÆ¡n: schedule theo khung giá» demo, kiá»ƒm soÃ¡t log ingestion, dá»n ECR Ä‘á»‹nh ká»³. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.5-week5/",
	"title": "Tuáº§n 5 - Dá»‹ch vá»¥ báº£o máº­t trÃªn AWS",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 5: Triá»ƒn khai cÃ¡c kiá»ƒm soÃ¡t báº£o máº­t toÃ n diá»‡n (IAM, KMS, WAF, Secrets). TÄƒng kháº£ nÄƒng quan sÃ¡t/giÃ¡m sÃ¡t vá»›i cÃ¡c cÃ´ng cá»¥ audit (CloudTrail, Config, GuardDuty). CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - IAM \u0026amp; Secrets: RÃ  soÃ¡t IAM Roles theo nguyÃªn táº¯c Least Privilege.\n- DÃ¹ng Secrets Manager/Parameter Store Ä‘á»ƒ lÆ°u thÃ´ng tin xÃ¡c thá»±c (credentials). AWS IAM/Secrets 3 - MÃ£ hoÃ¡ (Encryption): Táº¡o Customer Managed Keys (CMK) trong KMS.\n- MÃ£ hoÃ¡ S3/EBS báº±ng KMS keys. AWS KMS Docs 4 - Báº£o máº­t biÃªn (Edge Security): Triá»ƒn khai AWS WAF (Web Application Firewall) cho ALB/CloudFront.\n- Test rule cháº·n SQLi/XSS. AWS WAF Docs 5 - TuÃ¢n thá»§ (Compliance): Báº­t AWS Config vÃ  GuardDuty.\n- Kiá»ƒm tra cÃ¡c phÃ¡t hiá»‡n (findings) trÃªn Security Hub. AWS Security Hub 6 - Ã”n táº­p: Thá»±c hiá»‡n security audit vÃ  remediation.\n- BÃ¡o cÃ¡o tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 5: Táº­p trung quáº£n lÃ½ secrets báº±ng Secrets Manager/SSM. Báº£o vá»‡ endpoint báº±ng AWS WAF rules trÆ°á»›c cÃ¡c táº¥n cÃ´ng phá»• biáº¿n. Báº­t giÃ¡m sÃ¡t tuÃ¢n thá»§ liÃªn tá»¥c vá»›i AWS Config. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/7-feedback/",
	"title": "Chia sáº», Ä‘Ã³ng gÃ³p Ã½ kiáº¿n",
	"tags": [],
	"description": "",
	"content": " Táº¡i Ä‘Ã¢y báº¡n cÃ³ thá»ƒ tá»± do Ä‘Ã³ng gÃ³p Ã½ kiáº¿n cÃ¡ nhÃ¢n vá» nhá»¯ng tráº£i nghiá»‡m khi tham gia chÆ°Æ¡ng trÃ¬nh First Cloud Journey, giÃºp team FCJ cáº£i thiá»‡n nhá»¯ng váº¥n Ä‘á» cÃ²n thiáº¿u sÃ³t dá»±a trÃªn cÃ¡c háº¡ng má»¥c sau:\nÄÃ¡nh giÃ¡ chung 1. MÃ´i trÆ°á»ng lÃ m viá»‡c\nMÃ´i trÆ°á»ng lÃ m viá»‡c ráº¥t thÃ¢n thiá»‡n vÃ  cá»Ÿi má»Ÿ. CÃ¡c thÃ nh viÃªn trong FCJ luÃ´n sáºµn sÃ ng há»— trá»£ khi mÃ¬nh gáº·p khÃ³ khÄƒn, ká»ƒ cáº£ ngoÃ i giá» lÃ m viá»‡c. KhÃ´ng gian lÃ m viá»‡c gá»n gÃ ng, thoáº£i mÃ¡i, giÃºp mÃ¬nh táº­p trung tá»‘t hÆ¡n.\n2. Sá»± há»— trá»£ cá»§a mentor / team admin\nMentor hÆ°á»›ng dáº«n ráº¥t chi tiáº¿t, giáº£i thÃ­ch rÃµ rÃ ng khi mÃ¬nh chÆ°a hiá»ƒu vÃ  luÃ´n khuyáº¿n khÃ­ch mÃ¬nh Ä‘áº·t cÃ¢u há»i. Team admin há»— trá»£ cÃ¡c thá»§ tá»¥c, tÃ i liá»‡u vÃ  táº¡o Ä‘iá»u kiá»‡n Ä‘á»ƒ mÃ¬nh lÃ m viá»‡c thuáº­n lá»£i. Mentor há»— trá»£ tá»‘t á»Ÿ cÃ¡c bÆ°á»›c quan trá»ng nhÆ° thiáº¿t káº¿ kiáº¿n trÃºc, lá»±a chá»n best practices vÃ  hÆ°á»›ng debug theo hÆ°á»›ng â€œtá»± tÃ¬m root causeâ€. Äiá»u nÃ y giÃºp tÃ´i há»c Ä‘Æ°á»£c cÃ¡ch váº­n hÃ nh há»‡ thá»‘ng thay vÃ¬ chá»‰ lÃ m theo hÆ°á»›ng dáº«n.\n3. Sá»± phÃ¹ há»£p giá»¯a cÃ´ng viá»‡c vÃ  chuyÃªn ngÃ nh há»c\nCÃ´ng viá»‡c mÃ¬nh Ä‘Æ°á»£c giao phÃ¹ há»£p vá»›i kiáº¿n thá»©c mÃ¬nh Ä‘Ã£ há»c á»Ÿ trÆ°á»ng, Ä‘á»“ng thá»i má»Ÿ rá»™ng thÃªm nhá»¯ng máº£ng má»›i mÃ  mÃ¬nh chÆ°a tá»«ng Ä‘Æ°á»£c tiáº¿p cáº­n. Nhá» váº­y, mÃ¬nh vá»«a cá»§ng cá»‘ kiáº¿n thá»©c ná»n táº£ng, vá»«a há»c thÃªm ká»¹ nÄƒng thá»±c táº¿. Workshop cÃ³ liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ¡c máº£ng tÃ´i quan tÃ¢m nhÆ° Cloud, DevOps vÃ  Security/Networking: VPC, IAM/IRSA, container security (non-root, scan-on-push), logging/monitoring vÃ  quáº£n lÃ½ chi phÃ­. CÃ¡c pháº§n nÃ y giÃºp tÃ´i káº¿t ná»‘i kiáº¿n thá»©c há»c thuáº­t vá»›i tÃ¬nh huá»‘ng triá»ƒn khai thá»±c táº¿.\n4. CÆ¡ há»™i há»c há»i \u0026amp; phÃ¡t triá»ƒn ká»¹ nÄƒng\nTrong quÃ¡ trÃ¬nh thá»±c táº­p, mÃ¬nh há»c Ä‘Æ°á»£c nhiá»u ká»¹ nÄƒng má»›i nhÆ° sá»­ dá»¥ng cÃ´ng cá»¥ quáº£n lÃ½ dá»± Ã¡n, ká»¹ nÄƒng lÃ m viá»‡c nhÃ³m, vÃ  cáº£ cÃ¡ch giao tiáº¿p chuyÃªn nghiá»‡p trong mÃ´i trÆ°á»ng cÃ´ng ty. Mentor cÅ©ng chia sáº» nhiá»u kinh nghiá»‡m thá»±c táº¿ giÃºp mÃ¬nh Ä‘á»‹nh hÆ°á»›ng tá»‘t hÆ¡n cho sá»± nghiá»‡p.\nTÃ´i cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ ká»¹ nÄƒng triá»ƒn khai vÃ  váº­n hÃ nh:\nThiáº¿t káº¿ pipeline training + registry + deployment theo vÃ²ng Ä‘á»i MLOps ÄÃ³ng gÃ³i vÃ  triá»ƒn khai FastAPI inference service lÃªn EKS Thiáº¿t láº­p monitoring cÆ¡ báº£n báº±ng CloudWatch vÃ  log retention Tá»‘i Æ°u chi phÃ­ báº±ng Spot, lifecycle policies, schedule start/stop, teardown scripts 5. VÄƒn hÃ³a \u0026amp; tinh tháº§n Ä‘á»“ng Ä‘á»™i\nVÄƒn hÃ³a cÃ´ng ty ráº¥t tÃ­ch cá»±c: má»i ngÆ°á»i tÃ´n trá»ng láº«n nhau, lÃ m viá»‡c nghiÃªm tÃºc nhÆ°ng váº«n vui váº».\n6. ChÃ­nh sÃ¡ch / phÃºc lá»£i cho thá»±c táº­p sinh\nCÃ´ng ty cÃ³ há»— trá»£ xÃ¡c nháº­n thá»±c táº­p vÃ  táº¡o Ä‘iá»u kiá»‡n vá» thá»i gian linh hoáº¡t khi cáº§n thiáº¿t.\nMá»™t sá»‘ cÃ¢u há»i khÃ¡c Äiá»u báº¡n hÃ i lÃ²ng nháº¥t trong quÃ¡ trÃ¬nh thá»±c táº­p:\nTÃ´i hÃ i lÃ²ng nháº¥t khi Ä‘Æ°á»£c tham gia vÃ o mÃ´i trÆ°á»ng lÃ m viá»‡c thoáº£i mÃ¡i, cÃ³ quy trÃ¬nh rÃµ rÃ ng. TÃ´i há»c Ä‘Æ°á»£c cÃ¡ch phá»‘i há»£p vá»›i mentor/Ä‘á»“ng Ä‘á»™i, tá»± tÃ¬m hÆ°á»›ng giáº£i quyáº¿t trÆ°á»›c khi há»i, vÃ  cáº£i thiá»‡n tÆ° duy â€œlÃ m Ä‘Ãºng â€“ lÃ m Ä‘á»§ â€“ lÃ m sáº¡châ€ (triá»ƒn khai xong pháº£i kiá»ƒm tra, ghi chÃº, tá»‘i Æ°u vÃ  dá»n tÃ i nguyÃªn náº¿u khÃ´ng thÃ¬ pháº£i tráº£ giÃ¡ báº±ng tiá»n).\nÄiá»u báº¡n nghÄ© cÃ´ng ty cáº§n cáº£i thiá»‡n cho cÃ¡c thá»±c táº­p sinh sau? TÃ´i Ä‘á» xuáº¥t nÃªn cÃ³ cÃ¡c hoáº¡t Ä‘á»™ng giao lÆ°u giá»¯a cÃ¡c trÆ°á»ng Ä‘á»ƒ cÃ¡c báº¡n cÃ³ thá»ƒ há»c há»i láº«n nhau nhiá»u hÆ¡n\nNáº¿u giá»›i thiá»‡u cho báº¡n bÃ¨, báº¡n cÃ³ khuyÃªn há» thá»±c táº­p á»Ÿ Ä‘Ã¢y khÃ´ng? VÃ¬ sao? CÃ³. VÃ¬ mÃ´i trÆ°á»ng há»— trá»£ tá»‘t, mentor hÆ°á»›ng dáº«n theo hÆ°á»›ng giÃºp thá»±c táº­p sinh tá»± nÃ¢ng trÃ¬nh, vÃ  cÃ´ng viá»‡c mang tÃ­nh thá»±c táº¿ (Ä‘Æ°á»£c lÃ m ra sáº£n pháº©m/Ä‘áº§u ra cá»¥ thá»ƒ). Tuy nhiÃªn, sáº½ phÃ¹ há»£p nháº¥t vá»›i cÃ¡c báº¡n cÃ³ tinh tháº§n tá»± há»c vÃ  chá»§ Ä‘á»™ng cáº­p nháº­t kiáº¿n thá»©c.\nÄá» xuáº¥t \u0026amp; mong muá»‘n Báº¡n cÃ³ Ä‘á» xuáº¥t gÃ¬ Ä‘á»ƒ cáº£i thiá»‡n tráº£i nghiá»‡m trong ká»³ thá»±c táº­p? Báº¡n cÃ³ muá»‘n tiáº¿p tá»¥c chÆ°Æ¡ng trÃ¬nh nÃ y trong tÆ°Æ¡ng lai? GÃ³p Ã½ khÃ¡c (tá»± do chia sáº»): "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/7-eks-cluster/",
	"title": "EKS Cluster Setup",
	"tags": [],
	"description": "",
	"content": "ğŸ¯ Má»¥c tiÃªu Task 7 Triá»ƒn khai Amazon Elastic Kubernetes Service (EKS) Ä‘á»ƒ lÃ m ná»n táº£ng cháº¡y API dá»± Ä‘oÃ¡n (FastAPI) trong mÃ´i trÆ°á»ng production:\nEKS Control Plane: Cluster quáº£n lÃ½ bá»Ÿi AWS vá»›i Kubernetes 1.30 IRSA Integration: IAM Roles for Service Accounts cho secure AWS access Cost Optimization: Free Tier t2.micro instances cho development VPC Integration: Sá»­ dá»¥ng hybrid VPC vÃ  VPC Endpoints tá»« Task 5 ECR Integration: Container image pulls tá»« ECR repository â†’ Äáº£m báº£o há»‡ thá»‘ng á»•n Ä‘á»‹nh, má»Ÿ rá»™ng linh hoáº¡t (scalable), vÃ  tÃ­ch há»£p báº£o máº­t vá»›i IAM (IRSA).\nğŸ“¥ Input tá»« cÃ¡c Task trÆ°á»›c:\nTask 5 (Production VPC): Hybrid VPC, subnets, VPC Endpoints and security groups used for EKS networking Task 2 (IAM Roles \u0026amp; Audit): IAM roles and policies (cluster role, node role, IRSA foundations) Task 6 (ECR Registry): ECR repository for container images that EKS will pull Kiáº¿n trÃºc EKS trong MLOps Pipeline Tip: Sá»­ dá»¥ng private-only endpoint access cho production clusters vÃ  giá»›i háº¡n public access CIDRs theo IP ranges cá»¥ thá»ƒ. Báº­t táº¥t cáº£ control plane logs Ä‘á»ƒ audit báº£o máº­t vÃ  tuÃ¢n thá»§ quy Ä‘á»‹nh.\nCost Optimization Strategy Component Cost Free Tier Strategy EKS Control Plane $73/month âŒ Use for demo, destroy after t2.micro Nodes (2x) $0 âœ… 750h/month 12 months FREE EBS Storage (20GB) $0 âœ… 30GB FREE Within free tier VPC Endpoints $21.6/month âŒ Shared with other services Total ~$95/month $60/month savings Free Tier Benefits:\n750 hours/month t2.micro instances â†’ Enough for 24/7 demo 30GB EBS storage â†’ Adequate for basic workloads Total savings: $60/month vs t3.medium instances Warning: EKS control plane tá»‘n $73/thÃ¡ng báº¥t ká»ƒ má»©c sá»­ dá»¥ng. Äá»ƒ há»c táº­p/testing, cÃ¢n nháº¯c dÃ¹ng self-managed k3s trÃªn EC2 hoáº·c kind locally Ä‘á»ƒ trÃ¡nh chi phÃ­ cá»‘ Ä‘á»‹nh. XÃ³a cluster ngay sau khi demo.\n1. EKS Cluster Setup via Console 1.1. Create EKS Cluster Navigate to EKS Console:\nAWS Console â†’ EKS â†’ \u0026ldquo;Create cluster\u0026rdquo; Basic Configuration:\nCluster name: mlops-retail-cluster\rKubernetes version: 1.30\rCluster service role: Use existing from Task 2 (IAM) Networking Configuration:\nVPC: mlops-retail-forecast-hybrid-vpc (from Task 5)\rSubnets: Select all 4 subnets (2 public + 2 private)\rCluster endpoint access: Public and private\rPublic access sources: Specify your IP range\rSecurity groups: mlops-hybrid-eks-control-plane-sg Control Plane Logging: âœ… API server\râœ… Audit\râœ… Authenticator\râœ… Controller manager\râœ… Scheduler Add-ons Configuration: âœ… Amazon VPC CNI: v1.18.1-eksbuild.1\râœ… CoreDNS: v1.11.1-eksbuild.4\râœ… kube-proxy: v1.30.0-eksbuild.3\râœ… Amazon EBS CSI Driver: v1.30.0-eksbuild.1 1.2. Update kubeconfig # Configure kubectl for new cluster aws eks update-kubeconfig --region ap-southeast-1 --name mlops-retail-cluster # Verify connection kubectl get svc kubectl cluster-info Expected output:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rkubernetes ClusterIP 172.20.0.1 \u0026lt;none\u0026gt; 443/TCP 5m\rKubernetes control plane is running at https://ABC123.gr7.ap-southeast-1.eks.amazonaws.com\rCoreDNS is running at https://ABC123.gr7.ap-southeast-1.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy 1.3. Verify Add-ons # Check core add-ons kubectl get pods -n kube-system # Verify VPC CNI kubectl get daemonset -n kube-system aws-node # Check CoreDNS kubectl get deployment -n kube-system coredns # Verify kube-proxy kubectl get daemonset -n kube-system kube-proxy # Check EBS CSI driver kubectl get deployment -n kube-system ebs-csi-controller ğŸ¯ EKS Control Plane Ready!\nControl Plane Status:\nâœ… Kubernetes 1.30 cluster ACTIVE âœ… Multi-AZ managed control plane âœ… All essential add-ons running âœ… CloudWatch logging enabled âœ… kubectl access configured Info: EKS control plane tá»± Ä‘á»™ng cháº¡y trÃªn nhiá»u AZs. Äá»ƒ tá»‘i Æ°u chi phÃ­, Ä‘áº£m báº£o worker nodes cÃ¢n báº±ng giá»¯a cÃ¡c AZs Ä‘á»ƒ trÃ¡nh phÃ­ cross-AZ data transfer ($0.01/GB).\n2. IRSA (IAM Roles for Service Accounts) Setup 2.1. Associate OIDC Provider # Check if OIDC provider exists aws eks describe-cluster --name mlops-retail-cluster \\ --region ap-southeast-1 \\ --query \u0026#34;cluster.identity.oidc.issuer\u0026#34; --output text # Associate OIDC identity provider with cluster eksctl utils associate-iam-oidc-provider \\ --cluster mlops-retail-cluster \\ --region ap-southeast-1 \\ --approve Expected output:\nâœ… Created OIDC identity provider for cluster \u0026#34;mlops-retail-cluster\u0026#34; in region \u0026#34;ap-southeast-1\u0026#34; 2.2. Create IRSA Role for S3 Access Create file scripts/create-irsa-s3-role.sh:\n#!/bin/bash # Configuration CLUSTER_NAME=\u0026#34;mlops-retail-cluster\u0026#34; REGION=\u0026#34;ap-southeast-1\u0026#34; NAMESPACE=\u0026#34;mlops-retail-forecast\u0026#34; SERVICE_ACCOUNT=\u0026#34;s3-access-sa\u0026#34; ROLE_NAME=\u0026#34;mlops-irsa-s3-access-role\u0026#34; POLICY_NAME=\u0026#34;mlops-irsa-s3-policy\u0026#34; # Get OIDC issuer URL OIDC_ISSUER=$(aws eks describe-cluster \\ --name $CLUSTER_NAME \\ --region $REGION \\ --query \u0026#34;cluster.identity.oidc.issuer\u0026#34; \\ --output text | sed \u0026#39;s|https://||\u0026#39;) # Get AWS account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) echo \u0026#34;ğŸ”§ Creating IRSA role for S3 access...\u0026#34; echo \u0026#34;Cluster: $CLUSTER_NAME\u0026#34; echo \u0026#34;OIDC Issuer: $OIDC_ISSUER\u0026#34; echo \u0026#34;Namespace: $NAMESPACE\u0026#34; echo \u0026#34;Service Account: $SERVICE_ACCOUNT\u0026#34; # Create trust policy cat \u0026gt; trust-policy.json \u0026lt;\u0026lt; EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ISSUER}\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;${OIDC_ISSUER}:sub\u0026#34;: \u0026#34;system:serviceaccount:${NAMESPACE}:${SERVICE_ACCOUNT}\u0026#34;, \u0026#34;${OIDC_ISSUER}:aud\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34; } } } ] } EOF # Create IAM role aws iam create-role \\ --role-name $ROLE_NAME \\ --assume-role-policy-document file://trust-policy.json \\ --description \u0026#34;IRSA role for EKS pods to access S3\u0026#34; # Create S3 access policy cat \u0026gt; s3-policy.json \u0026lt;\u0026lt; EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::mlops-retail-forecast-models\u0026#34;, \u0026#34;arn:aws:s3:::mlops-retail-forecast-models/*\u0026#34;, \u0026#34;arn:aws:s3:::mlops-retail-forecast-data\u0026#34;, \u0026#34;arn:aws:s3:::mlops-retail-forecast-data/*\u0026#34; ] } ] } EOF # Create and attach policy aws iam create-policy \\ --policy-name $POLICY_NAME \\ --policy-document file://s3-policy.json \\ --description \u0026#34;S3 access policy for ML workloads\u0026#34; aws iam attach-role-policy \\ --role-name $ROLE_NAME \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/${POLICY_NAME} # Clean up temp files rm trust-policy.json s3-policy.json echo \u0026#34;âœ… IRSA S3 role created successfully!\u0026#34; echo \u0026#34;Role ARN: arn:aws:iam::${ACCOUNT_ID}:role/${ROLE_NAME}\u0026#34; Run the script:\nchmod +x scripts/create-irsa-s3-role.sh ./scripts/create-irsa-s3-role.sh 2.3. Create IRSA Role for CloudWatch Create file scripts/create-irsa-cloudwatch-role.sh:\n#!/bin/bash # Configuration CLUSTER_NAME=\u0026#34;mlops-retail-cluster\u0026#34; REGION=\u0026#34;ap-southeast-1\u0026#34; NAMESPACE=\u0026#34;mlops-retail-forecast\u0026#34; SERVICE_ACCOUNT=\u0026#34;cloudwatch-sa\u0026#34; ROLE_NAME=\u0026#34;mlops-irsa-cloudwatch-role\u0026#34; # Get OIDC issuer and account ID OIDC_ISSUER=$(aws eks describe-cluster \\ --name $CLUSTER_NAME \\ --region $REGION \\ --query \u0026#34;cluster.identity.oidc.issuer\u0026#34; \\ --output text | sed \u0026#39;s|https://||\u0026#39;) ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) echo \u0026#34;ğŸ”§ Creating IRSA role for CloudWatch access...\u0026#34; # Create trust policy for CloudWatch service account cat \u0026gt; cloudwatch-trust-policy.json \u0026lt;\u0026lt; EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_ISSUER}\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;${OIDC_ISSUER}:sub\u0026#34;: \u0026#34;system:serviceaccount:${NAMESPACE}:${SERVICE_ACCOUNT}\u0026#34;, \u0026#34;${OIDC_ISSUER}:aud\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34; } } } ] } EOF # Create IAM role aws iam create-role \\ --role-name $ROLE_NAME \\ --assume-role-policy-document file://cloudwatch-trust-policy.json \\ --description \u0026#34;IRSA role for EKS pods to access CloudWatch\u0026#34; # Attach CloudWatch agent policy aws iam attach-role-policy \\ --role-name $ROLE_NAME \\ --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy # Clean up rm cloudwatch-trust-policy.json echo \u0026#34;âœ… IRSA CloudWatch role created successfully!\u0026#34; echo \u0026#34;Role ARN: arn:aws:iam::${ACCOUNT_ID}:role/${ROLE_NAME}\u0026#34; Run the script:\nchmod +x scripts/create-irsa-cloudwatch-role.sh ./scripts/create-irsa-cloudwatch-role.sh 2.4. Create Kubernetes Service Accounts Create file k8s/service-accounts.yaml:\n--- apiVersion: v1 kind: Namespace metadata: name: mlops-retail-forecast labels: name: mlops-retail-forecast --- apiVersion: v1 kind: ServiceAccount metadata: name: s3-access-sa namespace: mlops-retail-forecast annotations: eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/mlops-irsa-s3-access-role labels: app.kubernetes.io/name: s3-access-service-account app.kubernetes.io/component: rbac --- apiVersion: v1 kind: ServiceAccount metadata: name: cloudwatch-sa namespace: mlops-retail-forecast annotations: eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/mlops-irsa-cloudwatch-role labels: app.kubernetes.io/name: cloudwatch-service-account app.kubernetes.io/component: monitoring --- apiVersion: v1 kind: ServiceAccount metadata: name: retail-api-sa namespace: mlops-retail-forecast annotations: eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/mlops-irsa-s3-access-role labels: app.kubernetes.io/name: retail-api-service-account app.kubernetes.io/component: api Apply service accounts:\n# Replace ACCOUNT_ID with your AWS account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) sed \u0026#34;s/ACCOUNT_ID/$ACCOUNT_ID/g\u0026#34; k8s/service-accounts.yaml | kubectl apply -f - # Verify service accounts kubectl get serviceaccounts -n mlops-retail-forecast kubectl describe serviceaccount s3-access-sa -n mlops-retail-forecast ğŸ¯ IRSA Setup Complete!\nIRSA Components:\nâœ… OIDC identity provider associated with EKS âœ… S3 access role for ML workloads âœ… CloudWatch role for monitoring âœ… Kubernetes service accounts with proper annotations âœ… Secure AWS access without hardcoded credentials Tip: Sá»­ dá»¥ng service accounts riÃªng biá»‡t cho tá»«ng workload vá»›i minimal IAM policies. TrÃ¡nh chia sáº» service accounts giá»¯a cÃ¡c á»©ng dá»¥ng vÃ  thÆ°á»ng xuyÃªn audit IRSA role permissions.\n3. Managed Node Group Setup 3.1. Create Node Group via Console Navigate to Node Groups:\nEKS Console â†’ mlops-retail-cluster â†’ Compute â†’ Node groups â†’ \u0026ldquo;Add node group\u0026rdquo; Node Group Configuration:\nName: mlops-retail-nodegroup-t2micro\rNode IAM role: mlops-hybrid-eks-nodes-role (from Task 5) Instance Configuration:\nAMI type: Amazon Linux 2 (AL2_x86_64)\rCapacity type: On-Demand\rInstance types: t2.micro\rDisk size: 20 GB Scaling Configuration:\nDesired size: 2\rMinimum size: 1\rMaximum size: 4 Network Configuration:\nSubnets: Both private workload subnets\rConfigure remote access: Enable (optional)\rSSH key pair: Your EC2 key pair (optional)\rSource security groups: mlops-hybrid-eks-nodes-sg 3.2. Alternative: eksctl Node Group (Command Line) Create file scripts/create-nodegroup.sh:\n#!/bin/bash # Configuration CLUSTER_NAME=\u0026#34;mlops-retail-cluster\u0026#34; REGION=\u0026#34;ap-southeast-1\u0026#34; NODEGROUP_NAME=\u0026#34;mlops-retail-nodegroup-t2micro\u0026#34; echo \u0026#34;ğŸ”§ Creating EKS node group with t2.micro instances...\u0026#34; # Create node group using eksctl eksctl create nodegroup \\ --cluster=$CLUSTER_NAME \\ --region=$REGION \\ --name=$NODEGROUP_NAME \\ --node-type=t2.micro \\ --nodes=2 \\ --nodes-min=1 \\ --nodes-max=4 \\ --node-volume-size=20 \\ --node-volume-type=gp3 \\ --ssh-access \\ --ssh-public-key=YOUR_KEY_NAME \\ --managed \\ --node-private-networking \\ --node-zones=ap-southeast-1a,ap-southeast-1b echo \u0026#34;âœ… Node group creation initiated!\u0026#34; echo \u0026#34;Check status: eksctl get nodegroup --cluster $CLUSTER_NAME --region $REGION\u0026#34; 3.3. Verify Node Group # Check node group status aws eks describe-nodegroup \\ --cluster-name mlops-retail-cluster \\ --nodegroup-name mlops-retail-nodegroup-t2micro \\ --region ap-southeast-1 # Verify nodes in Kubernetes kubectl get nodes kubectl get nodes -o wide # Check node labels and annotations kubectl describe nodes Expected output:\nNAME STATUS ROLES AGE VERSION\rip-10-0-101-123.ap-southeast-1.compute.internal Ready \u0026lt;none\u0026gt; 5m v1.30.0-eks-xyz\rip-10-0-102-456.ap-southeast-1.compute.internal Ready \u0026lt;none\u0026gt; 5m v1.30.0-eks-xyz ğŸ¯ Node Group Ready!\nNode Group Status:\nâœ… 2x t2.micro instances (Free Tier) âœ… Private subnet deployment âœ… Auto Scaling Group configured (1-4 instances) âœ… All nodes in Ready state âœ… EKS optimized AMI with latest patches Warning (t2.micro): Giá»›i háº¡n 1 vCPU vÃ  1GB RAM. Theo dÃµi CPU credits vÃ  cÃ¢n nháº¯c giá»›i háº¡n burstable performance. Cho production ML workloads, sá»­ dá»¥ng t3.medium+ instances.\n4. Cost Optimization with VPC Endpoints 4.1. Review VPC Endpoints for EKS VPC Endpoints Created in Task 5:\n# Verify VPC endpoints exist for EKS cost optimization aws ec2 describe-vpc-endpoints \\ --filters \u0026#34;Name=vpc-id,Values=vpc-xxxxx\u0026#34; \\ --query \u0026#39;VpcEndpoints[?ServiceName==`com.amazonaws.ap-southeast-1.s3`]\u0026#39; aws ec2 describe-vpc-endpoints \\ --filters \u0026#34;Name=vpc-id,Values=vpc-xxxxx\u0026#34; \\ --query \u0026#39;VpcEndpoints[?ServiceName==`com.amazonaws.ap-southeast-1.ecr.api`]\u0026#39; Expected VPC Endpoints:\nâœ… S3 Gateway Endpoint (FREE - no data charges) âœ… ECR API Endpoint ($0.01/hour per AZ) âœ… ECR Docker Endpoint ($0.01/hour per AZ) âœ… CloudWatch Logs Endpoint ($0.01/hour per AZ) 4.2. Cost Savings with VPC Endpoints Data Transfer Without VPC Endpoints With VPC Endpoints Savings S3 Access $0.09/GB (NAT Gateway) $0.00/GB (Gateway) 100% ECR Pull $0.09/GB (Internet) $0.00/GB (Private) 100% CloudWatch $0.09/GB (NAT Gateway) $0.00/GB (Private) 100% ğŸ’° Monthly Cost Breakdown vá»›i t2.micro + VPC Endpoints:\nEKS Control Plane: $73.00/month (fixed)\nt2.micro Nodes: $0.00/month (FREE tier - 750 hours)\nVPC Endpoints: ~$7.20/month (4 endpoints Ã— 2 AZ Ã— $0.01/hour)\nData Transfer: $0.00/month (all private via VPC endpoints)\nTotal Monthly Cost: ~$80.20/month\nSavings vs NAT Gateway: ~$45/month (NAT Gateway cost)\nInfo: VPC Endpoints loáº¡i bá» internet routing cho AWS services nhÆ°ng tá»‘n $0.01/giá» per endpoint per AZ. Theo dÃµi usage patterns vÃ  consolidate endpoints khi cÃ³ thá»ƒ.\n5. Deploy Sample Application with IRSA 5.1. Deploy FastAPI Prediction Service Create file k8s/retail-api-deployment.yaml:\n--- apiVersion: apps/v1 kind: Deployment metadata: name: retail-api namespace: mlops-retail-forecast labels: app: retail-api spec: replicas: 2 selector: matchLabels: app: retail-api template: metadata: labels: app: retail-api spec: serviceAccountName: retail-api-sa # IRSA Service Account containers: - name: retail-api image: ACCOUNT_ID.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest ports: - containerPort: 8000 name: http env: - name: AWS_DEFAULT_REGION value: \u0026#34;ap-southeast-1\u0026#34; - name: S3_BUCKET_MODELS value: \u0026#34;mlops-retail-forecast-models\u0026#34; - name: S3_BUCKET_DATA value: \u0026#34;mlops-retail-forecast-data\u0026#34; resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; readinessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 30 periodSeconds: 10 livenessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 60 periodSeconds: 20 --- apiVersion: v1 kind: Service metadata: name: retail-api-service namespace: mlops-retail-forecast labels: app: retail-api spec: type: ClusterIP ports: - port: 80 targetPort: 8000 name: http selector: app: retail-api 5.2. Deploy Application with IRSA # Replace ACCOUNT_ID in deployment file ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) sed \u0026#34;s/ACCOUNT_ID/$ACCOUNT_ID/g\u0026#34; k8s/retail-api-deployment.yaml | kubectl apply -f - # Verify deployment kubectl get deployments -n mlops-retail-forecast kubectl get pods -n mlops-retail-forecast kubectl get services -n mlops-retail-forecast # Check logs to verify IRSA working kubectl logs -l app=retail-api -n mlops-retail-forecast 5.3. Test IRSA S3 Access Create test pod:\n# Create test pod to verify IRSA S3 access kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Pod metadata: name: test-irsa-s3 namespace: mlops-retail-forecast spec: serviceAccountName: s3-access-sa containers: - name: test-s3 image: amazon/aws-cli:latest command: [\u0026#34;/bin/sh\u0026#34;] args: [\u0026#34;-c\u0026#34;, \u0026#34;aws s3 ls s3://mlops-retail-forecast-models \u0026amp;\u0026amp; sleep 3600\u0026#34;] env: - name: AWS_DEFAULT_REGION value: \u0026#34;ap-southeast-1\u0026#34; restartPolicy: Never EOF # Check if S3 access works via IRSA kubectl logs test-irsa-s3 -n mlops-retail-forecast Expected output:\n2024-01-15 10:30:45 model-v1.0.pkl\r2024-01-15 10:31:20 model-v1.1.pkl\rPRE training-data/ 6. Cluster Verification 6.1. Control Plane Health Check # Check cluster status kubectl get --raw=\u0026#39;/readyz?verbose\u0026#39; # Check API server health kubectl get --raw=\u0026#39;/healthz\u0026#39; # View cluster events kubectl get events --sort-by=.metadata.creationTimestamp 6.2. Add-ons Verification # Check CoreDNS kubectl get pods -n kube-system -l k8s-app=kube-dns # Check kube-proxy kubectl get daemonset -n kube-system kube-proxy # Check VPC CNI kubectl get daemonset -n kube-system aws-node # Check EBS CSI driver kubectl get deployment -n kube-system ebs-csi-controller 6.3. RBAC and Permissions # Check current user permissions kubectl auth can-i \u0026#34;*\u0026#34; \u0026#34;*\u0026#34; # List available API resources kubectl api-resources # Check cluster roles kubectl get clusterroles | grep eks 6.4. IRSA Verification # Deploy service accounts vá»›i IRSA annotations kubectl apply -f aws/k8s/service-accounts.yaml # Deploy test pod vá»›i IRSA authentication kubectl apply -f aws/k8s/test-pod-irsa.yaml # Verify pod cÃ³ thá»ƒ access S3 qua IRSA (no AWS credentials needed!) kubectl exec -it test-irsa-s3-access -- aws s3 ls # Check IRSA role annotations kubectl get serviceaccount s3-access-sa -o yaml # Verify OIDC provider aws iam list-open-id-connect-providers # List IRSA roles aws iam list-roles --query \u0026#39;Roles[?contains(RoleName, `irsa`)].{RoleName:RoleName,CreateDate:CreateDate}\u0026#39; # Test CloudWatch access kubectl exec -it test-irsa-s3-access -- aws cloudwatch list-metrics --namespace \u0026#34;MLOps/RetailForecast\u0026#34; # Cleanup test pod kubectl delete pod test-irsa-s3-access 7. Monitoring vÃ  Logging 7.1. CloudWatch Integration # Check if cluster logging is enabled aws eks describe-cluster \\ --name mlops-retail-forecast-dev-cluster \\ --query \u0026#39;cluster.logging\u0026#39; # View control plane logs in CloudWatch aws logs describe-log-groups \\ --log-group-name-prefix /aws/eks/mlops-retail-forecast-dev-cluster 7.2. Setup CloudWatch Container Insights File: aws/k8s/cloudwatch-insights.yaml\napiVersion: v1 kind: Namespace metadata: name: amazon-cloudwatch labels: name: amazon-cloudwatch --- apiVersion: v1 kind: ServiceAccount metadata: name: cloudwatch-agent namespace: amazon-cloudwatch annotations: eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/mlops-retail-forecast-dev-irsa-cloudwatch --- apiVersion: apps/v1 kind: DaemonSet metadata: name: cloudwatch-agent namespace: amazon-cloudwatch spec: selector: matchLabels: name: cloudwatch-agent template: metadata: labels: name: cloudwatch-agent spec: serviceAccountName: cloudwatch-agent containers: - name: cloudwatch-agent image: amazon/cloudwatch-agent:1.300026.2b251814 env: - name: AWS_REGION value: ap-southeast-1 - name: CLUSTER_NAME value: mlops-retail-forecast-dev-cluster volumeMounts: - name: cwagentconfig mountPath: /etc/cwagentconfig - name: rootfs mountPath: /rootfs readOnly: true - name: dockersock mountPath: /var/run/docker.sock readOnly: true - name: varlibdocker mountPath: /var/lib/docker readOnly: true volumes: - name: cwagentconfig configMap: name: cwagentconfig - name: rootfs hostPath: path: / - name: dockersock hostPath: path: /var/run/docker.sock - name: varlibdocker hostPath: path: /var/lib/docker 8. Security Hardening 8.1. Network Security # Verify security groups aws ec2 describe-security-groups \\ --group-ids $(terraform output -raw eks_control_plane_security_group_id) \\ --query \u0026#39;SecurityGroups[0].{GroupId:GroupId,IpPermissions:IpPermissions}\u0026#39; # Check VPC configuration aws eks describe-cluster \\ --name mlops-retail-forecast-dev-cluster \\ --query \u0026#39;cluster.resourcesVpcConfig\u0026#39; 8.2. RBAC Configuration File: aws/k8s/rbac.yaml\napiVersion: v1 kind: ServiceAccount metadata: name: mlops-admin namespace: mlops-retail-forecast --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: mlops-retail-forecast name: mlops-admin-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;configmaps\u0026#34;, \u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;, \u0026#34;replicasets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: mlops-admin-binding namespace: mlops-retail-forecast subjects: - kind: ServiceAccount name: mlops-admin namespace: mlops-retail-forecast roleRef: kind: Role name: mlops-admin-role apiGroup: rbac.authorization.k8s.io 9. Dá»n dáº¹p Resources (AWS CLI) 9.1. XÃ³a Node Groups # List all node groups aws eks list-nodegroups --cluster-name mlops-retail-cluster --region ap-southeast-1 # Delete node group (this will take 5-10 minutes) aws eks delete-nodegroup \\ --cluster-name mlops-retail-cluster \\ --nodegroup-name mlops-retail-nodegroup-t2micro \\ --region ap-southeast-1 # Monitor deletion status aws eks describe-nodegroup \\ --cluster-name mlops-retail-cluster \\ --nodegroup-name mlops-retail-nodegroup-t2micro \\ --region ap-southeast-1 \\ --query \u0026#39;nodegroup.status\u0026#39; 9.2. XÃ³a IRSA Roles vÃ  OIDC Provider # Get account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # Detach and delete S3 access role aws iam detach-role-policy \\ --role-name mlops-irsa-s3-access-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/mlops-irsa-s3-policy aws iam delete-policy --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/mlops-irsa-s3-policy aws iam delete-role --role-name mlops-irsa-s3-access-role # Delete CloudWatch access role aws iam detach-role-policy \\ --role-name mlops-irsa-cloudwatch-role \\ --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy aws iam delete-role --role-name mlops-irsa-cloudwatch-role # Delete OIDC provider OIDC_URL=$(aws eks describe-cluster --name mlops-retail-cluster --region ap-southeast-1 --query \u0026#39;cluster.identity.oidc.issuer\u0026#39; --output text | sed \u0026#39;s|https://||\u0026#39;) aws iam delete-open-id-connect-provider --open-id-connect-provider-arn arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_URL} 9.3. XÃ³a EKS Cluster # Delete the cluster (ensure node groups are deleted first) aws eks delete-cluster --name mlops-retail-cluster --region ap-southeast-1 # Monitor deletion progress aws eks describe-cluster --name mlops-retail-cluster --region ap-southeast-1 --query \u0026#39;cluster.status\u0026#39; # Verify cluster is deleted (should return error) aws eks list-clusters --region ap-southeast-1 --query \u0026#39;clusters[?contains(@, `mlops-retail-cluster`)]\u0026#39; 9.4. Dá»n dáº¹p Kubernetes Resources # Delete namespace (this removes all resources in the namespace) kubectl delete namespace mlops-retail-forecast # Remove cluster from kubeconfig kubectl config delete-cluster arn:aws:eks:ap-southeast-1:${ACCOUNT_ID}:cluster/mlops-retail-cluster kubectl config delete-context arn:aws:eks:ap-southeast-1:${ACCOUNT_ID}:cluster/mlops-retail-cluster # Remove user from kubeconfig kubectl config unset users.arn:aws:eks:ap-southeast-1:${ACCOUNT_ID}:cluster/mlops-retail-cluster 9.5. Script Dá»n dáº¹p EKS Tá»± Ä‘á»™ng #!/bin/bash # eks-cleanup.sh CLUSTER_NAME=\u0026#34;mlops-retail-cluster\u0026#34; REGION=\u0026#34;ap-southeast-1\u0026#34; NODEGROUP_NAME=\u0026#34;mlops-retail-nodegroup-t2micro\u0026#34; echo \u0026#34;ğŸ§¹ Starting EKS cluster cleanup...\u0026#34; # 1. Delete applications and namespace echo \u0026#34;Deleting Kubernetes resources...\u0026#34; kubectl delete namespace mlops-retail-forecast --ignore-not-found=true # 2. Delete node group echo \u0026#34;Deleting node group...\u0026#34; aws eks delete-nodegroup \\ --cluster-name $CLUSTER_NAME \\ --nodegroup-name $NODEGROUP_NAME \\ --region $REGION # Wait for node group deletion echo \u0026#34;Waiting for node group deletion...\u0026#34; aws eks wait nodegroup-deleted \\ --cluster-name $CLUSTER_NAME \\ --nodegroup-name $NODEGROUP_NAME \\ --region $REGION # 3. Delete IRSA roles echo \u0026#34;Cleaning up IRSA roles...\u0026#34; ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # Delete S3 role aws iam detach-role-policy --role-name mlops-irsa-s3-access-role --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/mlops-irsa-s3-policy 2\u0026gt;/dev/null aws iam delete-policy --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/mlops-irsa-s3-policy 2\u0026gt;/dev/null aws iam delete-role --role-name mlops-irsa-s3-access-role 2\u0026gt;/dev/null # Delete CloudWatch role aws iam detach-role-policy --role-name mlops-irsa-cloudwatch-role --policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy 2\u0026gt;/dev/null aws iam delete-role --role-name mlops-irsa-cloudwatch-role 2\u0026gt;/dev/null # 4. Delete OIDC provider echo \u0026#34;Deleting OIDC provider...\u0026#34; OIDC_URL=$(aws eks describe-cluster --name $CLUSTER_NAME --region $REGION --query \u0026#39;cluster.identity.oidc.issuer\u0026#39; --output text 2\u0026gt;/dev/null | sed \u0026#39;s|https://||\u0026#39;) if [ ! -z \u0026#34;$OIDC_URL\u0026#34; ]; then aws iam delete-open-id-connect-provider --open-id-connect-provider-arn arn:aws:iam::${ACCOUNT_ID}:oidc-provider/${OIDC_URL} 2\u0026gt;/dev/null fi # 5. Delete cluster echo \u0026#34;Deleting EKS cluster...\u0026#34; aws eks delete-cluster --name $CLUSTER_NAME --region $REGION # Wait for cluster deletion echo \u0026#34;Waiting for cluster deletion (this may take 10-15 minutes)...\u0026#34; aws eks wait cluster-deleted --name $CLUSTER_NAME --region $REGION # 6. Clean up kubeconfig echo \u0026#34;Cleaning up kubeconfig...\u0026#34; kubectl config delete-cluster arn:aws:eks:$REGION:${ACCOUNT_ID}:cluster/$CLUSTER_NAME 2\u0026gt;/dev/null kubectl config delete-context arn:aws:eks:$REGION:${ACCOUNT_ID}:cluster/$CLUSTER_NAME 2\u0026gt;/dev/null kubectl config unset users.arn:aws:eks:$REGION:${ACCOUNT_ID}:cluster/$CLUSTER_NAME 2\u0026gt;/dev/null echo \u0026#34;âœ… EKS cleanup completed successfully!\u0026#34; 10. Báº£ng giÃ¡ EKS (ap-southeast-1) 10.1. Chi phÃ­ EKS Control Plane Component GiÃ¡ (USD/cluster/giá») GiÃ¡ (USD/cluster/thÃ¡ng) Ghi chÃº EKS Control Plane $0.10 $73.00 Chi phÃ­ cá»‘ Ä‘á»‹nh báº¥t ká»ƒ má»©c sá»­ dá»¥ng Fargate $0.04048/vCPU/giá» Biáº¿n Ä‘á»•i Tráº£ theo tÃ i nguyÃªn pod EC2 Worker Nodes GiÃ¡ EC2 Biáº¿n Ä‘á»•i t2.micro cÃ³ FREE tier 10.2. Chi phÃ­ EC2 Worker Nodes Loáº¡i Instance vCPU Memory GiÃ¡ (USD/giá») GiÃ¡ (USD/thÃ¡ng) Free Tier t2.micro 1 1GB $0.0116 $8.50 âœ… 750h/thÃ¡ng t3.micro 2 1GB $0.0104 $7.61 âŒ t3.small 2 2GB $0.0208 $15.22 âŒ t3.medium 2 4GB $0.0416 $30.45 âŒ m5.large 2 8GB $0.096 $70.27 âŒ 10.3. Chi phÃ­ VPC Endpoints cho EKS Loáº¡i Endpoint GiÃ¡ (USD/giá»/endpoint) Chi phÃ­ hÃ ng thÃ¡ng (2 AZ) Ghi chÃº S3 Gateway Miá»…n phÃ­ $0.00 KhÃ´ng phÃ­ theo giá» ECR API $0.01 $14.40 Báº¯t buá»™c cho image pulls ECR Docker $0.01 $14.40 Báº¯t buá»™c cho image pulls CloudWatch Logs $0.01 $14.40 TÃ¹y chá»n cho logging EC2 $0.01 $14.40 TÃ¹y chá»n cho instance metadata 10.4. Æ¯á»›c tÃ­nh chi phÃ­ cho Task 7 Cáº¥u hÃ¬nh EKS Cluster:\nControl Plane: 1 cluster Worker Nodes: 2x t2.micro instances VPC Endpoints: ECR API + ECR Docker Storage: 20GB EBS má»—i node Chi phÃ­ hÃ ng thÃ¡ng:\nComponent Cáº¥u hÃ¬nh Chi phÃ­ hÃ ng thÃ¡ng Giáº£m giÃ¡ Free Tier EKS Control Plane 1 cluster $73.00 âŒ t2.micro Instances 2x instances (1,500h) $17.00 -$17.00 (MIá»„N PHÃ) EBS Storage 40GB gp3 $3.20 -$3.20 (30GB MIá»„N PHÃ) ECR VPC Endpoints 2 endpoints Ã— 2 AZ $28.80 âŒ Data Transfer VPC ná»™i bá»™ $0.00 âœ… MIá»„N PHÃ Tá»•ng cá»™ng $122.00 -$20.20 Chi phÃ­ thá»±c táº¿ $101.80 10.5. So sÃ¡nh chi phÃ­ vá»›i cÃ¡c lá»±a chá»n khÃ¡c EKS vs Self-Managed Kubernetes:\nTÃ­nh nÄƒng EKS Self-Managed K8s Tiáº¿t kiá»‡m Control Plane $73/thÃ¡ng $0 (tá»± lÃ m) -$73 Quáº£n lÃ½ há»‡ thá»‘ng âœ… ÄÆ°á»£c quáº£n lÃ½ âŒ Cáº­p nháº­t thá»§ cÃ´ng Tiáº¿t kiá»‡m thá»i gian Báº£n vÃ¡ báº£o máº­t âœ… Tá»± Ä‘á»™ng âŒ Thá»§ cÃ´ng Báº£o máº­t Multi-AZ HA âœ… Sáºµn cÃ³ âŒ Thiáº¿t láº­p phá»©c táº¡p Äá»™ tin cáº­y TÃ­ch há»£p AWS âœ… Native âŒ Thá»§ cÃ´ng Dá»… sá»­ dá»¥ng EKS vs ECS Fargate:\nLoáº¡i workload Chi phÃ­ EKS Chi phÃ­ ECS Fargate Tháº¯ng APIs nhá» (0.25 vCPU) $73 + instance $7.30/thÃ¡ng Fargate Batch Jobs $73 + compute Tráº£ theo láº§n cháº¡y Fargate Dá»‹ch vá»¥ luÃ´n cháº¡y $73 + compute $29.20/thÃ¡ng EKS á»¨ng dá»¥ng nhiá»u service $73 + compute $N Ã— chi phÃ­ service EKS 10.6. Chi phÃ­ Data Transfer CÃ¡c tÃ¬nh huá»‘ng EKS Data Transfer:\nLoáº¡i Transfer Chi phÃ­ TrÆ°á»ng há»£p sá»­ dá»¥ng Pod to Pod (cÃ¹ng AZ) Miá»…n phÃ­ Giao tiáº¿p microservices Pod to Pod (khÃ¡c AZ) $0.01/GB Triá»ƒn khai multi-AZ Pod to Internet $0.12/GB API responses cho users Pod to S3 (VPC Endpoint) Miá»…n phÃ­ Truy cáº­p model/data Pod to S3 (Internet) $0.12/GB KhÃ´ng cÃ³ VPC endpoint 10.7. Chiáº¿n lÆ°á»£c tá»‘i Æ°u chi phÃ­ Right-sizing Instance:\n# Use Kubernetes resource requests/limits resources: requests: cpu: 100m # 0.1 vCPU memory: 128Mi # 128MB limits: cpu: 500m # 0.5 vCPU memory: 512Mi # 512MB Node Scaling:\n# Cluster Autoscaler configuration desired_size = 2 min_size = 1 max_size = 10 # Scale down quickly when not needed scale_down_delay_after_add = \u0026#34;2m\u0026#34; scale_down_unneeded_time = \u0026#34;2m\u0026#34; Spot Instances:\n# Mix spot vÃ  on-demand cho cost savings capacity_type = \u0026#34;SPOT\u0026#34; # 60-70% savings # hoáº·c capacity_type = \u0026#34;ON_DEMAND\u0026#34; # Stable pricing 10.8. Tá»‘i Æ°u hÃ³a Free Tier Lá»£i Ã­ch Free Tier 12 thÃ¡ng:\n750 giá»/thÃ¡ng t2.micro EC2 instances 30 GB EBS General Purpose (SSD) storage 2 triá»‡u requests tá»›i Lambda (náº¿u dÃ¹ng cho automation) 1 GB CloudWatch Logs (5GB Ä‘áº§u miá»…n phÃ­) Miá»…n phÃ­ vÄ©nh viá»…n:\n1 triá»‡u AWS Lambda requests má»—i thÃ¡ng 5 GB CloudWatch monitoring data S3 transfers trong cÃ¹ng region qua VPC endpoints ğŸ’° TÃ³m táº¯t chi phÃ­ cho Task 7:\nChi phÃ­ cá»‘ Ä‘á»‹nh: $73/thÃ¡ng (EKS control plane) Chi phÃ­ biáº¿n Ä‘á»•i: $28.80/thÃ¡ng (VPC endpoints) Tiáº¿t kiá»‡m Free Tier: $20.20/thÃ¡ng (instances + storage) Tá»•ng cá»™ng: $81.60/thÃ¡ng vá»›i free tier optimizations Má»Ÿ rá»™ng production: ThÃªm instance types dá»±a vÃ o workload needs Success tip: Theo dÃµi cluster costs vá»›i AWS Cost Explorer vÃ  thiáº¿t láº­p billing alerts. Sá»­ dá»¥ng eksctl hoáº·c Terraform cho infrastructure as code Ä‘á»ƒ Ä‘áº£m báº£o triá»ƒn khai consistent vÃ  reproducible.\nğŸ‘‰ Káº¿t quáº£ Task 7 Sau Task 7, báº¡n sáº½ cÃ³ EKS Cluster production-ready, cháº¡y hoÃ n toÃ n trong private subnet vÃ  tÃ­ch há»£p vá»›i VPC Endpoints tá»« Task 5, tiáº¿t kiá»‡m chi phÃ­ NAT Gateway vÃ  tÄƒng má»©c Ä‘á»™ báº£o máº­t.\nDeliverables Completed EKS Control Plane ACTIVE: Managed Kubernetes cluster vá»›i multi-AZ high availability IRSA Configured: OIDC provider vÃ  Service Account authentication setup Managed Node Groups: 2x t2.micro instances (FREE tier) tráº£i Ä‘á»u trÃªn â‰¥2 AZ VPC Endpoints Integration: Sá»­ dá»¥ng ECR, S3 endpoints tá»« Task 2 (70% cost savings) Core Add-ons: VPC CNI, CoreDNS, kube-proxy, metrics-server, EBS CSI driver Secure Pod Access: Pods cÃ³ thá»ƒ access S3/CloudWatch qua IRSA (no hardcoded credentials) kubectl Access: Local development environment configured vÃ  tested Cost Optimization: $117.40/month saved vá»›i FREE tier + VPC Endpoints ğŸ¯ Ready for Next Tasks:\nEKS cluster foundation Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ deploy:\nâœ… Task 5: EKS node groups scaling vÃ  optimization âœ… Task 6: ECR repository setup cho container images âœ… Task 7: Build vÃ  push inference API container âœ… Task 8: S3 data storage integration âœ… Task 13: Deploy inference API lÃªn EKS cluster ğŸ” LÆ°u Ã½ Báº£o máº­t \u0026amp; Báº£o trÃ¬:\nPublic Access: Giá»›i háº¡n cluster_endpoint_public_access_cidrs theo IP ranges thá»±c táº¿ trong production Logging: Báº­t táº¥t cáº£ control plane logging Ä‘á»ƒ audit báº£o máº­t Updates: ThÆ°á»ng xuyÃªn cáº­p nháº­t Kubernetes version vÃ  add-ons (hÃ ng quÃ½) RBAC: Triá»ƒn khai role-based access control phÃ¹ há»£p cho team members Monitoring: Thiáº¿t láº­p alerts cho node health, pod failures, vÃ  resource usage Backup: CÃ¢n nháº¯c EKS cluster backup strategy cho disaster recovery ğŸ¬ Video thá»±c hiá»‡n Task 7 Next Step: Task 08: Deploy Kubernetes\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/3-blogstranslated/3.7-blog7/",
	"title": "Giá»›i thiá»‡u AWS Infrastructure as Code MCP Server: Há»— trá»£ CDK vÃ  CloudFormation Ä‘Æ°á»£c há»— trá»£ bá»Ÿi AI",
	"tags": [],
	"description": "",
	"content": "TÃ¡c giáº£: Idriss Laouali Abdou â€” 28 NOV 2025\nTinh gá»n quÃ¡ trÃ¬nh phÃ¡t triá»ƒn háº¡ táº§ng AWS cá»§a báº¡n vá»›i tÃ¬m kiáº¿m tÃ i liá»‡u, xÃ¡c thá»±c vÃ  kháº¯c phá»¥c sá»± cá»‘ Ä‘Æ°á»£c há»— trá»£ bá»Ÿi AI\nGiá»›i thiá»‡u HÃ´m nay, chÃºng tÃ´i ráº¥t vui má»«ng giá»›i thiá»‡u AWS Infrastructure-as-Code (IaC) MCP Server, má»™t cÃ´ng cá»¥ má»›i giÃºp thu háº¹p khoáº£ng cÃ¡ch giá»¯a cÃ¡c trá»£ lÃ½ AI vÃ  quy trÃ¬nh phÃ¡t triá»ƒn háº¡ táº§ng AWS cá»§a báº¡n. ÄÆ°á»£c xÃ¢y dá»±ng trÃªn Model Context Protocol (MCP), mÃ¡y chá»§ nÃ y cho phÃ©p cÃ¡c trá»£ lÃ½ AI nhÆ° Kiro CLI, Claude hoáº·c Cursor giÃºp báº¡n tÃ¬m kiáº¿m tÃ i liá»‡u AWS CloudFormation vÃ  Cloud Development Kit (CDK), xÃ¡c thá»±c template, kháº¯c phá»¥c sá»± cá»‘ triá»ƒn khai vÃ  tuÃ¢n theo cÃ¡c best practices â€“ Ä‘á»“ng thá»i váº«n duy trÃ¬ tÃ­nh báº£o máº­t cá»§a viá»‡c thá»±c thi cá»¥c bá»™ (local execution).\nDÃ¹ báº¡n Ä‘ang viáº¿t AWS CloudFormation templates hay mÃ£ AWS Cloud Development Kit (CDK), IaC MCP Server Ä‘Ã³ng vai trÃ² nhÆ° má»™t ngÆ°á»i báº¡n Ä‘á»“ng hÃ nh thÃ´ng minh, hiá»ƒu nhu cáº§u vá» háº¡ táº§ng cá»§a báº¡n vÃ  cung cáº¥p há»— trá»£ theo ngá»¯ cáº£nh xuyÃªn suá»‘t vÃ²ng Ä‘á»i phÃ¡t triá»ƒn cá»§a báº¡n.\nModel Context Protocol (MCP) lÃ  má»™t tiÃªu chuáº©n má»Ÿ cho phÃ©p cÃ¡c trá»£ lÃ½ AI káº¿t ná»‘i an toÃ n vá»›i cÃ¡c nguá»“n dá»¯ liá»‡u vÃ  cÃ´ng cá»¥ bÃªn ngoÃ i. HÃ£y hÃ¬nh dung nÃ³ nhÆ° má»™t â€œbá»™ chuyá»ƒn Ä‘á»•iâ€ phá»• quÃ¡t cho phÃ©p cÃ¡c mÃ´ hÃ¬nh AI tÆ°Æ¡ng tÃ¡c vá»›i cÃ¡c cÃ´ng cá»¥ phÃ¡t triá»ƒn cá»§a báº¡n trong khi váº«n giá»¯ cÃ¡c thao tÃ¡c nháº¡y cáº£m á»Ÿ cá»¥c bá»™ vÃ  dÆ°á»›i sá»± kiá»ƒm soÃ¡t cá»§a báº¡n.\nIaC MCP Server cung cáº¥p chÃ­n cÃ´ng cá»¥ chuyÃªn biá»‡t, Ä‘Æ°á»£c tá»• chá»©c thÃ nh hai nhÃ³m:\nCÃ´ng cá»¥ tÃ¬m kiáº¿m tÃ i liá»‡u tá»« xa CÃ¡c cÃ´ng cá»¥ nÃ y káº¿t ná»‘i Ä‘áº¿n AWS Knowledge MCP backend Ä‘á»ƒ truy xuáº¥t thÃ´ng tin phÃ¹ há»£p, cáº­p nháº­t:\nsearch_cdk_documentation\nTÃ¬m kiáº¿m cÆ¡ sá»Ÿ kiáº¿n thá»©c AWS CDK cho APIs, khÃ¡i niá»‡m vÃ  hÆ°á»›ng dáº«n triá»ƒn khai. search_cdk_samples_and_constructs\nKhÃ¡m phÃ¡ cÃ¡c AWS CDK constructs vÃ  patterns dá»±ng sáºµn tá»« AWS Construct Library. search_cloudformation_documentation\nTruy váº¥n tÃ i liá»‡u CloudFormation cho cÃ¡c loáº¡i tÃ i nguyÃªn, thuá»™c tÃ­nh vÃ  cÃ¡c intrinsic functions. read_iac_documentation_page\nTruy xuáº¥t vÃ  Ä‘á»c Ä‘áº§y Ä‘á»§ cÃ¡c trang tÃ i liá»‡u CloudFormation vÃ  CDK Ä‘Æ°á»£c tráº£ vá» tá»« cÃ¡c tÃ¬m kiáº¿m hoáº·c tá»« cÃ¡c URL Ä‘Æ°á»£c cung cáº¥p. CÃ´ng cá»¥ xÃ¡c thá»±c vÃ  kháº¯c phá»¥c sá»± cá»‘ cá»¥c bá»™ CÃ¡c cÃ´ng cá»¥ nÃ y cháº¡y hoÃ n toÃ n trÃªn mÃ¡y cá»§a báº¡n\ncdk_best_practices\nTruy cáº­p má»™t bá»™ sÆ°u táº­p Ä‘Æ°á»£c tuyá»ƒn chá»n cÃ¡c best practices vÃ  nguyÃªn táº¯c thiáº¿t káº¿ cá»§a AWS CDK. validate_cloudformation_template\nThá»±c hiá»‡n kiá»ƒm tra cÃº phÃ¡p vÃ  schema báº±ng cfn-lint Ä‘á»ƒ phÃ¡t hiá»‡n lá»—i trÆ°á»›c khi triá»ƒn khai. check_cloudformation_template_compliance\nCháº¡y kiá»ƒm tra báº£o máº­t vÃ  tuÃ¢n thá»§ Ä‘á»‘i vá»›i templates cá»§a báº¡n báº±ng AWS Guard rules vÃ  cfn-guard. troubleshoot_cloudformation_deployment\nPhÃ¢n tÃ­ch cÃ¡c lá»—i triá»ƒn khai CloudFormation stack vá»›i phÃ¢n tÃ­ch sá»± kiá»‡n CloudTrail Ä‘Æ°á»£c tÃ­ch há»£p. CÃ´ng cá»¥ nÃ y sáº½ sá»­ dá»¥ng thÃ´ng tin xÃ¡c thá»±c AWS cá»§a báº¡n Ä‘á»ƒ phÃ¢n tÃ­ch tráº¡ng thÃ¡i stack. get_cloudformation_pre_deploy_validation_instructions\nTráº£ vá» hÆ°á»›ng dáº«n cho tÃ­nh nÄƒng xÃ¡c thá»±c trÆ°á»›c triá»ƒn khai (pre-deployment validation) cá»§a CloudFormation, tÃ­nh nÄƒng nÃ y xÃ¡c thá»±c templates trong quÃ¡ trÃ¬nh táº¡o change set. CÃ¡c trÆ°á»ng há»£p sá»­ dá»¥ng chÃ­nh 1. Trá»£ lÃ½ tÃ i liá»‡u thÃ´ng minh Thay vÃ¬ tá»± tay tÃ¬m kiáº¿m qua tÃ i liá»‡u, hÃ£y há»i trá»£ lÃ½ AI cá»§a báº¡n báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn:\nâ€œLÃ m tháº¿ nÃ o Ä‘á»ƒ táº¡o má»™t S3 bucket vá»›i báº­t mÃ£ hÃ³a (encryption) trong CDK?â€\nMÃ¡y chá»§ sáº½ tÃ¬m kiáº¿m CDK best practice vÃ  samples, tráº£ vá» cÃ¡c vÃ­ dá»¥ mÃ£ vÃ  giáº£i thÃ­ch liÃªn quan.\n2. XÃ¡c thá»±c template chá»§ Ä‘á»™ng TrÆ°á»›c khi triá»ƒn khai cÃ¡c thay Ä‘á»•i háº¡ táº§ng:\nUser: â€œValidate my CloudFormation template and check for security issuesâ€\nAI Agent: [Uses validate_cloudformation_template and check_cloudformation_template_compliance]\nâ€œFound 2 issues: Missing encryption on EBS volumes,\nand S3 bucket lacks public access block configurationâ€\n3. Kháº¯c phá»¥c sá»± cá»‘ triá»ƒn khai nhanh Khi má»™t stack triá»ƒn khai tháº¥t báº¡i:\nUser: â€œMy stack â€˜stack_03â€™ in us-east-1 failed to deploy. What happened?â€\nAI Agent: [Uses troubleshoot_stack_deployment with CloudTrail integration]\nâ€œThe deployment failed due to insufficient IAM permissions.\nCloudTrail shows AccessDenied for ec2:CreateVpc.\nYou need to add VPC permissions to your deployment role.â€\n4. Há»c táº­p vÃ  khÃ¡m phÃ¡ Má»›i lÃ m quen vá»›i AWS CDK? MÃ¡y chá»§ giÃºp báº¡n khÃ¡m phÃ¡ constructs vÃ  patterns:\nUser: â€œShow me how to build a serverless APIâ€\nAI Agent: [Searches CDK constructs and samples]\nâ€œHere are three approaches using API Gateway + Lambdaâ€¦â€\nKiáº¿n trÃºc vÃ  báº£o máº­t Thiáº¿t káº¿ báº£o máº­t Thá»±c thi cá»¥c bá»™ (Local Execution): MCP server cháº¡y hoÃ n toÃ n trÃªn mÃ¡y cá»¥c bá»™ cá»§a báº¡n báº±ng uv (trÃ¬nh quáº£n lÃ½ gÃ³i Python nhanh). KhÃ´ng cÃ³ mÃ£ hoáº·c templates nÃ o Ä‘Æ°á»£c gá»­i Ä‘áº¿n cÃ¡c dá»‹ch vá»¥ bÃªn ngoÃ i, ngoáº¡i trá»« cÃ¡c truy váº¥n tÃ¬m kiáº¿m tÃ i liá»‡u. ThÃ´ng tin xÃ¡c thá»±c AWS (AWS Credentials): MÃ¡y chá»§ sá»­ dá»¥ng thÃ´ng tin xÃ¡c thá»±c AWS hiá»‡n cÃ³ cá»§a báº¡n (tá»« ~/.aws/credentials, biáº¿n mÃ´i trÆ°á»ng, hoáº·c IAM roles) Ä‘á»ƒ truy cáº­p cÃ¡c CloudFormation vÃ  CloudTrail APIs. Äiá»u nÃ y tuÃ¢n theo cÃ¹ng mÃ´ hÃ¬nh báº£o máº­t nhÆ° AWS CLI. Giao tiáº¿p stdio: MÃ¡y chá»§ giao tiáº¿p vá»›i cÃ¡c trá»£ lÃ½ AI qua standard input/output (stdio), khÃ´ng má»Ÿ báº¥t ká»³ network ports nÃ o. Quyá»n tá»‘i thiá»ƒu (Minimal Permissions): Äá»ƒ cÃ³ Ä‘áº§y Ä‘á»§ chá»©c nÄƒng, mÃ¡y chá»§ cáº§n quyá»n chá»‰ Ä‘á»c (read-only) Ä‘á»‘i vá»›i CloudFormation stacks vÃ  CloudTrail eventsâ€”khÃ´ng cáº§n quyá»n ghi (write) cho cÃ¡c quy trÃ¬nh xÃ¡c thá»±c vÃ  kháº¯c phá»¥c sá»± cá»‘. Báº¯t Ä‘áº§u Äiá»u kiá»‡n tiÃªn quyáº¿t Python 3.10 hoáº·c má»›i hÆ¡n uv package manager AWS credentials Ä‘Æ°á»£c cáº¥u hÃ¬nh cá»¥c bá»™ MCP-compatible AI client (vÃ­ dá»¥: Kiro CLI, Claude Desktop) Cáº¥u hÃ¬nh Cáº¥u hÃ¬nh MCP server trong cáº¥u hÃ¬nh MCP client cá»§a báº¡n. Vá»›i bÃ i blog nÃ y, chÃºng tÃ´i sáº½ táº­p trung vÃ o Kiro CLI. Chá»‰nh sá»­a .kiro/settings/mcp.json):\n{ \u0026#34;mcpServers\u0026#34;: { \u0026#34;awslabs.aws-iac-mcp-server\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;uvx\u0026#34;, \u0026#34;args\u0026#34;: [\u0026#34;awslabs.aws-iac-mcp-server@latest\u0026#34;], \u0026#34;env\u0026#34;: { \u0026#34;AWS_PROFILE\u0026#34;: \u0026#34;your-named-profile\u0026#34;, \u0026#34;FASTMCP_LOG_LEVEL\u0026#34;: \u0026#34;ERROR\u0026#34; }, \u0026#34;disabled\u0026#34;: false, \u0026#34;autoApprove\u0026#34;: [] } } } JSON\nCÃ¡c cÃ¢n nháº¯c vá» báº£o máº­t Privacy Notice: MCP server nÃ y thá»±c thi cÃ¡c lá»‡nh gá»i AWS API báº±ng thÃ´ng tin xÃ¡c thá»±c cá»§a báº¡n vÃ  chia sáº» dá»¯ liá»‡u pháº£n há»“i vá»›i nhÃ  cung cáº¥p mÃ´ hÃ¬nh AI bÃªn thá»© ba cá»§a báº¡n (vÃ­ dá»¥: Amazon Q, Claude Desktop, Cursor, VS Code). NgÆ°á»i dÃ¹ng chá»‹u trÃ¡ch nhiá»‡m hiá»ƒu cÃ¡c thá»±c hÃ nh xá»­ lÃ½ dá»¯ liá»‡u cá»§a nhÃ  cung cáº¥p AI cá»§a báº¡n vÃ  Ä‘áº£m báº£o tuÃ¢n thá»§ cÃ¡c yÃªu cáº§u báº£o máº­t vÃ  quyá»n riÃªng tÆ° cá»§a tá»• chá»©c báº¡n khi sá»­ dá»¥ng cÃ´ng cá»¥ nÃ y vá»›i cÃ¡c tÃ i nguyÃªn AWS.\nQuyá»n IAM MCP server yÃªu cáº§u cÃ¡c quyá»n AWS sau:\nCho xÃ¡c thá»±c template vÃ  tuÃ¢n thá»§:\nKhÃ´ng yÃªu cáº§u quyá»n AWS (chá»‰ xÃ¡c thá»±c cá»¥c bá»™) Cho kháº¯c phá»¥c sá»± cá»‘ triá»ƒn khai:\ncloudformation:DescribeStacks cloudformation:DescribeStackEvents cloudformation:DescribeStackResources cloudtrail:LookupEvents (cho CloudTrail deep links) VÃ­ dá»¥ IAM policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:DescribeStacks\u0026#34;, \u0026#34;cloudformation:DescribeStackEvents\u0026#34;, \u0026#34;cloudformation:DescribeStackResources\u0026#34;, \u0026#34;cloudtrail:LookupEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } JSON\nVÃ­ dá»¥ trÆ°á»ng há»£p sá»­ dá»¥ng vá»›i Kiro CLI IMPORTANT: Äáº£m báº£o báº¡n Ä‘Ã£ Ä‘Ã¡p á»©ng táº¥t cáº£ cÃ¡c Ä‘iá»u kiá»‡n tiÃªn quyáº¿t trÆ°á»›c khi thá»­ cÃ¡c lá»‡nh nÃ y.\nKhi tá»‡p mcp.json Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p Ä‘Ãºng, hÃ£y thá»­ cháº¡y má»™t prompt máº«u. Trong terminal cá»§a báº¡n, cháº¡y kiro-cli chat Ä‘á»ƒ báº¯t Ä‘áº§u sá»­ dá»¥ng Kiro-cli trong CLI. Scenarios:\nâ€œWhat are the CDK best practices for Lambda functions?â€ â€œSearch for CDK samples that use DynamoDB with Lambdaâ€ â€œValidate my CloudFormation template at ./template.yamlâ€ â€œCheck if my template complies with security best practicesâ€ Thá»±c tiá»…n tá»‘t nháº¥t Báº¯t Ä‘áº§u vá»›i tÃ¬m kiáº¿m tÃ i liá»‡u: TrÆ°á»›c khi viáº¿t mÃ£, hÃ£y tÃ¬m kiáº¿m cÃ¡c constructs vÃ  patterns sáºµn cÃ³ XÃ¡c thá»±c sá»›m vÃ  thÆ°á»ng xuyÃªn: Cháº¡y cÃ¡c cÃ´ng cá»¥ xÃ¡c thá»±c trÆ°á»›c khi cá»‘ gáº¯ng triá»ƒn khai Kiá»ƒm tra tuÃ¢n thá»§: DÃ¹ng check_template_compliance Ä‘á»ƒ phÃ¡t hiá»‡n cÃ¡c váº¥n Ä‘á» báº£o máº­t trong quÃ¡ trÃ¬nh phÃ¡t triá»ƒn Táº­n dá»¥ng CloudTrail: Khi kháº¯c phá»¥c sá»± cá»‘, tÃ­ch há»£p CloudTrail cung cáº¥p ngá»¯ cáº£nh tháº¥t báº¡i chi tiáº¿t TuÃ¢n theo CDK Best Practices: DÃ¹ng cÃ´ng cá»¥ cdk_best_practices Ä‘á»ƒ bÃ¡m theo cÃ¡c khuyáº¿n nghá»‹ cá»§a AWS Tiáº¿p theo lÃ  gÃ¬? IAC MCP Server Ä‘áº¡i diá»‡n cho má»™t mÃ´ hÃ¬nh má»›i trong quy trÃ¬nh lÃ m viá»‡c AI agentic cho phÃ¡t triá»ƒn háº¡ táº§ng â€“ nÆ¡i cÃ¡c trá»£ lÃ½ AI hiá»ƒu cÃ´ng cá»¥ cá»§a báº¡n, giÃºp báº¡n Ä‘iá»u hÆ°á»›ng tÃ i liá»‡u phá»©c táº¡p vÃ  cung cáº¥p há»— trá»£ thÃ´ng minh xuyÃªn suá»‘t vÃ²ng Ä‘á»i phÃ¡t triá»ƒn.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.6-week6/",
	"title": "Tuáº§n 6 - Dá»‹ch vá»¥ cÆ¡ sá»Ÿ dá»¯ liá»‡u trÃªn AWS",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 6: Triá»ƒn khai vÃ  quáº£n lÃ½ CSDL quan há»‡ (RDS) vÃ  NoSQL (DynamoDB). XÃ¢y dá»±ng chiáº¿n lÆ°á»£c migrate cÆ¡ sá»Ÿ dá»¯ liá»‡u. CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - RDS: Khá»Ÿi táº¡o RDS (PostgreSQL/MySQL) trong private subnets.\n- Cáº¥u hÃ¬nh Multi-AZ vÃ  Automated Backups. AWS RDS Docs 3 - Truy cáº­p: Káº¿t ná»‘i RDS an toÃ n qua SSM/Bastion.\n- Theo dÃµi báº±ng Performance Insights. AWS Docs 4 - DynamoDB: Thiáº¿t káº¿ báº£ng (Partition/Sort Keys, GSIs).\n- Test CRUD operations vÃ  cáº¥u hÃ¬nh TTL. DynamoDB Guide 5 - Migration (Tuá»³ chá»n): Test AWS DMS (Database Migration Service).\n- So sÃ¡nh hiá»‡u nÄƒng RDS vs DynamoDB. AWS DMS Docs 6 - Ã”n táº­p: XÃ¡c minh failover vÃ  khÃ´i phá»¥c tá»« backup.\n- BÃ¡o cÃ¡o tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 6: Triá»ƒn khai RDS Multi-AZ an toÃ n kÃ¨m automated backups. Thiáº¿t káº¿ DynamoDB tá»‘i Æ°u vá»›i keys vÃ  indexes phÃ¹ há»£p. Thiáº¿t láº­p truy cáº­p CSDL an toÃ n, khÃ´ng public ra Internet. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/3-blogstranslated/3.8-blog8/",
	"title": "AWS Clean Rooms ra máº¯t tÃ­nh nÄƒng táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh ML",
	"tags": [],
	"description": "",
	"content": "TÃ¡c giáº£: Micah Walter â€” 30 NOV 2025\nHÃ´m nay, chÃºng tÃ´i cÃ´ng bá»‘ tÃ­nh nÄƒng táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° cho AWS Clean Rooms, má»™t kháº£ nÄƒng má»›i mÃ  cÃ¡c tá»• chá»©c vÃ  Ä‘á»‘i tÃ¡c cá»§a há» cÃ³ thá»ƒ sá»­ dá»¥ng Ä‘á»ƒ táº¡o cÃ¡c bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° tá»« dá»¯ liá»‡u chung cá»§a há» nháº±m huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh mÃ¡y há»c (ML) há»“i quy vÃ  phÃ¢n loáº¡i. Báº¡n cÃ³ thá»ƒ sá»­ dá»¥ng tÃ­nh nÄƒng nÃ y Ä‘á»ƒ táº¡o cÃ¡c bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n tá»•ng há»£p váº«n giá»¯ láº¡i cÃ¡c máº«u thá»‘ng kÃª cá»§a dá»¯ liá»‡u gá»‘c, mÃ  mÃ´ hÃ¬nh khÃ´ng cáº§n truy cáº­p vÃ o cÃ¡c báº£n ghi gá»‘c, má»Ÿ ra nhá»¯ng cÆ¡ há»™i má»›i cho viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»‘n trÆ°á»›c Ä‘Ã¢y khÃ´ng thá»ƒ thá»±c hiá»‡n do lo ngáº¡i vá» quyá»n riÃªng tÆ°.\nKhi xÃ¢y dá»±ng mÃ´ hÃ¬nh ML, cÃ¡c nhÃ  khoa há»c dá»¯ liá»‡u vÃ  nhÃ  phÃ¢n tÃ­ch thÆ°á»ng Ä‘á»‘i máº·t vá»›i má»™t mÃ¢u thuáº«n ná»n táº£ng giá»¯a tÃ­nh há»¯u Ã­ch cá»§a dá»¯ liá»‡u vÃ  báº£o vá»‡ quyá»n riÃªng tÆ°. Viá»‡c truy cáº­p dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao, chi tiáº¿t lÃ  Ä‘iá»u thiáº¿t yáº¿u Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh chÃ­nh xÃ¡c cÃ³ thá»ƒ nháº­n diá»‡n xu hÆ°á»›ng, cÃ¡ nhÃ¢n hÃ³a tráº£i nghiá»‡m vÃ  thÃºc Ä‘áº©y káº¿t quáº£ kinh doanh. Tuy nhiÃªn, viá»‡c sá»­ dá»¥ng dá»¯ liá»‡u chi tiáº¿t nhÆ° dá»¯ liá»‡u sá»± kiá»‡n á»Ÿ má»©c ngÆ°á»i dÃ¹ng tá»« nhiá»u bÃªn lÃ m phÃ¡t sinh cÃ¡c lo ngáº¡i lá»›n vá» quyá»n riÃªng tÆ° vÃ  cÃ¡c thÃ¡ch thá»©c tuÃ¢n thá»§. CÃ¡c tá»• chá»©c muá»‘n tráº£ lá»i nhá»¯ng cÃ¢u há»i nhÆ°, â€œNhá»¯ng Ä‘áº·c Ä‘iá»ƒm nÃ o cho tháº¥y kháº£ nÄƒng chuyá»ƒn Ä‘á»•i khÃ¡ch hÃ ng cao?â€, nhÆ°ng viá»‡c huáº¥n luyá»‡n dá»±a trÃªn cÃ¡c tÃ­n hiá»‡u á»Ÿ má»©c cÃ¡ nhÃ¢n thÆ°á»ng mÃ¢u thuáº«n vá»›i chÃ­nh sÃ¡ch quyá»n riÃªng tÆ° vÃ  cÃ¡c yÃªu cáº§u phÃ¡p lÃ½.\nTáº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° cho ML tÃ¹y chá»‰nh Äá»ƒ giáº£i quyáº¿t thÃ¡ch thá»©c nÃ y, chÃºng tÃ´i giá»›i thiá»‡u tÃ­nh nÄƒng táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° trong AWS Clean Rooms ML, cho phÃ©p cÃ¡c tá»• chá»©c táº¡o ra cÃ¡c phiÃªn báº£n tá»•ng há»£p cá»§a cÃ¡c bá»™ dá»¯ liá»‡u nháº¡y cáº£m Ä‘á»ƒ cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng an toÃ n hÆ¡n cho viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh ML. Kháº£ nÄƒng nÃ y sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t ML tiÃªn tiáº¿n Ä‘á»ƒ táº¡o ra cÃ¡c bá»™ dá»¯ liá»‡u má»›i duy trÃ¬ cÃ¡c thuá»™c tÃ­nh thá»‘ng kÃª cá»§a dá»¯ liá»‡u gá»‘c trong khi loáº¡i bá» Ä‘á»‹nh danh (de-identifying) cÃ¡c Ä‘á»‘i tÆ°á»£ng tá»« dá»¯ liá»‡u nguá»“n ban Ä‘áº§u.\nCÃ¡c ká»¹ thuáº­t áº©n danh truyá»n thá»‘ng nhÆ° che (masking) váº«n mang rá»§i ro tÃ¡i nháº­n diá»‡n cÃ¡ nhÃ¢n trong má»™t bá»™ dá»¯ liá»‡uâ€”biáº¿t cÃ¡c thuá»™c tÃ­nh vá» má»™t ngÆ°á»i nhÆ° mÃ£ bÆ°u chÃ­nh vÃ  ngÃ y sinh cÃ³ thá»ƒ Ä‘á»§ Ä‘á»ƒ xÃ¡c Ä‘á»‹nh há» khi Ä‘á»‘i chiáº¿u vá»›i dá»¯ liá»‡u Ä‘iá»u tra dÃ¢n sá»‘. TÃ­nh nÄƒng táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° giáº£i quyáº¿t rá»§i ro nÃ y báº±ng má»™t cÃ¡ch tiáº¿p cáº­n khÃ¡c vá» báº£n cháº¥t. Há»‡ thá»‘ng huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh há»c cÃ¡c máº«u thá»‘ng kÃª cá»‘t lÃµi cá»§a bá»™ dá»¯ liá»‡u gá»‘c, sau Ä‘Ã³ táº¡o cÃ¡c báº£n ghi tá»•ng há»£p báº±ng cÃ¡ch láº¥y máº«u cÃ¡c giÃ¡ trá»‹ tá»« bá»™ dá»¯ liá»‡u gá»‘c vÃ  dÃ¹ng mÃ´ hÃ¬nh Ä‘á»ƒ dá»± Ä‘oÃ¡n cá»™t giÃ¡ trá»‹ Ä‘Æ°á»£c dá»± Ä‘oÃ¡n (predicted value column). Thay vÃ¬ chá»‰ sao chÃ©p hoáº·c lÃ m nhiá»…u dá»¯ liá»‡u gá»‘c, há»‡ thá»‘ng sá»­ dá»¥ng má»™t ká»¹ thuáº­t giáº£m nÄƒng lá»±c mÃ´ hÃ¬nh (model capacity reduction) Ä‘á»ƒ giáº£m thiá»ƒu rá»§i ro mÃ´ hÃ¬nh ghi nhá»› thÃ´ng tin vá» cÃ¡c cÃ¡ nhÃ¢n trong dá»¯ liá»‡u huáº¥n luyá»‡n. Bá»™ dá»¯ liá»‡u tá»•ng há»£p thu Ä‘Æ°á»£c cÃ³ cÃ¹ng schema vÃ  Ä‘áº·c Ä‘iá»ƒm thá»‘ng kÃª nhÆ° dá»¯ liá»‡u gá»‘c, khiáº¿n nÃ³ phÃ¹ há»£p Ä‘á»ƒ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i vÃ  há»“i quy. CÃ¡ch tiáº¿p cáº­n nÃ y giáº£m Ä‘á»‹nh lÆ°á»£ng (quantifiably) rá»§i ro tÃ¡i nháº­n diá»‡n.\nCÃ¡c tá»• chá»©c sá»­ dá»¥ng kháº£ nÄƒng nÃ y cÃ³ quyá»n kiá»ƒm soÃ¡t cÃ¡c tham sá»‘ quyá»n riÃªng tÆ°, bao gá»“m lÆ°á»£ng nhiá»…u (noise) Ä‘Æ°á»£c Ã¡p dá»¥ng vÃ  má»©c báº£o vá»‡ chá»‘ng láº¡i cÃ¡c cuá»™c táº¥n cÃ´ng suy luáº­n thÃ nh viÃªn (membership inference attacks), trong Ä‘Ã³ káº» Ä‘á»‘i Ä‘á»‹ch cá»‘ gáº¯ng xÃ¡c Ä‘á»‹nh liá»‡u dá»¯ liá»‡u cá»§a má»™t cÃ¡ nhÃ¢n cá»¥ thá»ƒ cÃ³ Ä‘Æ°á»£c Ä‘Æ°a vÃ o táº­p huáº¥n luyá»‡n hay khÃ´ng. Sau khi táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p, AWS Clean Rooms cung cáº¥p cÃ¡c chá»‰ sá»‘ chi tiáº¿t Ä‘á»ƒ giÃºp khÃ¡ch hÃ ng vÃ  cÃ¡c nhÃ³m tuÃ¢n thá»§ cá»§a há» hiá»ƒu cháº¥t lÆ°á»£ng cá»§a bá»™ dá»¯ liá»‡u tá»•ng há»£p trÃªn hai chiá»u quan trá»ng: Ä‘á»™ trung thá»±c so vá»›i dá»¯ liá»‡u gá»‘c vÃ  má»©c báº£o toÃ n quyá»n riÃªng tÆ°. Äiá»ƒm Ä‘á»™ trung thá»±c (fidelity score) sá»­ dá»¥ng KL-divergence Ä‘á»ƒ Ä‘o má»©c Ä‘á»™ giá»‘ng nhau giá»¯a dá»¯ liá»‡u tá»•ng há»£p vÃ  bá»™ dá»¯ liá»‡u gá»‘c, vÃ  Ä‘iá»ƒm quyá»n riÃªng tÆ° (privacy score) Ä‘á»‹nh lÆ°á»£ng má»©c Ä‘á»™ bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c báº£o vá»‡ trÆ°á»›c cÃ¡c cuá»™c táº¥n cÃ´ng suy luáº­n thÃ nh viÃªn.\nLÃ m viá»‡c vá»›i dá»¯ liá»‡u tá»•ng há»£p trong AWS Clean Rooms Viá»‡c báº¯t Ä‘áº§u vá»›i tÃ­nh nÄƒng táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° tuÃ¢n theo quy trÃ¬nh lÃ m viá»‡c custom models cá»§a AWS Clean Rooms ML hiá»‡n cÃ³, vá»›i cÃ¡c bÆ°á»›c má»›i Ä‘á»ƒ chá»‰ Ä‘á»‹nh yÃªu cáº§u quyá»n riÃªng tÆ° vÃ  xem cÃ¡c chá»‰ sá»‘ cháº¥t lÆ°á»£ng. CÃ¡c tá»• chá»©c báº¯t Ä‘áº§u báº±ng cÃ¡ch táº¡o cÃ¡c báº£ng Ä‘Ã£ cáº¥u hÃ¬nh (configured tables) vá»›i cÃ¡c quy táº¯c phÃ¢n tÃ­ch (analysis rules) báº±ng cÃ¡c nguá»“n dá»¯ liá»‡u Æ°a thÃ­ch cá»§a há», sau Ä‘Ã³ join hoáº·c táº¡o má»™t collaboration vá»›i cÃ¡c Ä‘á»‘i tÃ¡c cá»§a há» vÃ  liÃªn káº¿t (associate) cÃ¡c báº£ng cá»§a há» vá»›i collaboration Ä‘Ã³.\nKháº£ nÄƒng má»›i giá»›i thiá»‡u má»™t máº«u phÃ¢n tÃ­ch (analysis template) nÃ¢ng cao, nÆ¡i cÃ¡c chá»§ sá»Ÿ há»¯u dá»¯ liá»‡u khÃ´ng chá»‰ xÃ¡c Ä‘á»‹nh truy váº¥n SQL táº¡o bá»™ dá»¯ liá»‡u mÃ  cÃ²n chá»‰ Ä‘á»‹nh ráº±ng bá»™ dá»¯ liá»‡u káº¿t quáº£ pháº£i lÃ  tá»•ng há»£p. Trong máº«u nÃ y, cÃ¡c tá»• chá»©c phÃ¢n loáº¡i cÃ¡c cá»™t Ä‘á»ƒ chá»‰ ra cá»™t nÃ o mÃ´ hÃ¬nh ML sáº½ dá»± Ä‘oÃ¡n vÃ  cá»™t nÃ o chá»©a giÃ¡ trá»‹ dáº¡ng phÃ¢n loáº¡i (categorical) so vá»›i dáº¡ng sá»‘ (numerical). Má»™t Ä‘iá»ƒm quan trá»ng lÃ  máº«u cÅ©ng bao gá»“m cÃ¡c ngÆ°á»¡ng quyá»n riÃªng tÆ° mÃ  dá»¯ liá»‡u tá»•ng há»£p Ä‘Æ°á»£c táº¡o ra pháº£i Ä‘Ã¡p á»©ng Ä‘á»ƒ cÃ³ thá»ƒ Ä‘Æ°á»£c cung cáº¥p cho viá»‡c huáº¥n luyá»‡n. CÃ¡c ngÆ°á»¡ng nÃ y bao gá»“m má»™t giÃ¡ trá»‹ epsilon chá»‰ Ä‘á»‹nh lÆ°á»£ng nhiá»…u cáº§n cÃ³ trong dá»¯ liá»‡u tá»•ng há»£p Ä‘á»ƒ báº£o vá»‡ chá»‘ng tÃ¡i nháº­n diá»‡n, vÃ  má»™t Ä‘iá»ƒm báº£o vá»‡ tá»‘i thiá»ƒu chá»‘ng láº¡i cÃ¡c cuá»™c táº¥n cÃ´ng suy luáº­n thÃ nh viÃªn. Viá»‡c Ä‘áº·t cÃ¡c ngÆ°á»¡ng nÃ y má»™t cÃ¡ch phÃ¹ há»£p Ä‘Ã²i há»i pháº£i hiá»ƒu cÃ¡c yÃªu cáº§u quyá»n riÃªng tÆ° vÃ  tuÃ¢n thá»§ cá»¥ thá»ƒ cá»§a tá»• chá»©c báº¡n, vÃ  chÃºng tÃ´i khuyáº¿n nghá»‹ phá»‘i há»£p vá»›i cÃ¡c nhÃ³m phÃ¡p lÃ½ vÃ  tuÃ¢n thá»§ trong quÃ¡ trÃ¬nh nÃ y.\nSau khi táº¥t cáº£ cÃ¡c chá»§ sá»Ÿ há»¯u dá»¯ liá»‡u xem xÃ©t vÃ  phÃª duyá»‡t máº«u phÃ¢n tÃ­ch, má»™t thÃ nh viÃªn collaboration táº¡o má»™t kÃªnh Ä‘áº§u vÃ o mÃ¡y há»c (machine learning input channel) tham chiáº¿u Ä‘áº¿n máº«u Ä‘Ã³. AWS Clean Rooms sau Ä‘Ã³ báº¯t Ä‘áº§u quy trÃ¬nh táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p, thÆ°á»ng hoÃ n táº¥t trong vÃ i giá» tÃ¹y thuá»™c vÃ o kÃ­ch thÆ°á»›c vÃ  Ä‘á»™ phá»©c táº¡p cá»§a bá»™ dá»¯ liá»‡u. Náº¿u bá»™ dá»¯ liá»‡u tá»•ng há»£p Ä‘Æ°á»£c táº¡o ra Ä‘Ã¡p á»©ng cÃ¡c ngÆ°á»¡ng quyá»n riÃªng tÆ° báº¯t buá»™c Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trong máº«u phÃ¢n tÃ­ch, má»™t kÃªnh Ä‘áº§u vÃ o mÃ¡y há»c tá»•ng há»£p (synthetic machine learning input channel) sáº½ trá»Ÿ nÃªn kháº£ dá»¥ng cÃ¹ng vá»›i cÃ¡c chá»‰ sá»‘ cháº¥t lÆ°á»£ng chi tiáº¿t. CÃ¡c nhÃ  khoa há»c dá»¯ liá»‡u cÃ³ thá»ƒ xem láº¡i Ä‘iá»ƒm báº£o vá»‡ thá»±c táº¿ Ä‘áº¡t Ä‘Æ°á»£c trÆ°á»›c má»™t cuá»™c táº¥n cÃ´ng suy luáº­n thÃ nh viÃªn mÃ´ phá»ng.\nKhi Ä‘Ã£ hÃ i lÃ²ng vá»›i cÃ¡c chá»‰ sá»‘ cháº¥t lÆ°á»£ng, cÃ¡c tá»• chá»©c cÃ³ thá»ƒ tiáº¿p tá»¥c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ML cá»§a há» báº±ng bá»™ dá»¯ liá»‡u tá»•ng há»£p trong collaboration cá»§a AWS Clean Rooms. TÃ¹y thuá»™c vÃ o trÆ°á»ng há»£p sá»­ dá»¥ng, há» cÃ³ thá»ƒ xuáº¥t cÃ¡c trá»ng sá»‘ mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n hoáº·c tiáº¿p tá»¥c cháº¡y cÃ¡c tÃ¡c vá»¥ suy luáº­n (inference jobs) ngay trong chÃ­nh collaboration.\nHÃ£y thá»­ xem Khi táº¡o má»™t collaboration AWS Clean Rooms má»›i, giá» Ä‘Ã¢y tÃ´i cÃ³ thá»ƒ Ä‘áº·t ai lÃ  ngÆ°á»i tráº£ tiá»n cho viá»‡c táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p. Sau khi Collaboration cá»§a tÃ´i Ä‘Æ°á»£c cáº¥u hÃ¬nh, tÃ´i cÃ³ thá»ƒ chá»n Require analysis template output to be synthetic khi táº¡o má»™t analysis template má»›i. Sau khi máº«u phÃ¢n tÃ­ch tá»•ng há»£p cá»§a tÃ´i sáºµn sÃ ng, tÃ´i cÃ³ thá»ƒ dÃ¹ng nÃ³ khi cháº¡y cÃ¡c truy váº¥n Ä‘Æ°á»£c báº£o vá»‡ (protected queries) vÃ  xem táº¥t cáº£ cÃ¡c chi tiáº¿t kÃªnh Ä‘áº§u vÃ o ML liÃªn quan. Clean Rooms Synthetic Data Console\nHiá»‡n Ä‘Ã£ cÃ³ sáºµn Báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u sá»­ dá»¥ng tÃ­nh nÄƒng táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° thÃ´ng qua AWS Clean Rooms ngay hÃ´m nay. TÃ­nh nÄƒng nÃ y cÃ³ sáºµn á»Ÿ táº¥t cáº£ cÃ¡c AWS Regions thÆ°Æ¡ng máº¡i nÆ¡i AWS Clean Rooms cÃ³ sáºµn. TÃ¬m hiá»ƒu thÃªm trong tÃ i liá»‡u AWS Clean Rooms.\nTÃ­nh nÄƒng táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° Ä‘Æ°á»£c tÃ­nh phÃ­ riÃªng dá»±a trÃªn má»©c sá»­ dá»¥ng. Báº¡n chá»‰ tráº£ tiá»n cho pháº§n compute Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p cá»§a báº¡n, Ä‘Æ°á»£c tÃ­nh phÃ­ theo Synthetic Data Generation Units (SDGUs). Sá»‘ lÆ°á»£ng SDGUs thay Ä‘á»•i tÃ¹y theo kÃ­ch thÆ°á»›c vÃ  Ä‘á»™ phá»©c táº¡p cá»§a bá»™ dá»¯ liá»‡u gá»‘c. Khoáº£n phÃ­ nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c cáº¥u hÃ¬nh nhÆ° má»™t thiáº¿t láº­p payer, nghÄ©a lÃ  báº¥t ká»³ thÃ nh viÃªn collaboration nÃ o cÅ©ng cÃ³ thá»ƒ Ä‘á»“ng Ã½ tráº£ chi phÃ­. Äá»ƒ biáº¿t thÃªm thÃ´ng tin vá» giÃ¡, hÃ£y tham kháº£o trang giÃ¡ AWS Clean Rooms.\nBáº£n phÃ¡t hÃ nh ban Ä‘áº§u há»— trá»£ huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i vÃ  há»“i quy trÃªn dá»¯ liá»‡u dáº¡ng báº£ng (tabular). CÃ¡c bá»™ dá»¯ liá»‡u tá»•ng há»£p hoáº¡t Ä‘á»™ng vá»›i cÃ¡c framework ML tiÃªu chuáº©n vÃ  cÃ³ thá»ƒ tÃ­ch há»£p vÃ o cÃ¡c pipeline phÃ¡t triá»ƒn mÃ´ hÃ¬nh hiá»‡n cÃ³ mÃ  khÃ´ng yÃªu cáº§u thay Ä‘á»•i quy trÃ¬nh lÃ m viá»‡c cá»§a báº¡n.\nKháº£ nÄƒng nÃ y Ä‘áº¡i diá»‡n cho má»™t bÆ°á»›c tiáº¿n Ä‘Ã¡ng ká»ƒ trong mÃ¡y há»c tÄƒng cÆ°á»ng quyá»n riÃªng tÆ°. CÃ¡c tá»• chá»©c cÃ³ thá»ƒ khai phÃ¡ giÃ¡ trá»‹ cá»§a dá»¯ liá»‡u nháº¡y cáº£m á»Ÿ má»©c ngÆ°á»i dÃ¹ng cho viá»‡c huáº¥n luyá»‡n mÃ´ hÃ¬nh trong khi giáº£m thiá»ƒu rá»§i ro thÃ´ng tin nháº¡y cáº£m vá» tá»«ng ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ bá»‹ rÃ² rá»‰. DÃ¹ báº¡n Ä‘ang tá»‘i Æ°u hÃ³a cÃ¡c chiáº¿n dá»‹ch quáº£ng cÃ¡o, cÃ¡ nhÃ¢n hÃ³a bÃ¡o giÃ¡ báº£o hiá»ƒm hay nÃ¢ng cao cÃ¡c há»‡ thá»‘ng phÃ¡t hiá»‡n gian láº­n, tÃ­nh nÄƒng táº¡o bá»™ dá»¯ liá»‡u tá»•ng há»£p tÄƒng cÆ°á»ng quyá»n riÃªng tÆ° giÃºp viá»‡c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh chÃ­nh xÃ¡c hÆ¡n thÃ´ng qua há»£p tÃ¡c dá»¯ liá»‡u trá»Ÿ nÃªn kháº£ thi, Ä‘á»“ng thá»i tÃ´n trá»ng quyá»n riÃªng tÆ° cÃ¡ nhÃ¢n.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.7-week7/",
	"title": "Tuáº§n 7 - Dá»‹ch vá»¥ Data Lake trÃªn AWS",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 7: XÃ¢y dá»±ng Data Lake serverless trÃªn S3. Triá»ƒn khai pipeline ETL vá»›i Glue vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u báº±ng Athena. CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - Data Lake: Thiáº¿t káº¿ cÃ¡c lá»›p S3 (Raw, Staging, Curated).\n- XÃ¢y dá»±ng chiáº¿n lÆ°á»£c partitioning. Task 3 Guide 3 - Glue Catalog: Thiáº¿t láº­p Glue Crawlers Ä‘á»ƒ tá»± Ä‘á»™ng phÃ¡t hiá»‡n schema.\n- XÃ¢y dá»±ng Glue Data Catalog. AWS Glue Docs 4 - ETL: Táº¡o Glue Jobs (Visual/Spark) Ä‘á»ƒ chuyá»ƒn CSV sang Parquet.\n- Triá»ƒn khai data partitioning. AWS Glue Studio 5 - PhÃ¢n tÃ­ch (Analytics): Query dá»¯ liá»‡u báº±ng Amazon Athena.\n- Trá»±c quan hoÃ¡ káº¿t quáº£ vá»›i QuickSight (tuá»³ chá»n). AWS Athena Docs 6 - Ã”n táº­p: Kiá»ƒm tra hiá»‡u quáº£ pipeline vÃ  hiá»‡u nÄƒng truy váº¥n.\n- BÃ¡o cÃ¡o tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 7: Thiáº¿t láº­p kiáº¿n trÃºc Data Lake nhiá»u lá»›p (multi-layer). Tá»‘i Æ°u lÆ°u trá»¯ vÃ  hiá»‡u nÄƒng truy váº¥n báº±ng Ä‘á»‹nh dáº¡ng Parquet. Cho phÃ©p truy váº¥n SQL linh hoáº¡t (ad-hoc) trá»±c tiáº¿p trÃªn dá»¯ liá»‡u S3 báº±ng Athena. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/8-deploy-kubernetes/",
	"title": "API Deployment on EKS",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Má»¥c tiÃªu Task 9:\nTriá»ƒn khai Retail Prediction API (FastAPI) lÃªn EKS Cluster, káº¿t ná»‘i model tá»« S3 vÃ  expose endpoint public qua Load Balancer (ALB).\nâ†’ Äáº£m báº£o dá»‹ch vá»¥ cháº¡y á»•n Ä‘á»‹nh, tá»± Ä‘á»™ng scale, báº£o máº­t, vÃ  cÃ³ thá»ƒ demo API tháº­t.\nğŸ“¥ Input tá»« cÃ¡c Task trÆ°á»›c:\nTask 5 (Production VPC): VPC design, subnets, VPC Endpoints and ALB networking required for cluster and load balancer Task 6 (ECR Container Registry): Container images and repository URIs to deploy Task 2 (IAM Roles \u0026amp; Audit): IRSA roles and policies for Pods to access S3 and other AWS services Task 7 (EKS Cluster): EKS cluster and node groups where manifests will be applied 1. Tá»•ng quan API Deployment lÃ  bÆ°á»›c triá»ƒn khai service dá»± Ä‘oÃ¡n Ä‘Ã£ Ä‘Æ°á»£c container hÃ³a lÃªn Kubernetes (EKS). BÆ°á»›c nÃ y Ä‘áº£m báº£o á»©ng dá»¥ng Ä‘Æ°á»£c triá»ƒn khai theo kiáº¿n trÃºc microservice, tá»± Ä‘á»™ng scale vÃ  cÃ³ tÃ­nh sáºµn sÃ ng cao.\nKiáº¿n trÃºc triá»ƒn khai EKS Deployment Architecture:\nClient â†’ ALB â†’ EKS Service â†’ API Pods â†’ S3 Models\râ†“\rAuto-scaling (HPA) Components:\nNamespace: mlops ServiceAccount: IRSA cho SageMaker access Deployment: API pods vá»›i ECR Singapore image Service: LoadBalancer service HPA: Auto-scaling dá»±a trÃªn CPU 2. Kubernetes Manifests Cáº§n táº¡o 5 file chÃ­nh:\nnamespace.yaml - Táº¡o namespace mlops serviceaccount.yaml - IRSA service account deployment.yaml - API application vá»›i SageMaker Registry service.yaml - LoadBalancer service hpa.yaml - Auto-scaling 2.1 Namespace Configuration # namespace.yaml apiVersion: v1 kind: Namespace metadata: name: mlops labels: app.kubernetes.io/name: retail-api --- 2.2 ServiceAccount vá»›i IRSA # serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: retail-api-sa namespace: mlops annotations: eks.amazonaws.com/role-arn: arn:aws:iam::842676018087:role/eks-sagemaker-access-role --- 2.3 Deployment Configuration # deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: retail-api namespace: mlops labels: app: retail-api spec: replicas: 2 selector: matchLabels: app: retail-api template: metadata: labels: app: retail-api spec: serviceAccountName: retail-api-sa containers: - name: retail-api image: 842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:latest ports: - containerPort: 8000 env: - name: PORT value: \u0026#34;8000\u0026#34; - name: AWS_DEFAULT_REGION value: \u0026#34;ap-southeast-1\u0026#34; - name: MODEL_PACKAGE_GROUP value: \u0026#34;retail-price-sensitivity-models\u0026#34; resources: requests: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; livenessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /health port: 8000 initialDelaySeconds: 10 periodSeconds: 5 --- 3. Service (Load Balancer) # service.yaml apiVersion: v1 kind: Service metadata: name: retail-api-service namespace: mlops labels: app: retail-api spec: selector: app: retail-api ports: - name: http port: 80 targetPort: 8000 type: LoadBalancer 4. Auto-scaling (HPA) # hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: retail-api-hpa namespace: mlops spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: retail-api minReplicas: 2 maxReplicas: 5 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 60 5. Deploy to EKS 5.1 Apply Manifests # Deploy all manifests in order kubectl apply -f namespace.yaml kubectl apply -f serviceaccount.yaml kubectl apply -f deployment.yaml kubectl apply -f service.yaml kubectl apply -f hpa.yaml 5.2 Kiá»ƒm tra Tráº¡ng thÃ¡i Deployment # Kiá»ƒm tra tráº¡ng thÃ¡i pods kubectl get pods -n mlops # Kiá»ƒm tra service vÃ  load balancer kubectl get svc -n mlops # Kiá»ƒm tra horizontal pod autoscaler kubectl get hpa -n mlops # Kiá»ƒm tra logs cá»§a pod kubectl logs -l app=retail-api -n mlops --tail=50 5.3 Láº¥y LoadBalancer URL vÃ  Test API # Láº¥y URL cá»§a LoadBalancer kubectl get svc retail-api-service -n mlops -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39; # Test health check endpoint curl http://[LOAD_BALANCER_URL]/health # Test API documentation curl http://[LOAD_BALANCER_URL]/docs # Test prediction endpoint vá»›i data format tháº­t curl -X POST http://[LOAD_BALANCER_URL]/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;BASKET_SIZE\u0026#34;: \u0026#34;M\u0026#34;, \u0026#34;BASKET_TYPE\u0026#34;: \u0026#34;MIXED\u0026#34;, \u0026#34;STORE_REGION\u0026#34;: \u0026#34;LONDON\u0026#34;, \u0026#34;STORE_FORMAT\u0026#34;: \u0026#34;LS\u0026#34;, \u0026#34;SPEND\u0026#34;: 125.50, \u0026#34;QUANTITY\u0026#34;: 3, \u0026#34;PROD_CODE_20\u0026#34;: \u0026#34;FOOD\u0026#34;, \u0026#34;PROD_CODE_30\u0026#34;: \u0026#34;FRESH\u0026#34; }\u0026#39; 6. Kiá»ƒm tra qua AWS Console 6.1 EKS Console - Kiá»ƒm tra Cluster Status Truy cáº­p EKS Console: AWS Console â†’ EKS â†’ Clusters â†’ mlops-retail-cluster Kiá»ƒm tra Resources Tab: mlops-retail-cluster â†’ Resources â†’ All namespaces â†’ Filter: mlops 6.2 EKS Workloads - Chi tiáº¿t Deployment Kiá»ƒm tra Deployment: Resources â†’ Deployments â†’ retail-api Kiá»ƒm tra Pods: Click vÃ o Deployment â†’ Pods tab Pod status: Running (náº¿u Pending thÃ¬ cÃ³ váº¥n Ä‘á» vá» resources) Restart count: 0 (náº¿u \u0026gt; 0 thÃ¬ cÃ³ crash) 6.3 Debug khi Pods Pending Náº¿u Pods á»Ÿ tráº¡ng thÃ¡i Pending:\nCheck Events section Ä‘á»ƒ xem lá»—i: Insufficient CPU/Memory: Cáº§n scale nodes Image pull error: ECR permissions issue PodSecurityPolicy: IAM role issue Náº¿u LoadBalancer timeout/connection refused:\nTarget Groups unhealthy: Pods chÆ°a pass health check (/health endpoint) Security Groups: EKS worker nodes pháº£i allow inbound tá»« Load Balancer Subnets: Load Balancer cáº§n Ã­t nháº¥t 2 public subnets Kiá»ƒm tra Events trong EKS Console:\nResources â†’ Events â†’ Filter namespace: mlops TÃ¬m Warning/Error events liÃªn quan Ä‘áº¿n deployment 7. Testing vÃ  Load Testing 7.1 Local Testing vá»›i Port Forward # Port forward service Ä‘áº¿n localhost (náº¿u LoadBalancer chÆ°a ready) kubectl port-forward service/retail-api-service 8080:80 -n mlops # Test qua port forward curl http://localhost:8080/health 7.2 Test SageMaker Model Registry Integration # Kiá»ƒm tra model info endpoint curl http://[LOAD_BALANCER_URL]/model/info # Kiá»ƒm tra model metrics tá»« SageMaker Registry curl http://[LOAD_BALANCER_URL]/model/metrics # Expected response: Accuracy 84.7%, F1-Score 83.2% tá»« Registry 7.3 Load Testing Ä‘á»ƒ Test Auto-scaling # Load test vá»›i data format Ä‘Ãºng for i in {1..100}; do curl -X POST http://[LOAD_BALANCER_URL]/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;BASKET_SIZE\u0026#34;:\u0026#34;M\u0026#34;,\u0026#34;BASKET_TYPE\u0026#34;:\u0026#34;MIXED\u0026#34;,\u0026#34;STORE_REGION\u0026#34;:\u0026#34;LONDON\u0026#34;,\u0026#34;STORE_FORMAT\u0026#34;:\u0026#34;LS\u0026#34;,\u0026#34;SPEND\u0026#34;:125.50,\u0026#34;QUANTITY\u0026#34;:3,\u0026#34;PROD_CODE_20\u0026#34;:\u0026#34;FOOD\u0026#34;,\u0026#34;PROD_CODE_30\u0026#34;:\u0026#34;FRESH\u0026#34;}\u0026#39; \u0026amp; done # Theo dÃµi HPA scaling kubectl get hpa retail-api-hpa -n mlops -w # Theo dÃµi pods Ä‘Æ°á»£c scale up (tá»« 2 â†’ max 5) kubectl get pods -n mlops -w 8. Chi phÃ­ Æ°á»›c tÃ­nh ThÃ nh pháº§n Æ¯á»›c tÃ­nh Ghi chÃº EKS Pod (2 replica Spot node) ~0.012 USD/h Chi phÃ­ compute ALB/NLB (public) ~0.02 USD/h Chá»‰ báº­t khi demo Tá»•ng (1h demo) â‰ˆ 0.03â€“0.04 USD Cá»±c tháº¥p náº¿u táº¯t ngay sau demo Chi phÃ­ tÃ­nh toÃ¡n dá»±a trÃªn Spot instances t3.medium vÃ  NLB táº¡i region ap-southeast-1. Chi phÃ­ thá»±c táº¿ cÃ³ thá»ƒ thay Ä‘á»•i tÃ¹y theo cáº¥u hÃ¬nh vÃ  thá»i gian sá»­ dá»¥ng.\nğŸ¯ Task 9 Complete - API Deployment on EKS\nKubernetes manifests ready EKS deployment configured vá»›i IRSA Load Balancer service cho external access Auto-scaling vá»›i HPA 9. Clean Up Resources 9.1 XÃ³a Deployment vÃ  Resources # XÃ³a táº¥t cáº£ resources trong namespace mlops kubectl delete namespace mlops # Hoáº·c xÃ³a tá»«ng resource riÃªng láº» kubectl delete deployment retail-api -n mlops kubectl delete service retail-api-service -n mlops kubectl delete hpa retail-api-hpa -n mlops kubectl delete serviceaccount retail-api-sa -n mlops # Kiá»ƒm tra LoadBalancer Ä‘Ã£ bá»‹ xÃ³a aws elbv2 describe-load-balancers --query \u0026#39;LoadBalancers[?contains(LoadBalancerName, `k8s-mlops`)].LoadBalancerArn\u0026#39; 9.2 XÃ³a ECR Images (Optional) # List images trong repository aws ecr describe-images --repository-name mlops/retail-api --region ap-southeast-1 # XÃ³a specific image tag aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --image-ids imageTag=v3 \\ --region ap-southeast-1 # XÃ³a táº¥t cáº£ images aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --image-ids \u0026#34;$(aws ecr describe-images --repository-name mlops/retail-api --region ap-southeast-1 --query \u0026#39;imageDetails[].imageDigest\u0026#39; --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | sed \u0026#39;s/.*/imageDigest=\u0026amp;/\u0026#39;)\u0026#34; \\ --region ap-southeast-1 9.3 Kiá»ƒm tra Clean Up # Kiá»ƒm tra khÃ´ng cÃ²n pods nÃ o kubectl get pods -n mlops # Kiá»ƒm tra khÃ´ng cÃ²n services nÃ o kubectl get svc -n mlops # Kiá»ƒm tra LoadBalancer Ä‘Ã£ bá»‹ terminate aws elbv2 describe-load-balancers --query \u0026#39;LoadBalancers[?contains(LoadBalancerName, `k8s-mlops`)]\u0026#39; 10. Báº£ng giÃ¡ Kubernetes Deployment (ap-southeast-1) 10.1. Chi phÃ­ Pod Resources Resource Type Request Limit Cost Impact CPU 250m 500m ~25% of node CPU Memory 512Mi 1Gi ~25% of node memory Storage (EBS) - - From EBS pricing Vá»›i t2.micro node (1 vCPU, 1GB RAM):\n1 API pod sá»­ dá»¥ng ~50% resources CÃ³ thá»ƒ cháº¡y 2 pods vá»›i resource requests Scaling bá»‹ giá»›i háº¡n bá»Ÿi node capacity 10.2. Chi phÃ­ Load Balancer Load Balancer Type GiÃ¡ (USD/hour) GiÃ¡ (USD/month) Data Processing Classic LB $0.025 $18.25 $0.008/GB Application LB $0.0225 $16.43 $0.008/LCU-hour Network LB $0.0225 $16.43 $0.006/NLCU-hour 10.3. Chi phÃ­ Service Types Service Type AWS Resource Monthly Cost Use Case ClusterIP None $0 Internal communication NodePort EC2 Security Groups $0 Development testing LoadBalancer ELB/ALB/NLB $16.43+ Production external access ExternalName None $0 External service mapping 10.4. Auto-scaling Costs Horizontal Pod Autoscaler (HPA):\nHPA controller: Free (part of EKS) Additional pods: EC2 instance costs Scaling triggers: CPU/Memory metrics (free) Cluster Autoscaler:\nController: Free New nodes: Full EC2 instance pricing Scale-down: Automatic cost reduction 10.5. Æ¯á»›c tÃ­nh chi phÃ­ Task 8 Basic Deployment (2 replicas):\nComponent Quantity Resource Usage Monthly Cost API Pods 2 replicas 500m CPU, 1Gi RAM Included in node cost LoadBalancer Service 1 ALB Base + LCU usage $16.43 + usage HPA 1 autoscaler Controller only $0 Ingress Optional Same ALB $0 additional Total ~$16.43 + LCU With Auto-scaling (2-5 replicas):\nScenario Pods Node Requirements Additional Cost Low load 2 pods 2x t2.micro (free) $0 Medium load 3-4 pods 1x t3.small $15.18 High load 5 pods 1x t3.medium $30.37 10.6. Data Transfer Costs Transfer Type Cost Use Case Pod-to-Pod (same AZ) Free Internal communication Pod-to-Pod (cross-AZ) $0.01/GB Multi-AZ deployment LoadBalancer to Internet $0.12/GB API responses to clients VPC Endpoints Free S3/ECR access 10.7. Storage Costs cho Persistent Volumes # Example PVC for model storage apiVersion: v1 kind: PersistentVolumeClaim metadata: name: model-storage namespace: mlops spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: gp3 Storage pricing:\n10GB gp3: $0.80/month Snapshots: $0.50/month (10GB) IOPS (if \u0026gt; 3000): $0.065/IOPS/month 10.8. Cost Optimization cho Deployments Resource Right-sizing:\nresources: requests: memory: \u0026#34;256Mi\u0026#34; # Start smaller cpu: \u0026#34;100m\u0026#34; # Minimal CPU request limits: memory: \u0026#34;512Mi\u0026#34; # Reasonable limit cpu: \u0026#34;250m\u0026#34; # Allow bursting Efficient Pod Scheduling:\n# Node affinity for cost optimization affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 preference: matchExpressions: - key: kubernetes.io/instance-type operator: In values: [\u0026#34;t3.micro\u0026#34;, \u0026#34;t3.small\u0026#34;] # Prefer cheaper instances LoadBalancer Optimization:\n# Use single ALB for multiple services apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: shared-alb annotations: kubernetes.io/ingress.class: alb spec: rules: - http: paths: - path: /api/v1/* backend: service: name: retail-api-service port: number: 80 - path: /admin/* backend: service: name: admin-service port: number: 80 10.9. Monitoring Costs # Monitor pod resource usage kubectl top pods -n mlops # Check actual vs requested resources kubectl describe pod \u0026lt;pod-name\u0026gt; -n mlops # Monitor HPA behavior kubectl get hpa -w -n mlops # Check LoadBalancer usage aws elbv2 describe-load-balancers --names \u0026lt;alb-name\u0026gt; Cost tracking commands:\n# ELB costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon Elastic Load Balancing\u0026#34;]}}\u0026#39; # EC2 costs for nodes aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=INSTANCE_TYPE ğŸ’° Cost Summary cho Task 8:\nPods: Included in node cost (no additional charge) LoadBalancer: $16.43/month base + usage Auto-scaling: $0-30.37/month depending on load Storage: $0.80/month per 10GB PVC Total: $17-47/month depending on scaling ğŸ¬ Video thá»±c hiá»‡n Task 8 Next Step: Task 09: Elastic Load Balancing\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/3-blogstranslated/3.9-blog9/",
	"title": "AWS Partner Central nay Ä‘Ã£ cÃ³ sáºµn trong AWS Management Console",
	"tags": [],
	"description": "",
	"content": "TÃ¡c giáº£: SÃ©bastien Stormacq â€” 30 NOV 2025\nHÃ´m nay, chÃºng tÃ´i cÃ´ng bá»‘ ráº±ng AWS Partner Central hiá»‡n Ä‘Ã£ cÃ³ sáºµn trá»±c tiáº¿p trong AWS Management Console, táº¡o ra má»™t tráº£i nghiá»‡m há»£p nháº¥t giÃºp chuyá»ƒn Ä‘á»•i cÃ¡ch báº¡n tÆ°Æ¡ng tÃ¡c vá»›i AWS vá»›i vai trÃ² vá»«a lÃ  khÃ¡ch hÃ ng vá»«a lÃ  AWS Partners.\nLÃ  ngÆ°á»i Ä‘Ã£ lÃ m viá»‡c vá»›i vÃ´ sá»‘ khÃ¡ch hÃ ng AWS trong nhiá»u nÄƒm qua, tÃ´i Ä‘Ã£ quan sÃ¡t cÃ¡ch cÃ¡c tá»• chá»©c phÃ¡t triá»ƒn trong hÃ nh trÃ¬nh AWS cá»§a há». Nhiá»u Partners thÃ nh cÃ´ng nháº¥t cá»§a chÃºng tÃ´i báº¯t Ä‘áº§u lÃ  khÃ¡ch hÃ ng AWSâ€”trÆ°á»›c háº¿t sá»­ dá»¥ng cÃ¡c dá»‹ch vá»¥ cá»§a chÃºng tÃ´i Ä‘á»ƒ xÃ¢y dá»±ng háº¡ táº§ng vÃ  giáº£i phÃ¡p cá»§a riÃªng há», sau Ä‘Ã³ má»Ÿ rá»™ng Ä‘á»ƒ táº¡o ra cÃ¡c dá»‹ch vá»¥ cung cáº¥p cho ngÆ°á»i khÃ¡c. NhÃ¬n tháº¥y sá»± tiáº¿n triá»ƒn tá»± nhiÃªn tá»« khÃ¡ch hÃ ng thÃ nh Partner nÃ y, chÃºng tÃ´i nháº­n ra má»™t cÆ¡ há»™i Ä‘á»ƒ tinh gá»n hai tráº£i nghiá»‡m vá»‘n tÃ¡ch biá»‡t theo truyá»n thá»‘ng thÃ nh má»™t hÃ nh trÃ¬nh há»£p nháº¥t.\nKhi AWS phÃ¡t triá»ƒn, nhu cáº§u cá»§a cá»™ng Ä‘á»“ng Partner cá»§a chÃºng tÃ´i cÅ©ng thay Ä‘á»•i. CÃ¡c tá»• chá»©c ngÃ y nay hoáº¡t Ä‘á»™ng trong nhiá»u vai trÃ²: sá»­ dá»¥ng cÃ¡c dá»‹ch vá»¥ AWS cho háº¡ táº§ng cá»§a chÃ­nh há» Ä‘á»“ng thá»i xÃ¢y dá»±ng vÃ  cung cáº¥p cÃ¡c giáº£i phÃ¡p cho khÃ¡ch hÃ ng cá»§a há». Doanh nghiá»‡p hiá»‡n Ä‘áº¡i cáº§n cÃ¡c quy trÃ¬nh lÃ m viá»‡c Ä‘Æ°á»£c tinh gá»n, há»— trá»£ sá»± phÃ¡t triá»ƒn tá»« khÃ¡ch hÃ ng AWS thÃ nh Partner rá»“i thÃ nh ngÆ°á»i bÃ¡n trÃªn AWS Marketplace, vá»›i cÃ¡c tÃ­nh nÄƒng báº£o máº­t cáº¥p doanh nghiá»‡p phÃ¹ há»£p vá»›i cÃ¡ch há» thá»±c sá»± lÃ m viá»‡c vá»›i AWS ngÃ y nay.\nMá»™t tráº£i nghiá»‡m console há»£p nháº¥t má»›i Viá»‡c tÃ­ch há»£p AWS Partner Central vÃ o Console Ä‘áº¡i diá»‡n cho má»™t sá»± thay Ä‘á»•i mang tÃ­nh ná»n táº£ng vá» kháº£ nÄƒng tiáº¿p cáº­n quan há»‡ Ä‘á»‘i tÃ¡c. Vá»›i cÃ¡c khÃ¡ch hÃ ng AWS hiá»‡n cÃ³, nhÆ° báº¡n, viá»‡c trá»Ÿ thÃ nh má»™t AWS Partner giá» Ä‘Ã¢y rÃµ rÃ ng nhÆ° viá»‡c truy cáº­p báº¥t ká»³ dá»‹ch vá»¥ AWS nÃ o khÃ¡c. Giao diá»‡n console quen thuá»™c cung cáº¥p quyá»n truy cáº­p trá»±c tiáº¿p Ä‘áº¿n cÃ¡c cÆ¡ há»™i há»£p tÃ¡c, quyá»n lá»£i chÆ°Æ¡ng trÃ¬nh vÃ  cÃ¡c kháº£ nÄƒng cá»§a AWS Marketplace mÃ  khÃ´ng cáº§n Ä‘Äƒng nháº­p riÃªng hay Ä‘iá»u hÆ°á»›ng giá»¯a cÃ¡c há»‡ thá»‘ng khÃ¡c nhau.\nBáº¯t Ä‘áº§u vá»›i vai trÃ² AWS Partner giá» Ä‘Ã¢y chá»‰ máº¥t vÃ i cÃº nháº¥p chuá»™t trong mÃ´i trÆ°á»ng console hiá»‡n cÃ³ cá»§a báº¡n. Báº¡n cÃ³ thá»ƒ khÃ¡m phÃ¡ cÃ¡c cÆ¡ há»™i há»£p tÃ¡c, hiá»ƒu cÃ¡c yÃªu cáº§u cá»§a chÆ°Æ¡ng trÃ¬nh vÃ  báº¯t Ä‘áº§u hÃ nh trÃ¬nh Partner cá»§a mÃ¬nh mÃ  khÃ´ng cáº§n rá»i khá»i giao diá»‡n AWS mÃ  báº¡n Ä‘Ã£ biáº¿t vÃ  tin dÃ¹ng.\nTÃ­ch há»£p vÃ o console táº¡o ra má»™t lá»™ trÃ¬nh trá»±c quan Ä‘á»ƒ cÃ¡c khÃ¡ch hÃ ng hiá»‡n cÃ³ chuyá»ƒn sang trá»Ÿ thÃ nh AWS Marketplace Sellers. Giá» Ä‘Ã¢y, báº¡n cÃ³ thá»ƒ truy cáº­p cÃ¡c kháº£ nÄƒng dÃ nh cho ngÆ°á»i bÃ¡n trÃªn AWS Marketplace song song vá»›i cÃ¡c dá»‹ch vá»¥ AWS hiá»‡n cÃ³, quáº£n lÃ½ cáº£ háº¡ táº§ng láº«n hoáº¡t Ä‘á»™ng kinh doanh AWS Marketplace tá»« má»™t giao diá»‡n duy nháº¥t. CÃ¡c yÃªu cáº§u private offer vÃ  Ä‘Ã m phÃ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c quáº£n lÃ½ trá»±c tiáº¿p trong AWS Partner Central, vÃ  báº¡n cÃ³ thá»ƒ quáº£n lÃ½ cÃ¡c listings trÃªn AWS Marketplace song song vá»›i cÃ¡c hoáº¡t Ä‘á»™ng AWS khÃ¡c cá»§a mÃ¬nh thÃ´ng qua cÃ¡c quy trÃ¬nh lÃ m viá»‡c Ä‘Æ°á»£c tinh gá»n.\nTrá»Ÿ thÃ nh má»™t AWS Partner Tráº£i nghiá»‡m console há»£p nháº¥t cung cáº¥p quyá»n truy cáº­p vÃ o cÃ¡c quyá»n lá»£i há»£p tÃ¡c toÃ n diá»‡n Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tÄƒng tá»‘c tÄƒng trÆ°á»Ÿng doanh nghiá»‡p cá»§a báº¡n.\nHÃ£y tham gia AWS Partner Network (APN) vÃ  hoÃ n táº¥t cÃ¡c yÃªu cáº§u dÃ nh cho Partner vÃ  AWS Marketplace Seller má»™t cÃ¡ch liá»n máº¡ch trong cÃ¹ng má»™t giao diá»‡n. ÄÄƒng kÃ½ cÃ¡c Partner Paths phÃ¹ há»£p vá»›i cÃ¡c giáº£i phÃ¡p khÃ¡ch hÃ ng cá»§a báº¡n Ä‘á»ƒ xÃ¢y dá»±ng, tiáº¿p thá»‹, niÃªm yáº¿t vÃ  bÃ¡n trÃªn AWS Marketplace Ä‘á»“ng thá»i phÃ¡t triá»ƒn cÃ¹ng AWS. Khi báº¡n Ä‘Ã£ vá»¯ng vÃ ng, hÃ£y dÃ¹ng cÃ¡c chÆ°Æ¡ng trÃ¬nh Partner Ä‘á»ƒ táº¡o khÃ¡c biá»‡t cho giáº£i phÃ¡p cá»§a báº¡n, niÃªm yáº¿t trÃªn AWS Marketplace Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng Ä‘Æ°á»£c khÃ¡m phÃ¡ trong go-to-market, vÃ  xÃ¢y dá»±ng chuyÃªn mÃ´n AWS thÃ´ng qua cÃ¡c chá»©ng chá»‰ Ä‘á»ƒ thÃºc Ä‘áº©y lá»£i nhuáº­n báº±ng cÃ¡ch náº¯m báº¯t cÃ¡c dÃ²ng doanh thu má»›i. Má»Ÿ rá»™ng doanh nghiá»‡p báº±ng cÃ¡ch bÃ¡n hoáº·c bÃ¡n láº¡i pháº§n má»m vÃ  dá»‹ch vá»¥ chuyÃªn nghiá»‡p trÃªn AWS Marketplace, giÃºp báº¡n tÄƒng tá»‘c cÃ¡c thÆ°Æ¡ng vá»¥, tÄƒng doanh thu vÃ  má»Ÿ rá»™ng pháº¡m vi tiáº¿p cáº­n khÃ¡ch hÃ ng sang cÃ¡c khu vá»±c Ä‘á»‹a lÃ½, ngÃ nh vÃ  phÃ¢n khÃºc má»›i.\nTrong suá»‘t hÃ nh trÃ¬nh cá»§a báº¡n, báº¡n cÃ³ thá»ƒ tiáº¿p tá»¥c sá»­ dá»¥ng Amazon Q trong console, cÃ´ng cá»¥ cung cáº¥p hÆ°á»›ng dáº«n cÃ¡ nhÃ¢n hÃ³a thÃ´ng qua AWS Partner Assistant.\nHÃ£y xem Partner Central console má»›i AWS Partner Central má»›i cÃ³ thá»ƒ truy cáº­p tá»« console nhÆ° báº¥t ká»³ dá»‹ch vá»¥ AWS nÃ o khÃ¡c. Trong sá»‘ nhiá»u kháº£ nÄƒng má»›i, nÃ³ cung cáº¥p bá»‘n khu vá»±c chÃ­nh há»— trá»£ váº­n hÃ nh Partner vÃ  tÄƒng trÆ°á»Ÿng kinh doanh trong AWS Partner Network:\n1. NÃ³ giÃºp báº¡n bÃ¡n cÃ¡c giáº£i phÃ¡p cá»§a mÃ¬nh AWS Partner Central - Solutions Báº¡n cÃ³ thá»ƒ táº¡o vÃ  xuáº¥t báº£n cÃ¡c giáº£i phÃ¡p Ä‘Ã¡p á»©ng cÃ¡c nhu cáº§u cá»¥ thá»ƒ cá»§a khÃ¡ch hÃ ng thÃ´ng qua AWS Marketplace. Solutions Ä‘Æ°á»£c táº¡o thÃ nh tá»« cÃ¡c products nhÆ° pháº§n má»m dáº¡ng dá»‹ch vá»¥ (SaaS), Amazon Machine Images (AMI), containers, dá»‹ch vá»¥ chuyÃªn nghiá»‡p, AI agents vÃ  tools, vÃ  nhiá»u hÆ¡n ná»¯a. Kháº£ nÄƒng quáº£n lÃ½ giáº£i phÃ¡p hÆ°á»›ng dáº«n báº¡n xÃ¢y dá»±ng cÃ¡c offerings bao gá»“m cáº£ cÃ¡c products báº¡n sá»Ÿ há»¯u vÃ  cÃ¡c products báº¡n Ä‘Æ°á»£c á»§y quyá»n Ä‘á»ƒ bÃ¡n láº¡i. Báº¡n cÃ³ thá»ƒ xÃ¢y dá»±ng cÃ¡c Ä‘á» xuáº¥t giÃ¡ trá»‹ (value propositions) vÃ  mÃ´ táº£ thuyáº¿t phá»¥c, truyá»n Ä‘áº¡t rÃµ rÃ ng lá»£i Ã­ch cá»§a giáº£i phÃ¡p Ä‘áº¿n nhá»¯ng ngÆ°á»i mua tiá»m nÄƒng Ä‘ang duyá»‡t AWS Marketplace.\nTÃ´i chá»n Create solution Ä‘á»ƒ báº¯t Ä‘áº§u niÃªm yáº¿t má»™t giáº£i phÃ¡p má»›i trÃªn AWS Marketplace, nhÆ° thá»ƒ hiá»‡n trong hÃ¬nh sau.\nAWS Partner Central - Create solution 2. NÃ³ giÃºp báº¡n cáº­p nháº­t vÃ  quáº£n lÃ½ há»“ sÆ¡ Partner cá»§a mÃ¬nh AWS Partner Central - Manage profile Há»“ sÆ¡ Partner cá»§a báº¡n giá»›i thiá»‡u chuyÃªn mÃ´n vÃ  nÄƒng lá»±c cá»§a tá»• chá»©c báº¡n tá»›i cá»™ng Ä‘á»“ng AWS. Báº¡n kiá»ƒm soÃ¡t cÃ¡ch doanh nghiá»‡p cá»§a mÃ¬nh xuáº¥t hiá»‡n trÆ°á»›c cÃ¡c khÃ¡ch hÃ ng vÃ  Partners tiá»m nÄƒng báº±ng cÃ¡ch nÃªu báº­t cÃ¡c phÃ¢n khÃºc ngÃ nh báº¡n phá»¥c vá»¥ vÃ  mÃ´ táº£ cÃ¡c sáº£n pháº©m hoáº·c dá»‹ch vá»¥ chÃ­nh cá»§a báº¡n. CÃ¡c thiáº¿t láº­p hiá»ƒn thá»‹ há»“ sÆ¡ cung cáº¥p cho báº¡n tÃ¹y chá»n chá»n thÃ´ng tin cá»§a báº¡n lÃ  cÃ´ng khai hay riÃªng tÆ°.\n3. NÃ³ giÃºp báº¡n theo dÃµi cÃ¡c cÆ¡ há»™i AWS Partner Central - Track Opportunities Báº¡n cÃ³ thá»ƒ quáº£n lÃ½ pipeline cÃ¡c khÃ¡ch hÃ ng AWS cá»§a mÃ¬nh, há»— trá»£ cÃ¡c hoáº¡t Ä‘á»™ng cá»™ng tÃ¡c chung vá»›i AWS trong cÃ¡c láº§n tÆ°Æ¡ng tÃ¡c vá»›i khÃ¡ch hÃ ng. Báº¡n theo dÃµi cÃ¡c prospects nÃ y báº±ng cÃ¡c chá»‰ bÃ¡o tráº¡ng thÃ¡i rÃµ rÃ ng: approved, rejected, draft vÃ  pending approval. Báº£ng Ä‘iá»u khiá»ƒn cÆ¡ há»™i hiá»ƒn thá»‹ cÃ¡c giai Ä‘oáº¡n, doanh thu Ä‘á»‹nh ká»³ hÃ ng thÃ¡ng Æ°á»›c tÃ­nh cá»§a AWS (AWS Monthly Recurring Revenue) vÃ  cÃ¡c chá»‰ sá»‘ quan trá»ng khÃ¡c giÃºp báº¡n hiá»ƒu pipeline cá»§a mÃ¬nh. Báº¡n cÃ³ thá»ƒ táº¡o thÃªm cÃ¡c cÆ¡ há»™i trá»±c tiáº¿p trong console vÃ  xuáº¥t dá»¯ liá»‡u cho bÃ¡o cÃ¡o vÃ  phÃ¢n tÃ­ch cá»§a riÃªng báº¡n.\n4. NÃ³ cung cáº¥p cho báº¡n kháº£ nÄƒng khÃ¡m phÃ¡ vÃ  káº¿t ná»‘i vá»›i cÃ¡c Partners khÃ¡c Sau khi trá»Ÿ thÃ nh má»™t AWS Partner, báº¡n cÃ³ quyá»n truy cáº­p vÃ o máº¡ng lÆ°á»›i AWS Partners, nÆ¡i báº¡n cÃ³ thá»ƒ tÃ¬m kiáº¿m cÃ¡c Partners khÃ¡c. Báº¡n cÃ³ thá»ƒ káº¿t ná»‘i vá»›i há» Ä‘á»ƒ há»£p tÃ¡c trong cÃ¡c cÆ¡ há»™i bÃ¡n hÃ ng vÃ  má»Ÿ rá»™ng pháº¡m vi tiáº¿p cáº­n khÃ¡ch hÃ ng cá»§a báº¡n.\nAWS Partner Central - Discover and Search for partners Báº¡n tÃ¬m kiáº¿m trong danh sÃ¡ch Partners hiá»‡n cÃ³ báº±ng cÃ¡c bá»™ lá»c theo ngÃ nh, vá»‹ trÃ­, loáº¡i chÆ°Æ¡ng trÃ¬nh Partner vÃ  chuyÃªn mÃ´n. Báº£ng Ä‘iá»u khiá»ƒn táº­p trung hiá»ƒn thá»‹ cÃ¡c káº¿t ná»‘i Ä‘ang hoáº¡t Ä‘á»™ng, cÃ¡c yÃªu cáº§u Ä‘ang chá» vÃ  lá»‹ch sá»­ káº¿t ná»‘i, Ä‘á»ƒ báº¡n cÃ³ thá»ƒ quáº£n lÃ½ cÃ¡c má»‘i quan há»‡ kinh doanh vÃ  xÃ¡c Ä‘á»‹nh cÃ¡c cÆ¡ há»™i há»£p tÃ¡c cÃ³ thá»ƒ má»Ÿ rá»™ng pháº¡m vi cá»§a báº¡n. Giá»‘ng nhÆ° cÃ¡c dá»‹ch vá»¥ AWS khÃ¡c, cÃ¡c kháº£ nÄƒng káº¿t ná»‘i Partner nÃ y hiá»‡n cÅ©ng cÃ³ sáºµn dÆ°á»›i dáº¡ng APIs, cung cáº¥p tá»± Ä‘á»™ng hÃ³a vÃ  tÃ­ch há»£p vÃ o cÃ¡c quy trÃ¬nh lÃ m viá»‡c hiá»‡n cÃ³ cá»§a báº¡n.\nAWS Partner Central - Manage contact requests CÃ¡c kháº£ nÄƒng nÃ y phá»‘i há»£p cÃ¹ng nhau trong AWS Partner Central console má»›i, cÃ³ thá»ƒ truy cáº­p trá»±c tiáº¿p tá»« console, giÃºp báº¡n chuyá»ƒn Ä‘á»•i tá»« khÃ¡ch hÃ ng AWS thÃ nh Partner thÃ nh cÃ´ng vá»›i báº£o máº­t cáº¥p doanh nghiá»‡p vÃ  quy trÃ¬nh lÃ m viá»‡c Ä‘Æ°á»£c tinh gá»n.\nNá»n táº£ng ká»¹ thuáº­t: Di chuyá»ƒn há»‡ thá»‘ng Ä‘á»‹nh danh Tráº£i nghiá»‡m console há»£p nháº¥t nÃ y trá»Ÿ nÃªn kháº£ thi nhá» viá»‡c chÃºng tÃ´i chuyá»ƒn sang má»™t há»‡ thá»‘ng Ä‘á»‹nh danh hiá»‡n Ä‘áº¡i Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn AWS Identity and Access Management (IAM). ChÃºng tÃ´i Ä‘Ã£ chuyá»ƒn tá»« háº¡ táº§ng Ä‘á»‹nh danh káº¿ thá»«a sang IAM Identity Center, cung cáº¥p cÃ¡c kháº£ nÄƒng báº£o máº­t cáº¥p doanh nghiá»‡p bao gá»“m kháº£ nÄƒng single sign-on vÃ  xÃ¡c thá»±c Ä‘a yáº¿u tá»‘. Vá»›i báº£o máº­t lÃ  Æ°u tiÃªn sá»‘ khÃ´ng (job zero), Ä‘á»£t di chuyá»ƒn nÃ y mang Ä‘áº¿n cho cÃ¡c Partners má»›i vÃ  hiá»‡n cÃ³ kháº£ nÄƒng káº¿t ná»‘i cÃ¡c identity providers cá»§a riÃªng há» vá»›i AWS Partner Central. NÃ³ cung cáº¥p tÃ­ch há»£p liá»n máº¡ch vá»›i cÃ¡c há»‡ thá»‘ng xÃ¡c thá»±c doanh nghiá»‡p hiá»‡n cÃ³, Ä‘á»“ng thá»i loáº¡i bá» sá»± phá»©c táº¡p cá»§a viá»‡c quáº£n lÃ½ cÃ¡c thÃ´ng tin xÃ¡c thá»±c tÃ¡ch biá»‡t giá»¯a cÃ¡c dá»‹ch vá»¥ khÃ¡c nhau.\nThÃªm má»™t Ä‘iá»u ná»¯a APIs lÃ  cá»‘t lÃµi trong nhá»¯ng gÃ¬ chÃºng tÃ´i lÃ m táº¡i AWS, vÃ  AWS Partner Central cÅ©ng khÃ´ng ngoáº¡i lá»‡. Báº¡n cÃ³ thá»ƒ tá»± Ä‘á»™ng hÃ³a vÃ  tinh gá»n cÃ¡c quy trÃ¬nh lÃ m viá»‡c co-sell báº±ng cÃ¡ch káº¿t ná»‘i cÃ¡c cÃ´ng cá»¥ kinh doanh cá»§a báº¡n vá»›i AWS Partner Central. CÃ¡c APIs do AWS Partner Central cung cáº¥p giÃºp báº¡n tÄƒng tá»‘c cÃ¡c quyá»n lá»£i APNâ€”tá»« Account Management (Account API) vÃ  Solution Management (Solution API) Ä‘áº¿n co-selling vá»›i Opportunity vÃ  Leads APIs, vÃ  Benefits APIs Ä‘á»ƒ kÃ­ch hoáº¡t quyá»n lá»£i nhanh hÆ¡n.\nBáº¡n cÃ³ thá»ƒ dÃ¹ng cÃ¡c APIs nÃ y Ä‘á»ƒ tÆ°Æ¡ng tÃ¡c vá»›i AWS vÃ  phÃ¡t triá»ƒn hoáº¡t Ä‘á»™ng kinh doanh Partner cá»§a báº¡n ngay tá»« cÃ¡c cÃ´ng cá»¥ CRM cá»§a riÃªng báº¡n.\nBáº¯t Ä‘áº§u ngay hÃ´m nay Viá»‡c tÃ­ch há»£p giá»¯a console vÃ  AWS Partner Central pháº£n Ã¡nh cam káº¿t cá»§a chÃºng tÃ´i trong viá»‡c giáº£m Ä‘á»™ phá»©c táº¡p vÃ  cáº£i thiá»‡n tráº£i nghiá»‡m Partner. ChÃºng tÃ´i Ä‘Æ°a AWS Partner Central vÃ o console Ä‘á»ƒ táº¡o ra má»™t con Ä‘Æ°á»ng trá»±c quan hÆ¡n cho cÃ¡c tá»• chá»©c phÃ¡t triá»ƒn cÃ¹ng AWS, tá»« viá»‡c Ã¡p dá»¥ng vá»›i tÆ° cÃ¡ch khÃ¡ch hÃ ng ban Ä‘áº§u cho Ä‘áº¿n tham gia quan há»‡ Ä‘á»‘i tÃ¡c toÃ n diá»‡n vÃ  thÃ nh cÃ´ng trÃªn AWS Marketplace.\nHÃ nh trÃ¬nh cá»§a báº¡n tá»« khÃ¡ch hÃ ng AWS Ä‘áº¿n AWS Partner thÃ nh cÃ´ng vÃ  AWS Marketplace Seller báº¯t Ä‘áº§u chá»‰ vá»›i vÃ i cÃº nháº¥p chuá»™t trong console cá»§a báº¡n. TÃ´i khuyáº¿n khÃ­ch báº¡n khÃ¡m phÃ¡ tráº£i nghiá»‡m há»£p nháº¥t má»›i ngay hÃ´m nay vÃ  khÃ¡m phÃ¡ cÃ¡ch AWS Partner Central trong console cÃ³ thá»ƒ tÄƒng tá»‘c tÄƒng trÆ°á»Ÿng vÃ  thÃ nh cÃ´ng cá»§a tá»• chá»©c báº¡n trong cá»™ng Ä‘á»“ng AWS.\nSáºµn sÃ ng báº¯t Ä‘áº§u? HÃ£y truy cáº­p AWS Partner Central trong console cá»§a báº¡n Ä‘á»ƒ tÃ¬m hiá»ƒu thÃªm vá» AWS Partner Network vÃ  khÃ¡m phÃ¡ partnership path phÃ¹ há»£p cho tá»• chá»©c cá»§a báº¡n.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.8-week8/",
	"title": "Tuáº§n 8 - Äiá»‡n toÃ¡n Serverless",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 8: XÃ¢y dá»±ng á»©ng dá»¥ng hÆ°á»›ng sá»± kiá»‡n (event-driven) báº±ng Lambda vÃ  API Gateway. Triá»ƒn khai xá»­ lÃ½ báº¥t Ä‘á»“ng bá»™ vá»›i SQS/SNS. CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - Lambda: PhÃ¡t triá»ƒn functions (Python/Node.js) cÃ³ dÃ¹ng environment variables.\n- Cáº¥u hÃ¬nh memory vÃ  timeouts. AWS Lambda Docs 3 - API Gateway: Táº¡o HTTP/REST APIs tÃ­ch há»£p vá»›i Lambda.\n- Thiáº¿t láº­p CORS vÃ  Stages. AWS APIGW Docs 4 - Event-Driven: Cáº¥u hÃ¬nh S3 Event Notifications Ä‘á»ƒ trigger Lambda.\n- Thiáº¿t láº­p EventBridge rules. AWS EventBridge 5 - Messaging: DÃ¹ng SQS Ä‘á»ƒ tÃ¡ch rá»i (decoupling) vÃ  thiáº¿t láº­p DLQ Ä‘á»ƒ xá»­ lÃ½ lá»—i.\n- DÃ¹ng SNS cho mÃ´ hÃ¬nh fan-out messaging. AWS SQS/SNS 6 - Ã”n táº­p: Test luá»“ng serverless end-to-end.\n- BÃ¡o cÃ¡o tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 8: PhÃ¡t triá»ƒn serverless API báº±ng API Gateway vÃ  Lambda. Triá»ƒn khai xá»­ lÃ½ lá»—i á»•n Ä‘á»‹nh vá»›i SQS Dead Letter Queues. Giáº£m overhead váº­n hÃ nh nhá» táº­n dá»¥ng cÃ¡c managed services. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/9-elastic-load-balancing/",
	"title": "Load Balancing",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Má»¥c tiÃªu Task 11:\nThiáº¿t láº­p cÆ¡ cháº¿ phÃ¢n phá»‘i lÆ°u lÆ°á»£ng (Load Balancing) cho API Retail Prediction, Ä‘áº£m báº£o:\nCÃ³ endpoint public Ä‘á»ƒ demo API /predict vÃ  /docs Tá»± Ä‘á»™ng phÃ¢n phá»‘i traffic giá»¯a nhiá»u Pod khi scaling Duy trÃ¬ tÃ­nh sáºµn sÃ ng \u0026amp; báº£o máº­t cá»§a dá»‹ch vá»¥ ğŸ“¥ Input tá»« cÃ¡c Task trÆ°á»›c:\nTask 5 (Production VPC): VPC subnets, security groups vÃ  VPC Endpoints Ä‘á»ƒ ALB vÃ  EKS hoáº¡t Ä‘á»™ng Task 7 (EKS Cluster): EKS cluster vÃ  Service/Ingress targets Ä‘á»ƒ ALB forward traffic Task 6 (ECR Container Registry): Container images (API) Ä‘Æ°á»£c deploy vÃ o EKS vÃ  expose qua ALB 1. Tá»•ng quan vá» Load Balancing cho Retail Prediction API Load Balancing lÃ  má»™t thÃ nh pháº§n thiáº¿t yáº¿u trong kiáº¿n trÃºc microservices trÃªn AWS EKS, Ä‘áº·c biá»‡t quan trá»ng cho Retail Prediction API - dá»‹ch vá»¥ dá»± Ä‘oÃ¡n doanh sá»‘ cÃ³ thá»ƒ pháº£i xá»­ lÃ½ khá»‘i lÆ°á»£ng request lá»›n vÃ  Ä‘á»™t biáº¿n. Load Balancing Ä‘áº£m báº£o:\nKháº£ nÄƒng má»Ÿ rá»™ng (Scalability): PhÃ¢n phá»‘i Ä‘á»“ng Ä‘á»u requests giá»¯a nhiá»u Pod, Ä‘áº£m báº£o API pháº£n há»“i nhanh ngay cáº£ khi lÆ°á»£ng yÃªu cáº§u dá»± Ä‘oÃ¡n tÄƒng Ä‘á»™t biáº¿n TÃ­nh sáºµn sÃ ng cao (High Availability): Tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  cÃ¡ch ly Pod khÃ´ng khá»e máº¡nh, Ä‘áº£m báº£o dá»‹ch vá»¥ dá»± Ä‘oÃ¡n luÃ´n hoáº¡t Ä‘á»™ng Auto-scaling tá»± Ä‘á»™ng: Káº¿t há»£p vá»›i HPA Ä‘á»ƒ tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh sá»‘ lÆ°á»£ng Pod dá»±a trÃªn lÆ°u lÆ°á»£ng thá»±c táº¿ Endpoint nháº¥t quÃ¡n: Cung cáº¥p má»™t endpoint duy nháº¥t Ä‘á»ƒ client (web/mobile app) káº¿t ná»‘i tá»›i dá»‹ch vá»¥ dá»± Ä‘oÃ¡n Kháº£ nÄƒng quan sÃ¡t (Observability): Thu tháº­p metrics vá» lÆ°u lÆ°á»£ng, latency vÃ  errors cho viá»‡c theo dÃµi hiá»‡u suáº¥t API 2. Thiáº¿t láº­p Application Load Balancer (ALB) AWS Application Load Balancer (ALB) lÃ  lá»±a chá»n tá»‘t cho Retail Prediction API vÃ¬:\nHoáº¡t Ä‘á»™ng á»Ÿ Layer 7 (Application Layer) Há»— trá»£ route cÃ¡c request dá»±a trÃªn path hoáº·c host header TÃ­ch há»£p Ä‘Æ°á»£c vá»›i AWS WAF Ä‘á»ƒ báº£o vá»‡ API Há»— trá»£ SSL/TLS termination 2.1 Service Type LoadBalancer vá»›i ALB CÃ¡ch Ä‘Æ¡n giáº£n nháº¥t Ä‘á»ƒ táº¡o ALB lÃ  sá»­ dá»¥ng Service vá»›i type lÃ  LoadBalancer:\n# service-alb.yaml apiVersion: v1 kind: Service metadata: name: retail-api-service namespace: retail-prediction labels: app: retail-api annotations: # Chá»‰ Ä‘á»‹nh sá»­ dá»¥ng Application Load Balancer service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;application\u0026#34; service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \u0026#34;http\u0026#34; # Health check configuration service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: \u0026#34;/health\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: \u0026#34;30\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: \u0026#34;5\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: \u0026#34;2\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: \u0026#34;3\u0026#34; spec: type: LoadBalancer ports: - port: 80 targetPort: 8000 protocol: TCP name: http selector: app: retail-api --- Khi sá»­ dá»¥ng annotation service.beta.kubernetes.io/aws-load-balancer-type: \u0026quot;application\u0026quot;, EKS sáº½ tá»± Ä‘á»™ng táº¡o ALB thay vÃ¬ NLB hoáº·c Classic Load Balancer.\n2.2 Cáº¥u hÃ¬nh Health Checks vÃ  Routing ALB tá»± Ä‘á»™ng thá»±c hiá»‡n health check Ä‘áº¿n endpoint /health Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng chá»‰ cÃ¡c Pod khá»e máº¡nh má»›i nháº­n Ä‘Æ°á»£c traffic:\nsequenceDiagram\rparticipant ALB as Application Load Balancer\rparticipant Pod1 as API Pod 1 (Healthy)\rparticipant Pod2 as API Pod 2 (Unhealthy)\rparticipant Pod3 as API Pod 3 (Healthy)\rALB-\u003e\u003ePod1: Health Check: GET /health\rPod1-\u003e\u003eALB: 200 OK: {\"status\": \"healthy\"}\rALB-\u003e\u003ePod2: Health Check: GET /health\rPod2-\u003e\u003eALB: 500 Error: Service Unavailable\rALB-\u003e\u003ePod3: Health Check: GET /health\rPod3-\u003e\u003eALB: 200 OK: {\"status\": \"healthy\"}\rNote over ALB,Pod2: Pod2 Ä‘Æ°á»£c Ä‘Ã¡nh dáº¥u unhealthy\rClient-\u003e\u003eALB: Request: POST /predict\rALB-\u003e\u003ePod1: Forward request\rPod1-\u003e\u003eALB: Response\rALB-\u003e\u003eClient: Return response\rClient-\u003e\u003eALB: Request: GET /docs\rALB-\u003e\u003ePod3: Forward request\rPod3-\u003e\u003eALB: Response\rALB-\u003e\u003eClient: Return response\rGiáº£i thÃ­ch flow:\nALB liÃªn tá»¥c kiá»ƒm tra health cá»§a táº¥t cáº£ Pod thÃ´ng qua endpoint /health Pod nÃ o tráº£ vá» status code khÃ´ng pháº£i 200 OK sáº½ bá»‹ Ä‘Ã¡nh dáº¥u unhealthy CÃ¡c request tá»« client sáº½ chá»‰ Ä‘Æ°á»£c forward Ä‘áº¿n cÃ¡c Pod healthy Khi má»™t Pod unhealthy trá»Ÿ láº¡i healthy, nÃ³ sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c Ä‘Æ°a vÃ o rotation Nhá» cÆ¡ cháº¿ nÃ y, API luÃ´n duy trÃ¬ tÃ­nh sáºµn sÃ ng cao ngay cáº£ khi má»™t sá»‘ Pod gáº·p sá»± cá»‘.\n2.3 Advanced ALB Configuration # service-alb-advanced.yaml apiVersion: v1 kind: Service metadata: name: retail-api-service namespace: retail-prediction labels: app: retail-api annotations: # Application Load Balancer configuration service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;application\u0026#34; service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \u0026#34;http\u0026#34; # Health check configuration service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: \u0026#34;/health\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: \u0026#34;8000\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: \u0026#34;HTTP\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: \u0026#34;20\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: \u0026#34;5\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: \u0026#34;2\u0026#34; service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: \u0026#34;2\u0026#34; # Access logging (optional) service.beta.kubernetes.io/aws-load-balancer-access-log-enabled: \u0026#34;true\u0026#34; service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name: \u0026#34;retail-prediction-alb-logs\u0026#34; service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix: \u0026#34;api-access-logs\u0026#34; # Target group configuration service.beta.kubernetes.io/aws-load-balancer-attributes: \u0026#34;idle_timeout.timeout_seconds=60\u0026#34; # Target type (IP mode preferred for pods) service.beta.kubernetes.io/aws-load-balancer-target-type: \u0026#34;ip\u0026#34; spec: type: LoadBalancer ports: - port: 80 targetPort: 8000 protocol: TCP name: http selector: app: retail-forecast-api externalTrafficPolicy: Local # Preserve source IP --- 3. AWS Load Balancer Controller cho Ingress AWS Load Balancer Controller lÃ  má»™t controller Kubernetes quáº£n lÃ½ ALB (vÃ  NLB) cho cÃ¡c dá»‹ch vá»¥ trong EKS. Controller nÃ y cho phÃ©p cáº¥u hÃ¬nh ALB chi tiáº¿t hÆ¡n thÃ´ng qua Ingress vÃ  IngressClass thay vÃ¬ chá»‰ sá»­ dá»¥ng annotations trÃªn Service.\n3.1 CÃ i Ä‘áº·t AWS Load Balancer Controller # ThÃªm eks-charts repository helm repo add eks https://aws.github.io/eks-charts helm repo update # Táº¡o IAM policy cho ALB controller curl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.6.0/docs/install/iam_policy.json # Táº¡o IAM policy aws iam create-policy \\ --policy-name AWSLoadBalancerControllerIAMPolicy \\ --policy-document file://iam_policy.json # Táº¡o IAM role vÃ  service account vá»›i IRSA eksctl create iamserviceaccount \\ --cluster=retail-prediction-cluster \\ --namespace=kube-system \\ --name=aws-load-balancer-controller \\ --role-name AmazonEKSLoadBalancerControllerRole \\ --attach-policy-arn=arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:policy/AWSLoadBalancerControllerIAMPolicy \\ --approve # CÃ i Ä‘áº·t AWS Load Balancer Controller helm install aws-load-balancer-controller eks/aws-load-balancer-controller \\ -n kube-system \\ --set clusterName=retail-prediction-cluster \\ --set serviceAccount.create=false \\ --set serviceAccount.name=aws-load-balancer-controller Controller nÃ y hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch theo dÃµi cÃ¡c resource Service vÃ  Ingress, sau Ä‘Ã³ tá»± Ä‘á»™ng táº¡o vÃ  quáº£n lÃ½ ALB tÆ°Æ¡ng á»©ng trÃªn AWS. Báº¡n cÃ³ thá»ƒ xÃ¡c nháº­n tráº¡ng thÃ¡i cá»§a controller báº±ng lá»‡nh kubectl get pods -n kube-system | grep aws-load-balancer-controller.\n3.2 ALB Ingress Configuration cho Retail Prediction API Cáº¥u hÃ¬nh Ingress cho phÃ©p kiá»ƒm soÃ¡t chi tiáº¿t hÆ¡n cÃ¡ch ALB xá»­ lÃ½ traffic, bao gá»“m routing path-based, SSL termination vÃ  nhiá»u tÃ­nh nÄƒng nÃ¢ng cao khÃ¡c:\n# ingress-retail-prediction.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: retail-api-ingress namespace: retail-prediction annotations: # ALB Controller configuration kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip alb.ingress.kubernetes.io/load-balancer-name: retail-prediction-alb # Health check configuration alb.ingress.kubernetes.io/healthcheck-path: /health alb.ingress.kubernetes.io/healthcheck-interval-seconds: \u0026#34;20\u0026#34; alb.ingress.kubernetes.io/healthcheck-timeout-seconds: \u0026#34;5\u0026#34; alb.ingress.kubernetes.io/healthy-threshold-count: \u0026#34;2\u0026#34; alb.ingress.kubernetes.io/unhealthy-threshold-count: \u0026#34;2\u0026#34; # SSL configuration alb.ingress.kubernetes.io/ssl-redirect: \u0026#34;443\u0026#34; alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}, {\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:\u0026lt;ACCOUNT_ID\u0026gt;:certificate/\u0026lt;CERT_ID\u0026gt; # Access logging alb.ingress.kubernetes.io/load-balancer-attributes: | access_logs.s3.enabled=true, access_logs.s3.bucket=retail-prediction-alb-logs, access_logs.s3.prefix=api-access-logs, idle_timeout.timeout_seconds=60 # Performance and deregistration configuration alb.ingress.kubernetes.io/target-group-attributes: | deregistration_delay.timeout_seconds=30, slow_start.duration_seconds=30, load_balancing.algorithm.type=least_outstanding_requests # WAF integration for API protection alb.ingress.kubernetes.io/wafv2-acl-arn: arn:aws:wafv2:us-east-1:\u0026lt;ACCOUNT_ID\u0026gt;:regional/webacl/retail-prediction-waf/\u0026lt;WAF_ID\u0026gt; # Tags for cost allocation alb.ingress.kubernetes.io/tags: Environment=Production,Project=RetailPrediction,Service=API spec: rules: - http: # Default rule for all incoming requests paths: # API Documentation - path: /docs pathType: Prefix backend: service: name: retail-api-service port: number: 80 # Swagger JSON endpoint - path: /openapi.json pathType: Exact backend: service: name: retail-api-service port: number: 80 # Health check and readiness endpoints - path: /health pathType: Exact backend: service: name: retail-api-service port: number: 80 # Prediction API endpoints - path: /predict pathType: Exact backend: service: name: retail-api-service port: number: 80 # Default catch-all for all other paths - path: / pathType: Prefix backend: service: name: retail-api-service port: number: 80 3.3 Advanced ALB with Multiple Paths # ingress-alb-advanced.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: retail-forecast-alb-advanced namespace: mlops annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/scheme: internet-facing alb.ingress.kubernetes.io/target-type: ip # Listen ports for HTTP and HTTPS alb.ingress.kubernetes.io/listen-ports: \u0026#39;[{\u0026#34;HTTP\u0026#34;: 80}, {\u0026#34;HTTPS\u0026#34;: 443}]\u0026#39; # Actions for different paths alb.ingress.kubernetes.io/actions.weighted-routing: | { \u0026#34;type\u0026#34;: \u0026#34;forward\u0026#34;, \u0026#34;forwardConfig\u0026#34;: { \u0026#34;targetGroups\u0026#34;: [ { \u0026#34;serviceName\u0026#34;: \u0026#34;retail-forecast-service\u0026#34;, \u0026#34;servicePort\u0026#34;: 80, \u0026#34;weight\u0026#34;: 100 } ] } } # Rate limiting alb.ingress.kubernetes.io/target-group-attributes: | stickiness.enabled=false, load_balancing.algorithm.type=round_robin, slow_start.duration_seconds=30 spec: rules: - host: api.retail-forecast.com http: paths: # Health check endpoint - path: /healthz pathType: Exact backend: service: name: retail-forecast-service port: number: 80 # Prediction endpoints - path: /predict pathType: Exact backend: service: name: retail-forecast-service port: number: 80 # Batch prediction - path: /batch-predict pathType: Exact backend: service: name: retail-forecast-service port: number: 80 # Model information - path: /model-info pathType: Exact backend: service: name: retail-forecast-service port: number: 80 # Metrics endpoint - path: /metrics pathType: Exact backend: service: name: retail-forecast-service port: number: 80 # Default catch-all - path: / pathType: Prefix backend: service: name: retail-forecast-service port: number: 80 --- 4. HTTPS vÃ  SSL/TLS Configuration Báº£o máº­t API báº±ng HTTPS lÃ  má»™t yÃªu cáº§u quan trá»ng cho má»i dá»‹ch vá»¥ production, Ä‘áº·c biá»‡t lÃ  dá»‹ch vá»¥ dá»± Ä‘oÃ¡n cÃ³ thá»ƒ chá»©a dá»¯ liá»‡u nháº¡y cáº£m cá»§a doanh nghiá»‡p.\n4.1 Táº¡o SSL Certificate vá»›i AWS Certificate Manager (ACM) # Request SSL certificate aws acm request-certificate \\ --domain-name api.retail-prediction.example.com \\ --validation-method DNS \\ --region us-east-1 # Get certificate ARN CERT_ARN=$(aws acm list-certificates --region us-east-1 --query \u0026#34;CertificateSummaryList[?DomainName==\u0026#39;api.retail-prediction.example.com\u0026#39;].CertificateArn\u0026#34; --output text) echo $CERT_ARN 4.2 Cáº¥u hÃ¬nh HTTPS cho ALB # service-alb-https.yaml apiVersion: v1 kind: Service metadata: name: retail-api-service namespace: retail-prediction annotations: # Basic ALB configuration service.beta.kubernetes.io/aws-load-balancer-type: \u0026#34;application\u0026#34; service.beta.kubernetes.io/aws-load-balancer-scheme: \u0026#34;internet-facing\u0026#34; # SSL configuration service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \u0026#34;arn:aws:acm:us-east-1:123456789012:certificate/abcdef12-3456-7890-abcd-ef1234567890\u0026#34; service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \u0026#34;443\u0026#34; service.beta.kubernetes.io/aws-load-balancer-backend-protocol: \u0026#34;http\u0026#34; service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: \u0026#34;ELBSecurityPolicy-TLS-1-2-2017-01\u0026#34; # Redirect HTTP to HTTPS service.beta.kubernetes.io/aws-load-balancer-ssl-redirect: \u0026#34;443\u0026#34; # Other configurations... spec: type: LoadBalancer ports: - port: 443 targetPort: 8000 protocol: TCP name: https - port: 80 targetPort: 8000 protocol: TCP name: http selector: app: retail-api sequenceDiagram\rparticipant Client\rparticipant ALB as Application Load Balancer\rparticipant API as API Pod\rClient-\u003e\u003eALB: HTTPS Request (port 443)\rNote over ALB: SSL Termination\rALB-\u003e\u003eAPI: HTTP Request (port 8080)\rAPI-\u003e\u003eALB: HTTP Response\rALB-\u003e\u003eClient: HTTPS Response\rClient-\u003e\u003eALB: HTTP Request (port 80)\rNote over ALB: HTTP to HTTPS Redirect\rALB-\u003e\u003eClient: 301 Redirect to HTTPS\rClient-\u003e\u003eALB: HTTPS Request (port 443)\rNote over ALB: SSL Termination\rALB-\u003e\u003eAPI: HTTP Request (port 8080)\rAPI-\u003e\u003eALB: HTTP Response\rALB-\u003e\u003eClient: HTTPS Response\r4.3 Route 53 DNS Setup # Get the ALB DNS name ALB_DNS=$(kubectl get service retail-api-service -n retail-prediction -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) # Create Route 53 record aws route53 change-resource-record-sets \\ --hosted-zone-id YOUR_HOSTED_ZONE_ID \\ --change-batch \u0026#39;{ \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;CREATE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;api.retail-prediction.example.com\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;CNAME\u0026#34;, \u0026#34;TTL\u0026#34;: 300, \u0026#34;ResourceRecords\u0026#34;: [{\u0026#34;Value\u0026#34;: \u0026#34;\u0026#39;$ALB_DNS\u0026#39;\u0026#34;}] } }] }\u0026#39; 4.4 XÃ¡c nháº­n HTTPS Ä‘ang hoáº¡t Ä‘á»™ng # Test HTTPS endpoint curl -k https://api.retail-prediction.example.com/health # Verify with proper SSL validation curl https://api.retail-prediction.example.com/health # Test HTTP to HTTPS redirect curl -v http://api.retail-prediction.example.com/health # Expected: HTTP/1.1 301 Moved Permanently 5. Báº£o máº­t cho Retail Prediction API Viá»‡c báº£o máº­t cho API dá»± Ä‘oÃ¡n retail ráº¥t quan trá»ng vÃ¬ API cÃ³ thá»ƒ chá»©a thÃ´ng tin nháº¡y cáº£m vá» doanh sá»‘ vÃ  chiáº¿n lÆ°á»£c kinh doanh. ChÃºng ta sáº½ triá»ƒn khai nhiá»u lá»›p báº£o máº­t:\n5.1 AWS WAF (Web Application Firewall) WAF giÃºp báº£o vá»‡ API khá»i cÃ¡c cuá»™c táº¥n cÃ´ng phá»• biáº¿n nhÆ° SQL Injection, XSS vÃ  cÃ¡c táº¥n cÃ´ng DDoS:\n# Táº¡o WAF Web ACL aws wafv2 create-web-acl \\ --name retail-prediction-waf \\ --scope REGIONAL \\ --region us-east-1 \\ --default-action Allow={} \\ --visibility-config SampledRequestsEnabled=true,CloudWatchMetricsEnabled=true,MetricName=RetailAPI \\ --rules file://waf-rules.json Ná»™i dung file waf-rules.json:\n[ { \u0026#34;Name\u0026#34;: \u0026#34;API-RateLimit\u0026#34;, \u0026#34;Priority\u0026#34;: 0, \u0026#34;Statement\u0026#34;: { \u0026#34;RateBasedStatement\u0026#34;: { \u0026#34;Limit\u0026#34;: 3000, \u0026#34;AggregateKeyType\u0026#34;: \u0026#34;IP\u0026#34; } }, \u0026#34;Action\u0026#34;: { \u0026#34;Block\u0026#34;: {} }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;SampledRequestsEnabled\u0026#34;: true, \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;API-RateLimit\u0026#34; } }, { \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesSQLiRuleSet\u0026#34;, \u0026#34;Priority\u0026#34;: 1, \u0026#34;Statement\u0026#34;: { \u0026#34;ManagedRuleGroupStatement\u0026#34;: { \u0026#34;VendorName\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesSQLiRuleSet\u0026#34; } }, \u0026#34;OverrideAction\u0026#34;: { \u0026#34;None\u0026#34;: {} }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;SampledRequestsEnabled\u0026#34;: true, \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;SQLiRuleSet\u0026#34; } }, { \u0026#34;Name\u0026#34;: \u0026#34;BlockHighRiskRequests\u0026#34;, \u0026#34;Priority\u0026#34;: 2, \u0026#34;Statement\u0026#34;: { \u0026#34;ManagedRuleGroupStatement\u0026#34;: { \u0026#34;VendorName\u0026#34;: \u0026#34;AWS\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;AWSManagedRulesKnownBadInputsRuleSet\u0026#34; } }, \u0026#34;OverrideAction\u0026#34;: { \u0026#34;None\u0026#34;: {} }, \u0026#34;VisibilityConfig\u0026#34;: { \u0026#34;SampledRequestsEnabled\u0026#34;: true, \u0026#34;CloudWatchMetricsEnabled\u0026#34;: true, \u0026#34;MetricName\u0026#34;: \u0026#34;BadInputs\u0026#34; } } ] 5.2 Security Group Configuration # Táº¡o security group cho ALB aws ec2 create-security-group \\ --group-name retail-prediction-alb-sg \\ --description \u0026#34;Security group for Retail Prediction API ALB\u0026#34; \\ --vpc-id vpc-0123456789abcdef0 # Cho phÃ©p traffic HTTPS (port 443) aws ec2 authorize-security-group-ingress \\ --group-id sg-0123456789abcdef0 \\ --protocol tcp \\ --port 443 \\ --cidr 0.0.0.0/0 \\ --description \u0026#34;HTTPS from anywhere\u0026#34; # Cho phÃ©p traffic HTTP (port 80) - chá»‰ Ä‘á»ƒ redirect sang HTTPS aws ec2 authorize-security-group-ingress \\ --group-id sg-0123456789abcdef0 \\ --protocol tcp \\ --port 80 \\ --cidr 0.0.0.0/0 \\ --description \u0026#34;HTTP from anywhere (for redirect only)\u0026#34; 5.3 PhÃ¢n quyá»n API vá»›i JWT Authentication Äá»ƒ báº£o vá»‡ API khá»i truy cáº­p trÃ¡i phÃ©p, chÃºng ta nÃªn triá»ƒn khai xÃ¡c thá»±c JWT:\n# ThÃªm mÃ£ sau vÃ o file main.py cá»§a FastAPI from fastapi import Depends, FastAPI, HTTPException, status from fastapi.security import OAuth2PasswordBearer import jwt from jwt.exceptions import PyJWTError # Setup OAuth2 oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\u0026#34;token\u0026#34;) # JWT Configuration JWT_SECRET_KEY = os.getenv(\u0026#34;JWT_SECRET_KEY\u0026#34;, \u0026#34;your-secret-key\u0026#34;) # NÃªn lÆ°u trong AWS Secrets Manager JWT_ALGORITHM = \u0026#34;HS256\u0026#34; # Verify JWT token def get_current_user(token: str = Depends(oauth2_scheme)): credentials_exception = HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=\u0026#34;Could not validate credentials\u0026#34;, headers={\u0026#34;WWW-Authenticate\u0026#34;: \u0026#34;Bearer\u0026#34;}, ) try: payload = jwt.decode(token, JWT_SECRET_KEY, algorithms=[JWT_ALGORITHM]) username: str = payload.get(\u0026#34;sub\u0026#34;) if username is None: raise credentials_exception return {\u0026#34;username\u0026#34;: username} except PyJWTError: raise credentials_exception # Protected API endpoint @app.post(\u0026#34;/predict\u0026#34;, response_model=PredictionResponse) async def predict( request: PredictionRequest, current_user: dict = Depends(get_current_user) ): # Log the user who made the prediction logger.info(f\u0026#34;Prediction requested by user: {current_user[\u0026#39;username\u0026#39;]}\u0026#34;) # Process prediction as normal prediction = model.predict(request.features) return {\u0026#34;prediction\u0026#34;: float(prediction), \u0026#34;model_version\u0026#34;: model.version} sequenceDiagram\rparticipant Client\rparticipant ALB as ALB + WAF\rparticipant API as Retail API\rparticipant Auth as Authentication Service\rClient-\u003e\u003eAuth: Request JWT token (username/password)\rAuth-\u003e\u003eAuth: Verify credentials\rAuth-\u003e\u003eClient: Return JWT token\rClient-\u003e\u003eALB: POST /predict with Bearer token\rALB-\u003e\u003eALB: WAF rules check\rALB-\u003e\u003eAPI: Forward request\rAPI-\u003e\u003eAPI: Validate JWT token\rAPI-\u003e\u003eAPI: Perform prediction\rAPI-\u003e\u003eALB: Return prediction result\rALB-\u003e\u003eClient: Return response\rClient-\u003e\u003eALB: POST /predict (no token)\rALB-\u003e\u003eALB: WAF rules check\rALB-\u003e\u003eAPI: Forward request\rAPI-\u003e\u003eALB: 401 Unauthorized\rALB-\u003e\u003eClient: 401 Unauthorized\r6. Deployment vÃ  Verification 6.1 Deploy Load Balancer Services # Create mlops namespace if not exists kubectl create namespace mlops # Deploy LoadBalancer service kubectl apply -f service-nlb-advanced.yaml # Or deploy ALB Ingress kubectl apply -f ingress-alb.yaml # Check service status kubectl get services -n mlops # Check ingress status kubectl get ingress -n mlops 6.2 Verify External Access # Get external IP/hostname EXTERNAL_IP=$(kubectl get service retail-forecast-nlb -n mlops -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) echo \u0026#34;External endpoint: $EXTERNAL_IP\u0026#34; # Test health check curl http://$EXTERNAL_IP/healthz # Test prediction endpoint curl -X POST http://$EXTERNAL_IP/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;features\u0026#34;: { \u0026#34;store_id\u0026#34;: 1, \u0026#34;product_id\u0026#34;: 123, \u0026#34;date\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;price\u0026#34;: 29.99, \u0026#34;promotion\u0026#34;: 0 } }\u0026#39; 7. Load Testing vÃ  Performance 7.1 Load Testing with Apache Bench # Install Apache Bench # For Ubuntu/Debian: sudo apt-get install apache2-utils # For CentOS/RHEL: sudo yum install httpd-tools # Basic load test ab -n 1000 -c 10 http://$EXTERNAL_IP/healthz # POST request load test ab -n 100 -c 5 -p data.json -T application/json http://$EXTERNAL_IP/predict 7.2 Load Testing with Artillery # Install Artillery npm install -g artillery # Create load test configuration cat \u0026gt; load-test.yml \u0026lt;\u0026lt; EOF config: target: \u0026#39;http://$EXTERNAL_IP\u0026#39; phases: - duration: 60 arrivalRate: 10 - duration: 120 arrivalRate: 20 - duration: 60 arrivalRate: 5 scenarios: - name: \u0026#34;Health Check\u0026#34; weight: 30 flow: - get: url: \u0026#34;/healthz\u0026#34; - name: \u0026#34;Prediction\u0026#34; weight: 70 flow: - post: url: \u0026#34;/predict\u0026#34; json: features: store_id: 1 product_id: 123 date: \u0026#34;2024-01-01\u0026#34; price: 29.99 promotion: 0 EOF # Run load test artillery run load-test.yml 8. Monitoring vÃ  Observability 8.1 CloudWatch Metrics # Create CloudWatch dashboard for ALB metrics aws cloudwatch put-dashboard \\ --dashboard-name \u0026#34;RetailForecastALB\u0026#34; \\ --dashboard-body \u0026#39;{ \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/ApplicationELB\u0026#34;, \u0026#34;RequestCount\u0026#34;, \u0026#34;LoadBalancer\u0026#34;, \u0026#34;retail-forecast-alb\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;TargetResponseTime\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;HTTPCode_Target_2XX_Count\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;HTTPCode_Target_4XX_Count\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;], [\u0026#34;.\u0026#34;, \u0026#34;HTTPCode_Target_5XX_Count\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34;] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;ALB Metrics\u0026#34; } } ] }\u0026#39; 8.2 Prometheus Monitoring # servicemonitor-loadbalancer.yaml apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: retail-forecast-lb-monitor namespace: mlops labels: app: retail-forecast-api spec: selector: matchLabels: app: retail-forecast-api endpoints: - port: http path: /metrics interval: 30s scrapeTimeout: 10s --- 9. Troubleshooting 9.1 Common Load Balancer Issues # Check service events kubectl describe service retail-forecast-nlb -n mlops # Check ingress events kubectl describe ingress retail-forecast-alb-ingress -n mlops # Check ALB controller logs kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller # Check target group health aws elbv2 describe-target-health --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/k8s-mlops-retailfo-1234567890 9.2 Debug Network Connectivity # Test from inside cluster kubectl run debug-pod --image=nicolaka/netshoot -n mlops --rm -it -- /bin/bash # Inside the debug pod: # Test service connectivity curl http://retail-forecast-service.mlops.svc.cluster.local/healthz # Test DNS resolution nslookup retail-forecast-service.mlops.svc.cluster.local # Test external connectivity curl http://api.retail-forecast.com/healthz 10. Cost Optimization 10.1 ALB vs NLB Cost Comparison # ALB pricing (approximate) # - $0.0225 per ALB-hour # - $0.008 per LCU-hour (Load Balancer Capacity Unit) # NLB pricing (approximate) # - $0.0225 per NLB-hour # - $0.006 per NLCU-hour (Network Load Balancer Capacity Unit) # Monitor usage with AWS Cost Explorer aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE 10.2 Resource Optimization # Use target type \u0026#34;ip\u0026#34; for better performance apiVersion: v1 kind: Service metadata: annotations: service.beta.kubernetes.io/aws-load-balancer-target-type: \u0026#34;ip\u0026#34; # Enable cross-zone load balancing only if needed service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: \u0026#34;true\u0026#34; spec: # Use Local traffic policy to reduce network hops externalTrafficPolicy: Local --- Káº¿t quáº£ ká»³ vá»ng âœ… Checklist HoÃ n thÃ nh Application Load Balancer: Service type LoadBalancer vá»›i annotation aws-load-balancer-type: \u0026quot;application\u0026quot; Ä‘Æ°á»£c deploy thÃ nh cÃ´ng External Endpoint: ALB DNS Ä‘Æ°á»£c assign vÃ  cÃ³ thá»ƒ truy cáº­p Health Checks: ALB health checks tá»›i endpoint /health hoáº¡t Ä‘á»™ng Ä‘Ãºng API Access: CÃ¡c endpoint /predict vÃ  /docs cÃ³ thá»ƒ truy cáº­p tá»« internet Logging: Access logs Ä‘Æ°á»£c ghi vÃ o S3 bucket SSL/TLS: HTTPS endpoint Ä‘Æ°á»£c cáº¥u hÃ¬nh (optional) Domain Name: Custom domain Ä‘Æ°á»£c mapping qua Route 53 (optional) Security: WAF rules vÃ  security groups Ä‘Æ°á»£c cáº¥u hÃ¬nh Monitoring: CloudWatch ALB metrics Ä‘Æ°á»£c kÃ­ch hoáº¡t Load Testing: Performance testing Ä‘áº¡t cÃ¡c target vá» throughput vÃ  latency ğŸ“Š Verification Steps Service trong namespace retail-prediction cÃ³ EXTERNAL-IP hiá»ƒn thá»‹\nkubectl get services -n retail-prediction # Expected output: # NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE # retail-api-service LoadBalancer 10.100.123.45 retail-alb-123456789.us-east-1.elb.amazonaws.com 80:31234/TCP 5m CÃ³ thá»ƒ gá»­i request tá»« bÃªn ngoÃ i vÃ  nháº­n káº¿t quáº£ inference\n# Health check curl http://EXTERNAL-IP/health # Expected: {\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-01T12:00:00Z\u0026#34;} # API Documentation curl http://EXTERNAL-IP/docs # Expected: FastAPI Swagger UI loads # Prediction request curl -X POST http://EXTERNAL-IP/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;features\u0026#34;: {\u0026#34;store_id\u0026#34;: 1, \u0026#34;product_id\u0026#34;: 123, \u0026#34;date\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;price\u0026#34;: 29.99, \u0026#34;promotion\u0026#34;: 0}}\u0026#39; # Expected: {\u0026#34;prediction\u0026#34;: 150.75, \u0026#34;model_version\u0026#34;: \u0026#34;v1.0\u0026#34;} Load Balancer hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh vÃ  phÃ¢n phá»‘i traffic\n# Check target health aws elbv2 describe-target-health --target-group-arn \u0026lt;target-group-arn\u0026gt; # Expected: All targets should be \u0026#34;healthy\u0026#34; # Monitor load distribution kubectl logs -f deployment/retail-api -n retail-prediction # Should see requests distributed across different pods # Check ALB metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name RequestCount \\ --dimensions Name=LoadBalancer,Value=app/retail-alb/1234567890123456 \\ --start-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34; -d \u0026#34;1 hour ago\u0026#34;) \\ --end-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) \\ --period 300 \\ --statistics Sum ğŸ” Monitoring Commands # Check service status kubectl get svc -n retail-prediction -w # Monitor ALB controller status kubectl get deployment -n kube-system aws-load-balancer-controller # Check load balancer controller logs kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -f # Monitor target group health watch \u0026#34;aws elbv2 describe-target-health --target-group-arn \u0026lt;your-target-group-arn\u0026gt;\u0026#34; # Get ALB details aws elbv2 describe-load-balancers --names retail-api-alb # Check ALB metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name RequestCount \\ --dimensions Name=LoadBalancer,Value=app/retail-api-alb/1234567890123456 \\ --start-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34; -d \u0026#34;1 hour ago\u0026#34;) \\ --end-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) \\ --period 300 \\ --statistics Sum # Check target response time metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name TargetResponseTime \\ --dimensions Name=LoadBalancer,Value=app/retail-api-alb/1234567890123456 \\ --start-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34; -d \u0026#34;1 hour ago\u0026#34;) \\ --end-time $(date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) \\ --period 300 \\ --statistics Average 11. Clean Up Resources (AWS CLI) 11.1. XÃ³a Load Balancers # Liá»‡t kÃª ALBs aws elbv2 describe-load-balancers --query \u0026#39;LoadBalancers[?contains(LoadBalancerName, `retail`)].{Name:LoadBalancerName,ARN:LoadBalancerArn}\u0026#39; --output table # XÃ³a ALB (tá»± Ä‘á»™ng xÃ³a listeners) aws elbv2 delete-load-balancer --load-balancer-arn \u0026lt;alb-arn\u0026gt; # Liá»‡t kÃª Target Groups aws elbv2 describe-target-groups --query \u0026#39;TargetGroups[?contains(TargetGroupName, `retail`)].{Name:TargetGroupName,ARN:TargetGroupArn}\u0026#39; --output table # XÃ³a Target Groups aws elbv2 delete-target-group --target-group-arn \u0026lt;target-group-arn\u0026gt; 11.2. XÃ³a AWS Load Balancer Controller # Uninstall AWS Load Balancer Controller helm uninstall aws-load-balancer-controller -n kube-system # XÃ³a IRSA role cho load balancer controller aws iam detach-role-policy \\ --role-name AmazonEKSLoadBalancerControllerRole \\ --policy-arn arn:aws:iam::\u0026lt;account-id\u0026gt;:policy/AWSLoadBalancerControllerIAMPolicy aws iam delete-role --role-name AmazonEKSLoadBalancerControllerRole # XÃ³a IAM policy aws iam delete-policy --policy-arn arn:aws:iam::\u0026lt;account-id\u0026gt;:policy/AWSLoadBalancerControllerIAMPolicy 11.3. XÃ³a Ingress Resources # XÃ³a Ingress resources kubectl delete ingress --all -n mlops # XÃ³a IngressClass kubectl delete ingressclass alb # Verify cleanup kubectl get ingress -A kubectl get ingressclass 11.4. XÃ³a WAF vÃ  Security Configurations # Liá»‡t kÃª WAF Web ACLs aws wafv2 list-web-acls --scope REGIONAL --region ap-southeast-1 # XÃ³a WAF rules trÆ°á»›c aws wafv2 update-web-acl \\ --scope REGIONAL \\ --id \u0026lt;web-acl-id\u0026gt; \\ --lock-token \u0026lt;lock-token\u0026gt; \\ --rules \u0026#39;[]\u0026#39; \\ --default-action Allow={} # XÃ³a WAF Web ACL aws wafv2 delete-web-acl \\ --scope REGIONAL \\ --id \u0026lt;web-acl-id\u0026gt; \\ --lock-token \u0026lt;lock-token\u0026gt; 11.5. Clean Up SSL Certificates # Liá»‡t kÃª ACM certificates aws acm list-certificates --region ap-southeast-1 --query \u0026#39;CertificateSummaryList[*].{Domain:DomainName,ARN:CertificateArn}\u0026#39; # XÃ³a certificate (chá»‰ khi khÃ´ng cÃ²n sá»­ dá»¥ng) aws acm delete-certificate --certificate-arn \u0026lt;certificate-arn\u0026gt; --region ap-southeast-1 # XÃ³a Route53 records aws route53 change-resource-record-sets \\ --hosted-zone-id \u0026lt;zone-id\u0026gt; \\ --change-batch \u0026#39;{ \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;DELETE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;api.retail-prediction.example.com\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;DNSName\u0026#34;: \u0026#34;\u0026lt;alb-dns-name\u0026gt;\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false, \u0026#34;HostedZoneId\u0026#34;: \u0026#34;\u0026lt;alb-zone-id\u0026gt;\u0026#34; } } }] }\u0026#39; 11.6. Load Balancing Cleanup Script #!/bin/bash # loadbalancer-cleanup.sh NAMESPACE=\u0026#34;mlops\u0026#34; CLUSTER_NAME=\u0026#34;retail-prediction-cluster\u0026#34; REGION=\u0026#34;ap-southeast-1\u0026#34; echo \u0026#34;ğŸ§¹ Cleaning up Load Balancer resources...\u0026#34; # 1. Delete Kubernetes resources echo \u0026#34;Deleting Ingress resources...\u0026#34; kubectl delete ingress --all -n $NAMESPACE kubectl delete service --selector=app=retail-api -n $NAMESPACE # 2. Wait for ALB deletion echo \u0026#34;Waiting for ALB deletion...\u0026#34; sleep 60 # 3. Clean up AWS resources echo \u0026#34;Cleaning up AWS Load Balancer Controller...\u0026#34; helm uninstall aws-load-balancer-controller -n kube-system 2\u0026gt;/dev/null || true # 4. Delete remaining target groups echo \u0026#34;Cleaning up target groups...\u0026#34; TARGET_GROUPS=$(aws elbv2 describe-target-groups --query \u0026#39;TargetGroups[?contains(TargetGroupName, `k8s-`)].TargetGroupArn\u0026#39; --output text) for tg in $TARGET_GROUPS; do aws elbv2 delete-target-group --target-group-arn $tg 2\u0026gt;/dev/null || true done echo \u0026#34;âœ… Load Balancer cleanup completed\u0026#34; 12. Báº£ng giÃ¡ Load Balancing (ap-southeast-1) 12.1. Chi phÃ­ Application Load Balancer (ALB) Component GiÃ¡ (USD/hour) GiÃ¡ (USD/month) Ghi chÃº ALB Base Cost $0.0225 $16.43 Per ALB instance LCU (Load Balancer Capacity Unit) $0.008 $5.84 Per LCU-hour New connections Included Included Up to 25/second per LCU Active connections Included Included Up to 3,000 per LCU Data processed Included Included Up to 1GB per LCU Rule evaluations Included Included Up to 1,000 per LCU 12.2. Chi phÃ­ Network Load Balancer (NLB) Component GiÃ¡ (USD/hour) GiÃ¡ (USD/month) Ghi chÃº NLB Base Cost $0.0225 $16.43 Per NLB instance NLCU (Network LCU) $0.006 $4.38 Per NLCU-hour New connections/flows Included Included Up to 800/second per NLCU Active connections/flows Included Included Up to 100,000 per NLCU Data processed Included Included Up to 1GB per NLCU 12.3. Chi phÃ­ Classic Load Balancer (CLB) Component GiÃ¡ (USD/hour) GiÃ¡ (USD/month) Ghi chÃº CLB Base Cost $0.025 $18.25 Per CLB instance Data Transfer $0.008/GB Variable Data processed 12.4. So sÃ¡nh cÃ¡c loáº¡i Load Balancer Feature ALB NLB CLB Best For Layer Layer 7 (HTTP/HTTPS) Layer 4 (TCP/UDP) Layer 4/7 ALB: Web apps, NLB: High performance Base Cost $16.43/month $16.43/month $18.25/month ALB/NLB cheaper Capacity Units LCU ($5.84) NLCU ($4.38) Fixed NLB most cost-effective SSL Termination âœ… âœ… âœ… All support Path-based Routing âœ… âŒ âŒ ALB only WebSocket âœ… âœ… âŒ ALB/NLB Static IP âŒ âœ… âŒ NLB only 12.5. AWS Load Balancer Controller Costs Component Cost Ghi chÃº Controller Pods Free Runs on existing EKS nodes IRSA Role Free IAM integration Webhook Certificate Free TLS for admission controller Target Group Binding Free CRD for pod registration Ingress Management Free Kubernetes native 12.6. SSL/TLS Certificate Costs Service Cost Features AWS Certificate Manager (ACM) Free Public SSL certificates Route 53 DNS $0.50/hosted zone Domain validation Third-party Certificates $10-100/year Extended validation options 12.7. WAF (Web Application Firewall) Costs Component GiÃ¡ (USD/month) Ghi chÃº WAF Web ACL $1 Per Web ACL WAF Rules $0.60 Per rule per month WAF Requests $0.60 Per million requests Bot Control $10 Advanced bot protection Rate Limiting $2 Per rate-based rule 12.8. Æ¯á»›c tÃ­nh chi phÃ­ Task 9 Basic ALB Setup:\nComponent Usage Monthly Cost ALB Base 1 ALB $16.43 LCU Usage 1 LCU average $5.84 ACM Certificate 1 domain $0 Route 53 1 hosted zone $0.50 WAF (optional) Basic rules $3.20 Total $26.00 Production Setup vá»›i High Availability:\nComponent Usage Monthly Cost ALB Base 1 ALB (Multi-AZ) $16.43 LCU Usage 3 LCU average $17.52 ACM Certificate 2 domains $0 Route 53 2 hosted zones $1.00 WAF Advanced rules $15.20 CloudFront CDN integration $8.50 Total $58.65 12.9. Cost Optimization Strategies Single ALB for Multiple Services:\n# Cost-effective: 1 ALB serves multiple applications apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: shared-alb-ingress annotations: kubernetes.io/ingress.class: alb alb.ingress.kubernetes.io/group.name: shared-alb spec: rules: - host: api.retail.com http: paths: - path: /predict backend: service: name: retail-api port: number: 80 - host: admin.retail.com http: paths: - path: / backend: service: name: admin-dashboard port: number: 80 Right-size Load Balancer:\nMonitor LCU/NLCU usage vá»›i CloudWatch Optimize connection pooling Use appropriate health check intervals Configure proper idle timeouts 12.10. Monitoring vÃ  Cost Control # Monitor ALB metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name ConsumedLCUs \\ --dimensions Name=LoadBalancer,Value=\u0026lt;alb-full-name\u0026gt; \\ --start-time 2024-01-01T00:00:00Z \\ --end-time 2024-01-31T23:59:59Z \\ --period 3600 \\ --statistics Average,Maximum # Check ALB costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon Elastic Load Balancing\u0026#34;]}}\u0026#39; # Monitor target health aws elbv2 describe-target-health \\ --target-group-arn \u0026lt;target-group-arn\u0026gt; \\ --query \u0026#39;TargetHealthDescriptions[*].{Target:Target.Id,Health:TargetHealth.State}\u0026#39; Cost alerts:\n# Create cost alarm for Load Balancing aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;LoadBalancer-Cost-Alert\u0026#34; \\ --alarm-description \u0026#34;Alert when Load Balancer cost \u0026gt; $50/month\u0026#34; \\ --metric-name EstimatedCharges \\ --namespace AWS/Billing \\ --statistic Maximum \\ --period 86400 \\ --threshold 50 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=Currency,Value=USD Name=ServiceName,Value=AmazonELB ğŸ’° Cost Summary cho Task 9:\nBasic ALB: $26/month (single service) Shared ALB: $22/month (multiple services sharing 1 ALB) Production: $58.65/month (vá»›i WAF, CloudFront) Optimization: 15-30% savings vá»›i proper resource sharing ğŸ¬ Video thá»±c hiá»‡n Task 9 Next Step: Task 10: CloudWatch Monitoring \u0026amp; Alerting\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/3-blogstranslated/3.10-blog10/",
	"title": "CÃ´ng bá»‘ AWS Partner Central trong AWS Management Console",
	"tags": [],
	"description": "",
	"content": "TÃ¡c giáº£: Raj Kandaswamy, Nicole Schreiber, vÃ  Christin Voytko â€” 30 NOV 2025\nBá»Ÿi Raj Kandaswamy, Principal PMT-ES â€“ AWS\nBá»Ÿi Nicole Schreiber, Partner Experience Lead â€“ AWS\nBá»Ÿi Christin Voytko, Partner Experience Lead â€“ AWS\nChÃºng tÃ´i ráº¥t vui má»«ng thÃ´ng bÃ¡o ráº±ng AWS Partner Central hiá»‡n Ä‘Ã£ cÃ³ sáºµn trong AWS Management Console, giÃºp Ä‘Æ¡n giáº£n hÃ³a viá»‡c truy cáº­p Partner Central vÃ  AWS Marketplace Management Portal. Láº§n ra máº¯t nÃ y giá»›i thiá»‡u cÃ¡c API máº¡nh máº½ cung cáº¥p kháº£ nÄƒng tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh vÃ  tÃ­ch há»£p, giÃºp Partners káº¿t ná»‘i trá»±c tiáº¿p cÃ¡c há»‡ thá»‘ng kinh doanh cá»§a há» vá»›i Partner Central, Ä‘á»“ng thá»i tinh gá»n cÃ¡ch há» phÃ¡t triá»ƒn vÃ  má»Ÿ rá»™ng cÃ¹ng Amazon Web Services (AWS).\nÄÆ°á»£c xÃ¢y dá»±ng trÃªn AWS Identity and Access Management (IAM), tráº£i nghiá»‡m nÃ¢ng cao nÃ y cung cáº¥p cÃ¡c tÃ­nh nÄƒng báº£o máº­t cáº¥p doanh nghiá»‡p vÃ  kháº£ nÄƒng quáº£n lÃ½ ngÆ°á»i dÃ¹ng linh hoáº¡t cÃ¹ng kháº£ nÄƒng single sign-on (SSO)â€”giÃºp báº¡n dá»… dÃ ng hÆ¡n trong viá»‡c cung cáº¥p cho Ä‘á»™i ngÅ© quyá»n truy cáº­p vÃ o cÃ¡c cÃ´ng cá»¥ há» cáº§n Ä‘á»ƒ váº­n hÃ nh hoáº¡t Ä‘á»™ng kinh doanh AWS cá»§a báº¡n.\nNhá»¯ng Ä‘iá»ƒm má»›i trong tráº£i nghiá»‡m Partner Central nÃ¢ng cao Tráº£i nghiá»‡m Partner Central má»›i Ä‘Æ°á»£c xÃ¢y dá»±ng Ä‘á»ƒ há»— trá»£ cÃ¡c Ä‘á»™i nhÃ³m khi há» má»Ÿ rá»™ng cÃ¹ng AWS. Báº±ng cÃ¡ch há»£p nháº¥t quyá»n truy cáº­p vÃ  cÃ¡c há»‡ thá»‘ng, cÃ¡c Ä‘á»™i nhÃ³m cÃ³ thá»ƒ Ä‘iá»u hÆ°á»›ng nhanh hÆ¡n Ä‘áº¿n cÃ¡c cÃ´ng cá»¥ há» cáº§n Ä‘á»ƒ Ä‘á»“ng bÃ¡n (co-sell) hiá»‡u quáº£ vá»›i AWS vÃ  cÃ¡c Partners khÃ¡c. Tráº£i nghiá»‡m nÃ¢ng cao nÃ y cho phÃ©p báº¡n:\nChuyá»ƒn Ä‘á»•i váº­n hÃ nh vá»›i tÃ­ch há»£p vÃ  tá»± Ä‘á»™ng hÃ³a máº¡nh máº½ Káº¿t ná»‘i trá»±c tiáº¿p cÃ¡c há»‡ thá»‘ng kinh doanh hiá»‡n cÃ³ cá»§a báº¡n vá»›i Partner Central thÃ´ng qua cÃ¡c API Ä‘Æ°á»£c má»Ÿ rá»™ng, cho phÃ©p tÃ­ch há»£p liá»n máº¡ch vÃ  tá»± Ä‘á»™ng hÃ³a luá»“ng cÃ´ng viá»‡c. Loáº¡i bá» viá»‡c nháº­p dá»¯ liá»‡u trÃ¹ng láº·p giá»¯a Ä‘á»“ng bÃ¡n (co-selling) vÃ  cÃ¡c giao dá»‹ch AWS Marketplace, vÃ  tinh gá»n váº­n hÃ nh trÃªn toÃ n bá»™ quáº£n lÃ½ giáº£i phÃ¡p, Ä‘Äƒng kÃ½ quyá»n lá»£i, vÃ  yÃªu cáº§u tÃ i trá»£. CÃ¡c API cá»§a Partner Central bao gá»“m:\nAccount and Connections APIs cho Ä‘Äƒng kÃ½ Partner theo láº­p trÃ¬nh, quáº£n lÃ½ há»“ sÆ¡, vÃ  cá»™ng tÃ¡c Ä‘a Partner thÃ´ng qua cÃ¡c cÆ¡ há»™i chung (joint opportunities). Solution APIs Ä‘á»ƒ táº¡o vÃ  quáº£n lÃ½ danh sÃ¡ch giáº£i phÃ¡p (solution listings) trong AWS Marketplace. Benefits APIs cho viá»‡c khÃ¡m phÃ¡ quyá»n lá»£i táº­p trung vÃ  Ã¡p dá»¥ng quyá»n lá»£i xuyÃªn suá»‘t cÃ¡c chÆ°Æ¡ng trÃ¬nh Partner. Selling and Leads APIs hiá»‡n há»— trá»£ toÃ n bá»™ vÃ²ng Ä‘á»i Ä‘á»“ng bÃ¡nâ€”tá»« táº¡o lead vÃ  Ä‘Ã¡nh giÃ¡ Ä‘á»§ Ä‘iá»u kiá»‡n Ä‘áº¿n chuyá»ƒn Ä‘á»•i thÃ nh opportunityâ€”vá»›i theo dÃµi tiáº¿n trÃ¬nh deal theo thá»i gian thá»±c. CÃ¡c Partners Ä‘Ã£ vÃ  Ä‘ang táº­n dá»¥ng cÃ¡c API nÃ y Ä‘á»ƒ tinh gá»n váº­n hÃ nh. â€œLeveraging the Benefits API, weâ€™ve streamlined the application process for Marketplace Private Offer Promotion Program (MPOPP) applications,â€ says Adam Boyle, VP of Product \u0026amp; Engineering at Tackle.io. â€œWith this integration, users can now access pre-populated data, use AI-assisted justifications, and submit applications with a single click. These API capabilities allow sellers to manage fund requests from within the systems they are already working in on a day-to-day basis.â€\nTruy cáº­p API Reference Ä‘á»ƒ báº¯t Ä‘áº§u tá»± xÃ¢y dá»±ng cÃ¡c tÃ­ch há»£p tÃ¹y chá»‰nh hoáº·c káº¿t ná»‘i vá»›i má»™t nhÃ  cung cáº¥p cÃ³ thá»ƒ giÃºp báº¡n triá»ƒn khai tá»± Ä‘á»™ng hÃ³a cho Partner Central vÃ  AWS Marketplace.\nNháº­n hÆ°á»›ng dáº«n cÃ¡ nhÃ¢n hÃ³a báº±ng Amazon Q chat Partner Assistant Ä‘Æ°á»£c nÃ¢ng cáº¥p, nay Ä‘Ã£ tÃ­ch há»£p vá»›i Amazon Q, cung cáº¥p cÃ¡c pháº£n há»“i Ä‘Æ°á»£c â€œÄ‘o ni Ä‘Ã³ng giÃ yâ€ Ä‘á»ƒ giÃºp báº¡n Ä‘iá»u hÆ°á»›ng hÃ nh trÃ¬nh AWS Partner cá»§a mÃ¬nh. Nháº­n cÃ¡c khuyáº¿n nghá»‹ vÃ  insight cÃ¡ nhÃ¢n hÃ³a dá»±a trÃªn Partner Path, tier, giáº£i phÃ¡p, pipeline opportunity, vÃ  danh sÃ¡ch sáº£n pháº©m (product listings). Há»i cÃ¢u há»i báº±ng ngÃ´n ngá»¯ Ä‘á»‹a phÆ°Æ¡ng báº¡n Æ°u tiÃªn vá» báº¥t cá»© Ä‘iá»u gÃ¬, tá»« tá»‘i Ä‘a hÃ³a viá»‡c sá»­ dá»¥ng cÃ¡c quyá»n lá»£i AWS Partner Ä‘áº¿n kiá»ƒm tra tráº¡ng thÃ¡i cá»§a má»™t customer opportunity.\nMá»Ÿ rá»™ng váº­n hÃ nh vá»›i kiá»ƒm soÃ¡t truy cáº­p linh hoáº¡t ThÃ´ng qua AWS IAM, báº¡n cÃ³ thá»ƒ quáº£n lÃ½ quyá»n má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  sá»­ dá»¥ng Workforce Identity Provider hiá»‡n cÃ³ cá»§a báº¡n cho SSO. IAM cho báº¡n quyá»n kiá»ƒm soÃ¡t chÃ­nh xÃ¡c Ä‘á»‘i vá»›i cÃ¡c thiáº¿t láº­p truy cáº­p cá»§a ngÆ°á»i dÃ¹ng mÃ  khÃ´ng cÃ³ cÃ¡c giá»›i háº¡n truyá»n thá»‘ngâ€”nhÆ° bá»‹ giá»›i háº¡n chá»‰ má»™t Alliance Lead. Triá»ƒn khai cÃ¡c kiá»ƒm soÃ¡t truy cáº­p chi tiáº¿t (granular) Ä‘á»ƒ phÃ¹ há»£p vá»›i nhu cáº§u tá»• chá»©c cá»§a báº¡n, cháº³ng háº¡n nhÆ° giá»›i háº¡n cÃ¡c opportunity cá»¥ thá»ƒ cho cÃ¡c Ä‘á»™i nhÃ³m Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh.\nVá»›i SSO, cÃ¡c thÃ nh viÃªn trong nhÃ³m cá»§a báº¡n cÃ³ thá»ƒ truy cáº­p cÃ¡c cÃ´ng cá»¥ vÃ  tÃ i nguyÃªn Partner báº±ng má»™t bá»™ thÃ´ng tin Ä‘Äƒng nháº­p duy nháº¥t. Truy cáº­p hÆ°á»›ng dáº«n AWS IAM Identity Center Ä‘á»ƒ cÃ³ hÆ°á»›ng dáº«n tá»«ng bÆ°á»›c vá» viá»‡c káº¿t ná»‘i vá»›i cÃ¡c identity providers. CÃ¡c Partners sá»­ dá»¥ng Okta cÃ³ thá»ƒ táº­n dá»¥ng tÃ­ch há»£p AWS Identity Center cá»§a Okta Ä‘á»ƒ tinh gá»n onboarding ngÆ°á»i dÃ¹ng vÃ  báº­t SSO trÃªn toÃ n tá»• chá»©c cá»§a há».\nCÃ¡c Partners Ä‘Ã£ vÃ  Ä‘ang tráº£i nghiá»‡m nhá»¯ng lá»£i Ã­ch nÃ y. â€œArctic Wolfâ€™s onboarding to AWS Partner Central in the AWS Console was incredibly smooth using AWS Identity Center with our existing identity providerâ€”our IT department was able to provision users in minutes,â€ says Sean Phillips, VP of Strategic Alliances at Arctic Wolf. â€œIâ€™m excited to no longer manage individual user permissions myself, knowing our IT team has the security controls and streamlined administration they need through IAM. Weâ€™re looking forward to leveraging the new API capabilities and automated workflows to transform how we manage our AWS partnership.â€\nMá»Ÿ rá»™ng pháº¡m vi tiáº¿p cáº­n vá»›i kháº£ nÄƒng khÃ¡m phÃ¡ giáº£i phÃ¡p trong AWS Marketplace Trong Partner Central, giá» Ä‘Ã¢y báº¡n cÃ³ thá»ƒ xuáº¥t báº£n cÃ¡c giáº£i phÃ¡p Ä‘a sáº£n pháº©m, Ä‘a nhÃ  cung cáº¥p Ä‘á»ƒ cÃ´ng khai khÃ¡m phÃ¡ trong AWS Marketplace. NgoÃ i viá»‡c sá»­ dá»¥ng Partner Connections Ä‘á»ƒ tÃ¬m cÃ¡c Partners bá»• trá»£ cho hoáº¡t Ä‘á»™ng Ä‘á»“ng bÃ¡n, giá» Ä‘Ã¢y báº¡n cÃ³ thá»ƒ táº¡o cÃ¡c giáº£i phÃ¡p Ä‘a sáº£n pháº©m bao gá»“m nhiá»u thÃ nh pháº§n pháº§n má»m vÃ  dá»‹ch vá»¥ tá»« nhiá»u nhÃ  cung cáº¥p, vÃ  niÃªm yáº¿t cÃ´ng khai trong AWS Marketplace. KhÃ¡ch hÃ ng cÃ³ thá»ƒ khÃ¡m phÃ¡ cÃ¡c giáº£i phÃ¡p cá»§a báº¡n trong AWS Marketplace báº±ng cÃ¡ch tÃ¬m kiáº¿m theo váº¥n Ä‘á» kinh doanh (cháº³ng háº¡n nhÆ° â€œdata migrationâ€) thay vÃ¬ theo cÃ¡c sáº£n pháº©m cá»¥ thá»ƒ, giÃºp há» dá»… dÃ ng hÆ¡n trong viá»‡c tÃ¬m Ä‘Ãºng giáº£i phÃ¡p.\nTrong tÆ°Æ¡ng lai, khi chÃºng tÃ´i tiáº¿p tá»¥c nÃ¢ng cao tráº£i nghiá»‡m cho Partners, chÃºng tÃ´i Ä‘ang Ä‘áº§u tÆ° vÃ o cÃ¡c kháº£ nÄƒng á»©ng dá»¥ng AI Ä‘á»ƒ tÄƒng tá»‘c Ä‘Ã¡nh giÃ¡ ká»¹ thuáº­t, luá»“ng phÃª duyá»‡t, vÃ  cÃ¡c chuyá»ƒn Ä‘á»™ng Ä‘á»“ng bÃ¡nâ€”giÃºp báº¡n cáº£i thiá»‡n hiá»‡u quáº£ váº­n hÃ nh vÃ  má»Ÿ rá»™ng hoáº¡t Ä‘á»™ng kinh doanh nhanh hÆ¡n vá»›i AWS.\nBáº¯t Ä‘áº§u Truy cáº­p trang web AWS Partner Central Ä‘á»ƒ tÃ¬m hiá»ƒu thÃªm vá» tráº£i nghiá»‡m má»›i. CÃ¡c Partners má»›i cÃ³ thá»ƒ báº¯t Ä‘áº§u báº±ng cÃ¡ch Ä‘Äƒng kÃ½ trá»Ÿ thÃ nh AWS Partner trong AWS Console.\nCÃ¡c Partners hiá»‡n cÃ³ cáº§n thá»±c hiá»‡n hÃ nh Ä‘á»™ng Ä‘á»ƒ chuyá»ƒn sang tráº£i nghiá»‡m Partner Central má»›i:\nPhá»‘i há»£p vá»›i bá»™ pháº­n IT cá»§a báº¡n Ä‘á»ƒ liÃªn káº¿t tÃ i khoáº£n APN vá»›i má»™t tÃ i khoáº£n AWS vÃ  láº­p káº¿ hoáº¡ch chiáº¿n lÆ°á»£c onboarding ngÆ°á»i dÃ¹ng. Äá»ƒ onboard ngÆ°á»i dÃ¹ng, hÃ£y káº¿t ná»‘i vá»›i Identity Provider hiá»‡n cÃ³ cá»§a cÃ´ng ty báº¡n, sá»­ dá»¥ng AWS IAM Identity Center, hoáº·c quáº£n lÃ½ quyá»n trá»±c tiáº¿p trong IAM console. LÃªn lá»‹ch migration cá»§a báº¡n trong Partner Centralridge, Cloudsoft, CloudZone, Innovative Solutions, Labra, Rackspace, Spektra Systems, Suger, Tackle.io, vÃ  WorkSpan. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.9-week9/",
	"title": "Tuáº§n 9 - Äiá»‡n toÃ¡n Container",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 9: Náº¯m vá»¯ng Containerization (Docker) vÃ  Orchestration (ECS/Fargate). Quáº£n lÃ½ container images báº±ng ECR. CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - Docker: Táº¡o Multi-stage Dockerfiles Ä‘á»ƒ tá»‘i Æ°u á»©ng dá»¥ng.\n- Build vÃ  test images trÃªn mÃ¡y local. Docker Docs 3 - ECR: Thiáº¿t láº­p Private ECR Repositories kÃ¨m Lifecycle policies.\n- Push images lÃªn ECR. AWS ECR Docs 4 - ECS: Táº¡o ECS Cluster vÃ  Task Definitions (Fargate).\n- Äá»‹nh nghÄ©a Task Role/Execution Role. AWS ECS Docs 5 - Service: Triá»ƒn khai ECS Service tÃ­ch há»£p vá»›i ALB.\n- Cáº¥u hÃ¬nh Auto Scaling. AWS ELB/ASG 6 - Ã”n táº­p: XÃ¡c minh viá»‡c triá»ƒn khai container vÃ  hÃ nh vi scaling.\n- BÃ¡o cÃ¡o tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 9: Tá»‘i Æ°u container images báº±ng multi-stage builds. Triá»ƒn khai á»©ng dá»¥ng container hÃ³a cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng trÃªn ECS Fargate. Báº£o máº­t giao tiáº¿p container báº±ng Security Groups vÃ  IAM Roles. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/10-cloudwatch/",
	"title": "CloudWatch Monitoring",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Má»¥c tiÃªu Task 10:\nThiáº¿t láº­p CloudWatch monitoring cÆ¡ báº£n cho EKS cluster vÃ  retail API.\n1. Container Insights # Enable Container Insights cho EKS cluster aws eks update-addon \\ --cluster-name mlops-retail-cluster \\ --addon-name amazon-cloudwatch-observability \\ --addon-version v1.0.0-eksbuild.1 # Táº¡o Log Groups aws logs create-log-group \\ --log-group-name /aws/containerinsights/mlops-retail-cluster/application \\ --retention-in-days 7 Tip: Chá»n retention há»£p lÃ½ (vÃ­ dá»¥ 7â€“30 ngÃ y) cho log groups dá»±a trÃªn yÃªu cáº§u debug vÃ  compliance Ä‘á»ƒ cÃ¢n báº±ng chi phÃ­ vÃ  kháº£ nÄƒng truy váº¿t.\n2. Basic Alarms # CPU alarm aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;EKS-HighCPU\u0026#34; \\ --alarm-description \u0026#34;CPU \u0026gt; 80%\u0026#34; \\ --metric-name node_cpu_utilization \\ --namespace ContainerInsights \\ --statistic Average \\ --period 300 \\ --threshold 80 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=ClusterName,Value=mlops-retail-cluster # Memory alarm aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;EKS-HighMemory\u0026#34; \\ --alarm-description \u0026#34;Memory \u0026gt; 85%\u0026#34; \\ --metric-name node_memory_utilization \\ --namespace ContainerInsights \\ --statistic Average \\ --period 300 \\ --threshold 85 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=ClusterName,Value=mlops-retail-cluster Info: NÃªn cáº¥u hÃ¬nh action cho alarms (vÃ­ dá»¥ SNS topic hoáº·c webhook tá»›i PagerDuty) Ä‘á»ƒ tá»± Ä‘á»™ng thÃ´ng bÃ¡o cho team khi alarm trigger. Kiá»ƒm tra ká»¹ metric namespace vÃ  dimension trÆ°á»›c khi báº­t alarm.\n3. SageMaker Training Logs # Táº¡o SageMaker Training Job vá»›i logging configuration aws sagemaker create-training-job \\ --training-job-name retail-prediction-training \\ --algorithm-specification TrainingImage=842676018087.dkr.ecr.ap-southeast-1.amazonaws.com/mlops/retail-api:v3 \\ --role-arn arn:aws:iam::842676018087:role/SageMakerExecutionRole \\ --input-data-config \u0026#39;[{ \u0026#34;ChannelName\u0026#34;: \u0026#34;training\u0026#34;, \u0026#34;DataSource\u0026#34;: { \u0026#34;S3DataSource\u0026#34;: { \u0026#34;S3DataType\u0026#34;: \u0026#34;S3Prefix\u0026#34;, \u0026#34;S3Uri\u0026#34;: \u0026#34;s3://retail-prediction-bucket/data/\u0026#34;, \u0026#34;S3DataDistributionType\u0026#34;: \u0026#34;FullyReplicated\u0026#34; } }, \u0026#34;ContentType\u0026#34;: \u0026#34;text/csv\u0026#34;, \u0026#34;CompressionType\u0026#34;: \u0026#34;None\u0026#34; }]\u0026#39; \\ --output-data-config S3OutputPath=s3://retail-prediction-bucket/model-artifacts/ \\ --resource-config \u0026#39;{ \u0026#34;InstanceType\u0026#34;: \u0026#34;ml.m5.xlarge\u0026#34;, \u0026#34;InstanceCount\u0026#34;: 1, \u0026#34;VolumeSizeInGB\u0026#34;: 50 }\u0026#39; \\ --stopping-condition MaxRuntimeInSeconds=3600 # Log Analysis vá»›i CloudWatch Insights aws logs start-query \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training\u0026#34; \\ --start-time $(date -d \u0026#39;24 hour ago\u0026#39; +%s) \\ --end-time $(date +%s) \\ --query-string \u0026#39; filter @message like \u0026#34;loss\u0026#34; | parse @message \u0026#34;loss: *,\u0026#34; as loss_value | parse @message \u0026#34;epoch * \u0026#34; as epoch_num | stats avg(loss_value) as avg_loss by bin(1h)\u0026#39; # Táº¡o metric filter cho training loss aws logs put-metric-filter \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training\u0026#34; \\ --filter-name \u0026#34;TrainingLoss\u0026#34; \\ --filter-pattern \u0026#39;\u0026#34;loss: \u0026#34;\u0026#39; \\ --metric-transformations \\ metricName=TrainingLoss,metricNamespace=RetailMLOps,metricValue=1,unit=None Warning: SageMaker training logs cÃ³ thá»ƒ sinh lÆ°á»£ng lá»›n dá»¯ liá»‡u (Ä‘áº·c biá»‡t khi in nhiá»u dÃ²ng hoáº·c debug verbose). Giá»›i háº¡n level logging trong training scripts vÃ  cÃ¢n nháº¯c filter/metric extraction Ä‘á»ƒ trÃ¡nh chi phÃ­ ingestion/storage Ä‘á»™t biáº¿n.\n4. Verification # Check Container Insights kubectl get pods -n amazon-cloudwatch # Check alarms aws cloudwatch describe-alarms --alarm-names \u0026#34;EKS-HighCPU\u0026#34; \u0026#34;EKS-HighMemory\u0026#34; # Check logs aws logs describe-log-groups --log-group-name-prefix \u0026#34;/aws/containerinsights/mlops-retail-cluster\u0026#34; # Check SageMaker training logs aws logs describe-log-streams \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training\u0026#34; ğŸ¯ Task 10 Complete - CloudWatch Monitoring\nContainer Insights enabled cho EKS cluster CloudWatch Alarms configured cho CPU/Memory SageMaker training logs vá»›i custom metrics Log Groups vá»›i retention policy Note: TrÆ°á»›c khi xÃ³a log groups hoáº·c metric filters, export hoáº·c snapshot dashboard quan trá»ng (vÃ­ dá»¥ Grafana/CSV) náº¿u cáº§n lÆ°u trá»¯ cho audit hoáº·c so sÃ¡nh sau nÃ y.\n5. Clean Up Resources 5.1 XÃ³a CloudWatch Alarms # List táº¥t cáº£ alarms liÃªn quan Ä‘áº¿n dá»± Ã¡n aws cloudwatch describe-alarms \\ --alarm-name-prefix \u0026#34;EKS-\u0026#34; \\ --query \u0026#39;MetricAlarms[].AlarmName\u0026#39; \\ --output table # XÃ³a specific alarms aws cloudwatch delete-alarms \\ --alarm-names \u0026#34;EKS-HighCPU\u0026#34; \u0026#34;EKS-HighMemory\u0026#34; # XÃ³a alarms liÃªn quan Ä‘áº¿n SageMaker aws cloudwatch describe-alarms \\ --alarm-name-prefix \u0026#34;SageMaker-\u0026#34; \\ --query \u0026#39;MetricAlarms[].AlarmName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | xargs -I {} aws cloudwatch delete-alarms --alarm-names {} 5.2 XÃ³a Log Groups vÃ  Metric Filters # List táº¥t cáº£ log groups liÃªn quan aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/containerinsights/mlops-retail-cluster\u0026#34; \\ --query \u0026#39;logGroups[].logGroupName\u0026#39; \\ --output table # XÃ³a Container Insights log groups aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/containerinsights/mlops-retail-cluster/application\u0026#34; aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/containerinsights/mlops-retail-cluster/dataplane\u0026#34; aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/containerinsights/mlops-retail-cluster/host\u0026#34; # XÃ³a SageMaker training log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/sagemaker/TrainingJobs\u0026#34; \\ --query \u0026#39;logGroups[].logGroupName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read log_group; do echo \u0026#34;Deleting log group: $log_group\u0026#34; aws logs delete-log-group --log-group-name \u0026#34;$log_group\u0026#34; done # XÃ³a EKS cluster log groups aws logs delete-log-group --log-group-name \u0026#34;/aws/eks/mlops-retail-cluster/cluster\u0026#34; || true 5.3 Disable Container Insights # Disable Container Insights addon aws eks delete-addon \\ --cluster-name mlops-retail-cluster \\ --addon-name amazon-cloudwatch-observability # XÃ³a CloudWatch agent tá»« EKS cluster kubectl delete namespace amazon-cloudwatch || true # Remove CloudWatch agent DaemonSet náº¿u cÃ²n kubectl delete daemonset cloudwatch-agent -n amazon-cloudwatch || true kubectl delete configmap cwagentconfig -n amazon-cloudwatch || true kubectl delete serviceaccount cloudwatch-agent -n amazon-cloudwatch || true 5.4 XÃ³a Custom Metrics vÃ  Dashboards # List custom metrics trong RetailMLOps namespace aws cloudwatch list-metrics \\ --namespace \u0026#34;RetailMLOps\u0026#34; \\ --query \u0026#39;Metrics[].MetricName\u0026#39; \\ --output table # XÃ³a custom metric filters aws logs describe-metric-filters \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training\u0026#34; \\ --query \u0026#39;metricFilters[].filterName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read filter; do echo \u0026#34;Deleting metric filter: $filter\u0026#34; aws logs delete-metric-filter \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs/retail-prediction-training\u0026#34; \\ --filter-name \u0026#34;$filter\u0026#34; done # List vÃ  xÃ³a CloudWatch Dashboards aws cloudwatch list-dashboards \\ --dashboard-name-prefix \u0026#34;RetailMLOps\u0026#34; \\ --query \u0026#39;DashboardEntries[].DashboardName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read dashboard; do echo \u0026#34;Deleting dashboard: $dashboard\u0026#34; aws cloudwatch delete-dashboards --dashboard-names \u0026#34;$dashboard\u0026#34; done 5.5 Verification Clean Up # Verify alarms Ä‘Ã£ bá»‹ xÃ³a aws cloudwatch describe-alarms \\ --alarm-name-prefix \u0026#34;EKS-\u0026#34; \\ --query \u0026#39;MetricAlarms[].AlarmName\u0026#39; # Verify log groups Ä‘Ã£ bá»‹ xÃ³a aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/containerinsights/mlops-retail-cluster\u0026#34; \\ --query \u0026#39;logGroups[].logGroupName\u0026#39; # Verify Container Insights addon Ä‘Ã£ bá»‹ remove aws eks describe-addon \\ --cluster-name mlops-retail-cluster \\ --addon-name amazon-cloudwatch-observability || echo \u0026#34;Addon removed successfully\u0026#34; # Check namespaces trong EKS kubectl get namespaces | grep cloudwatch 6. Báº£ng giÃ¡ CloudWatch Monitoring (ap-southeast-1) 6.1. CloudWatch Logs Pricing Service Ingestion Storage Analysis Log Ingestion $0.67/GB - - Log Storage - $0.033/GB/month - Log Insights - - $0.0067/GB scanned 6.2. Container Insights Pricing Component Volume Monthly Cost Description Performance Logs ~2GB/month $1.34 Node vÃ  Pod metrics Application Logs ~1GB/month $0.67 Container stdout/stderr Storage (30 days) 3GB total $0.099 Log retention Insights Queries ~0.5GB scanned $0.0034 Monthly analysis Total Container Insights $2.11/month Per cluster 6.3. CloudWatch Alarms vÃ  Metrics Feature Quantity Unit Cost Monthly Cost Standard Metrics Free $0 $0 Custom Metrics 10 metrics $0.30/metric $3.00 Alarms 5 alarms $0.10/alarm $0.50 API Calls 1M calls $0.01/1000 calls $10.00 Total Monitoring $13.50/month 6.4. SageMaker Training Logs # Estimate SageMaker training log volume # Typical training job generates ~100MB logs # With 4 training jobs per month Training Scenario Log Volume Ingestion Cost Storage Cost Total Cost Single Training 100MB $0.067 $0.0033 $0.070 4 Jobs/month 400MB $0.268 $0.013 $0.281 Daily Training 3GB/month $2.01 $0.099 $2.11 6.5. CloudWatch Dashboards Dashboard Type Widgets Monthly Cost Use Case Basic Dashboard 3 widgets $3.00 EKS cluster overview Detailed Dashboard 10 widgets $3.00 Full MLOps monitoring Custom Dashboard 20 widgets $3.00 Multi-service view Note: CloudWatch Dashboard pricing is $3.00/month per dashboard, regardless of widget count\n6.6. Log Insights Query Costs # Example query costs for different scenarios Query Type Data Scanned Cost per Query Monthly Queries Monthly Cost Error Analysis 100MB $0.00067 50 $0.034 Performance Review 500MB $0.0034 20 $0.068 Full Log Search 2GB $0.013 10 $0.13 Total Query Cost $0.23/month 6.7. Data Transfer Costs Transfer Type Volume Cost Monthly Estimate CloudWatch API 1GB/month $0.12/GB $0.12 Log Streaming 500MB/month $0.12/GB $0.06 Cross-AZ Logs 200MB/month $0.01/GB $0.002 Total Transfer $0.18/month 6.8. Retention Cost Analysis Retention Period Storage Multiplier Cost Impact Use Case 1 day 1x Baseline Development 7 days 7x 7x storage cost Testing 30 days 30x 30x storage cost Production 1 year 365x 365x storage cost Compliance # Example: 1GB/day logs with different retention # 1 day: $0.033 storage cost # 30 days: $0.99 storage cost # 1 year: $12.05 storage cost 6.9. Cost Optimization Strategies Log Filtering:\n# Chá»‰ log ERROR vÃ  WARNING levels aws logs put-metric-filter \\ --log-group-name \u0026#34;/aws/containerinsights/mlops-retail-cluster/application\u0026#34; \\ --filter-name \u0026#34;ErrorsOnly\u0026#34; \\ --filter-pattern \u0026#34;ERROR WARN\u0026#34; \\ --metric-transformations metricName=ErrorCount,metricNamespace=RetailMLOps,metricValue=1 Intelligent Log Routing:\n# Kubernetes fluent-bit configuration apiVersion: v1 kind: ConfigMap metadata: name: fluent-bit-config data: output.conf: | [OUTPUT] Name cloudwatch_logs Match kube.var.log.containers.*error* log_group_name /aws/containerinsights/cluster/errors [OUTPUT] Name cloudwatch_logs Match kube.var.log.containers.*info* log_group_name /aws/containerinsights/cluster/info retention_in_days 7 Metric Sampling:\n# Sample metrics every 5 minutes instead of 1 minute aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;EKS-HighCPU-Sampled\u0026#34; \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 80 6.10. Tá»•ng káº¿t chi phÃ­ Task 10 Scenario 1: Basic Monitoring (Development)\nComponent Monthly Cost Container Insights (basic) $2.11 3 CloudWatch Alarms $0.30 1 Dashboard $3.00 Log storage (7 days) $0.23 Total Development $5.64/month Scenario 2: Production Monitoring\nComponent Monthly Cost Container Insights (full) $6.00 10 CloudWatch Alarms $1.00 2 Dashboards $6.00 SageMaker training logs $0.28 Log storage (30 days) $0.99 Custom metrics $3.00 Log Insights queries $0.23 Total Production $17.50/month Scenario 3: Enterprise Monitoring\nComponent Monthly Cost Container Insights (multi-cluster) $12.00 25 CloudWatch Alarms $2.50 5 Dashboards $15.00 Daily SageMaker training $2.11 Extended log retention $5.00 Heavy custom metrics $10.00 Frequent queries $2.00 Total Enterprise $48.61/month 6.11. Monitoring Cost Commands # Track CloudWatch costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon CloudWatch\u0026#34;]}}\u0026#39; # Monitor log ingestion volume aws logs describe-log-groups \\ --query \u0026#39;logGroups[?storedBytes \u0026gt; `1000000`].[logGroupName,storedBytes]\u0026#39; \\ --output table # Check metric usage aws cloudwatch list-metrics \\ --namespace \u0026#34;RetailMLOps\u0026#34; \\ --query \u0026#39;length(Metrics)\u0026#39; ğŸ’° Cost Summary cho Task 10:\nDevelopment: $5.64/month (basic monitoring) Production: $17.50/month (full monitoring) Enterprise: $48.61/month (extensive monitoring) Optimization potential: 30-50% cost reduction vá»›i log filtering vÃ  retention tuning ğŸ¬ Video thá»±c hiá»‡n Task 10 Next Step: Task 11: CI/CD Pipeline\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/3-blogstranslated/3.11-blog11/",
	"title": "CÃ´ng bá»‘ Amazon EKS Capabilities cho Ä‘iá»u phá»‘i workload vÃ  quáº£n lÃ½ tÃ i nguyÃªn Ä‘Ã¡m mÃ¢y",
	"tags": [],
	"description": "",
	"content": "TÃ¡c giáº£: Channy Yun (ìœ¤ì„ì°¬) â€” 30 NOV 2025\nHÃ´m nay, chÃºng tÃ´i cÃ´ng bá»‘ Amazon Elastic Kubernetes Service (Amazon EKS) Capabilities, má»™t táº­p há»£p cÃ³ thá»ƒ má»Ÿ rá»™ng cÃ¡c giáº£i phÃ¡p gá»‘c Kubernetes (Kubernetes-native) giÃºp Ä‘Æ¡n giáº£n hÃ³a viá»‡c Ä‘iá»u phá»‘i workload, quáº£n lÃ½ tÃ i nguyÃªn Ä‘Ã¡m mÃ¢y Amazon Web Services (AWS), cÅ©ng nhÆ° viá»‡c káº¿t há»£p (composition) vÃ  Ä‘iá»u phá»‘i tÃ i nguyÃªn Kubernetes. CÃ¡c nÄƒng lá»±c ná»n táº£ng Ä‘Æ°á»£c quáº£n lÃ½ toÃ n pháº§n (fully managed) vÃ  tÃ­ch há»£p nÃ y bao gá»“m cÃ¡c giáº£i phÃ¡p Kubernetes mÃ£ nguá»“n má»Ÿ mÃ  nhiá»u khÃ¡ch hÃ ng Ä‘ang sá»­ dá»¥ng hiá»‡n nay, cháº³ng háº¡n nhÆ° Argo CD, AWS Controllers for Kubernetes vÃ  Kube Resource Orchestrator.\nVá»›i EKS Capabilities, báº¡n cÃ³ thá»ƒ xÃ¢y dá»±ng vÃ  má»Ÿ rá»™ng cÃ¡c á»©ng dá»¥ng Kubernetes mÃ  khÃ´ng cáº§n quáº£n lÃ½ cÆ¡ sá»Ÿ háº¡ táº§ng giáº£i phÃ¡p phá»©c táº¡p. KhÃ¡c vá»›i cÃ¡c cÃ i Ä‘áº·t thÃ´ng thÆ°á»ng trong cá»¥m (in-cluster installations), cÃ¡c capabilities nÃ y thá»±c táº¿ cháº¡y trong cÃ¡c tÃ i khoáº£n do dá»‹ch vá»¥ EKS sá»Ÿ há»¯u vÃ  Ä‘Æ°á»£c trá»«u tÆ°á»£ng hÃ³a hoÃ n toÃ n khá»i khÃ¡ch hÃ ng.\nVÃ¬ AWS quáº£n lÃ½ viá»‡c má»Ÿ rá»™ng quy mÃ´ háº¡ táº§ng, vÃ¡ lá»—i vÃ  cáº­p nháº­t cÃ¡c nÄƒng lá»±c cá»§a cá»¥m nÃ y, báº¡n cÃ³ thá»ƒ táº­n dá»¥ng Ä‘á»™ tin cáº­y vÃ  báº£o máº­t cáº¥p doanh nghiá»‡p mÃ  khÃ´ng cáº§n pháº£i duy trÃ¬ vÃ  quáº£n lÃ½ cÃ¡c thÃ nh pháº§n ná»n táº£ng bÃªn dÆ°á»›i.\nDÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c capabilities cÃ³ sáºµn táº¡i thá»i Ä‘iá»ƒm ra máº¯t:\nArgo CD â€“ ÄÃ¢y lÃ  má»™t cÃ´ng cá»¥ GitOps dáº¡ng khai bÃ¡o (declarative) cho Kubernetes, cung cáº¥p cÃ¡c kháº£ nÄƒng triá»ƒn khai liÃªn tá»¥c (continuous deployment, CD) cho Kubernetes. CÃ´ng cá»¥ nÃ y Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i, vá»›i hÆ¡n 45% ngÆ°á»i dÃ¹ng cuá»‘i Kubernetes bÃ¡o cÃ¡o Ä‘ang dÃ¹ng trong mÃ´i trÆ°á»ng production hoáº·c dá»± Ä‘á»‹nh dÃ¹ng production trong Kháº£o sÃ¡t Cloud Native Computing Foundation (CNCF) nÄƒm 2024. AWS Controllers for Kubernetes (ACK) â€“ ACK ráº¥t phá»• biáº¿n vá»›i cÃ¡c nhÃ³m ná»n táº£ng doanh nghiá»‡p trong mÃ´i trÆ°á»ng production. ACK cung cáº¥p cÃ¡c tÃ i nguyÃªn tÃ¹y chá»‰nh (custom resources) cho Kubernetes, cho phÃ©p quáº£n lÃ½ cÃ¡c tÃ i nguyÃªn AWS Cloud trá»±c tiáº¿p tá»« bÃªn trong cÃ¡c cá»¥m cá»§a báº¡n. Kube Resource Orchestrator (KRO) â€“ KRO cung cáº¥p má»™t cÃ¡ch thá»©c tinh gá»n Ä‘á»ƒ táº¡o vÃ  quáº£n lÃ½ cÃ¡c tÃ i nguyÃªn tÃ¹y chá»‰nh trong Kubernetes. Vá»›i KRO, cÃ¡c nhÃ³m ná»n táº£ng cÃ³ thá»ƒ táº¡o cÃ¡c gÃ³i tÃ i nguyÃªn cÃ³ thá»ƒ tÃ¡i sá»­ dá»¥ng (reusable resource bundles) nháº±m trá»«u tÆ°á»£ng hÃ³a sá»± phá»©c táº¡p trong khi váº«n giá»¯ tÃ­nh native vá»›i há»‡ sinh thÃ¡i Kubernetes. Vá»›i cÃ¡c tÃ­nh nÄƒng nÃ y, báº¡n cÃ³ thá»ƒ tÄƒng tá»‘c vÃ  má»Ÿ rá»™ng viá»‡c sá»­ dá»¥ng Kubernetes cá»§a mÃ¬nh vá»›i cÃ¡c capabilities Ä‘Æ°á»£c quáº£n lÃ½ toÃ n pháº§n, sá»­ dá»¥ng cÃ¡c tÃ­nh nÄƒng mang tÃ­nh Ä‘á»‹nh hÆ°á»›ng (opinionated) nhÆ°ng linh hoáº¡t Ä‘á»ƒ xÃ¢y dá»±ng cho kháº£ nÄƒng má»Ÿ rá»™ng ngay tá»« Ä‘áº§u. Bá»™ nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cung cáº¥p má»™t táº­p cÃ¡c nÄƒng lá»±c ná»n táº£ng cá»§a cá»¥m cÃ³ thá»ƒ xáº¿p lá»›p (layer) vá»›i nhau má»™t cÃ¡ch liá»n máº¡ch, mang láº¡i cÃ¡c tÃ­nh nÄƒng tÃ­ch há»£p cho triá»ƒn khai liÃªn tá»¥c, Ä‘iá»u phá»‘i tÃ i nguyÃªn vÃ  káº¿t há»£p. Báº¡n cÃ³ thá»ƒ táº­p trung vÃ o viá»‡c quáº£n lÃ½ vÃ  phÃ¡t hÃ nh pháº§n má»m mÃ  khÃ´ng cáº§n pháº£i dÃ nh thá»i gian vÃ  tÃ i nguyÃªn Ä‘á»ƒ xÃ¢y dá»±ng vÃ  quáº£n lÃ½ cÃ¡c thÃ nh pháº§n ná»n táº£ng cá»§a ná»n táº£ng (platform) nÃ y.\nCÃ¡ch thá»©c hoáº¡t Ä‘á»™ng CÃ¡c ká»¹ sÆ° ná»n táº£ng (platform engineers) vÃ  quáº£n trá»‹ viÃªn cá»¥m (cluster administrators) cÃ³ thá»ƒ thiáº¿t láº­p EKS Capabilities Ä‘á»ƒ giáº£m táº£i viá»‡c xÃ¢y dá»±ng vÃ  quáº£n lÃ½ cÃ¡c giáº£i phÃ¡p tÃ¹y chá»‰nh nháº±m cung cáº¥p cÃ¡c dá»‹ch vá»¥ ná»n táº£ng phá»• biáº¿n, nghÄ©a lÃ  há» cÃ³ thá»ƒ táº­p trung vÃ o cÃ¡c tÃ­nh nÄƒng khÃ¡c biá»‡t hÆ¡n quan trá»ng Ä‘á»‘i vá»›i doanh nghiá»‡p cá»§a báº¡n.\nCÃ¡c nhÃ  phÃ¡t triá»ƒn á»©ng dá»¥ng cá»§a báº¡n chá»§ yáº¿u lÃ m viá»‡c vá»›i EKS Capabilities giá»‘ng nhÆ° vá»›i cÃ¡c tÃ­nh nÄƒng Kubernetes khÃ¡c. Há» lÃ m Ä‘iá»u nÃ y báº±ng cÃ¡ch Ã¡p dá»¥ng cáº¥u hÃ¬nh dáº¡ng khai bÃ¡o (declarative configuration) Ä‘á»ƒ táº¡o cÃ¡c tÃ i nguyÃªn Kubernetes báº±ng cÃ¡c cÃ´ng cá»¥ quen thuá»™c, cháº³ng háº¡n nhÆ° kubectl hoáº·c thÃ´ng qua tá»± Ä‘á»™ng hÃ³a tá»« git commit Ä‘áº¿n mÃ£ cháº¡y. Báº¯t Ä‘áº§u vá»›i EKS Capabilities Äá»ƒ báº­t EKS Capabilities, báº¡n cÃ³ thá»ƒ dÃ¹ng EKS console, AWS Command Line Interface (AWS CLI), eksctl hoáº·c cÃ¡c cÃ´ng cá»¥ Æ°a thÃ­ch khÃ¡c. Trong EKS console, chá»n Create capabilities trong tab Capabilities trÃªn cá»¥m EKS hiá»‡n cÃ³ cá»§a báº¡n. EKS Capabilities lÃ  cÃ¡c tÃ i nguyÃªn AWS, vÃ  chÃºng cÃ³ thá»ƒ Ä‘Æ°á»£c gáº¯n tháº» (tagged), quáº£n lÃ½ vÃ  xÃ³a. Báº¡n cÃ³ thá»ƒ chá»n má»™t hoáº·c nhiá»u capabilities Ä‘á»ƒ phá»‘i há»£p cÃ¹ng nhau. TÃ´i Ä‘Ã£ chá»n cáº£ ba capabilities: ArgoCD, ACK vÃ  KRO. Tuy nhiÃªn, cÃ¡c capabilities nÃ y hoÃ n toÃ n Ä‘á»™c láº­p vÃ  báº¡n cÃ³ thá»ƒ chá»n báº­t capabilities nÃ o báº¡n muá»‘n trÃªn cÃ¡c cá»¥m cá»§a mÃ¬nh. BÃ¢y giá» báº¡n cÃ³ thá»ƒ cáº¥u hÃ¬nh cÃ¡c capabilities Ä‘Ã£ chá»n. Báº¡n nÃªn táº¡o cÃ¡c vai trÃ² AWS Identity and Access Management (AWS IAM) Ä‘á»ƒ cho phÃ©p EKS váº­n hÃ nh cÃ¡c capabilities nÃ y trong cá»¥m cá»§a báº¡n. Xin lÆ°u Ã½ ráº±ng báº¡n khÃ´ng thá»ƒ sá»­a Ä‘á»•i tÃªn capability, namespace, vÃ¹ng xÃ¡c thá»±c (authentication region) hoáº·c phiÃªn báº£n AWS IAM Identity Center sau khi táº¡o capability. Chá»n Next, xem láº¡i cÃ¡c thiáº¿t láº­p vÃ  báº­t capabilities. Giá» Ä‘Ã¢y báº¡n cÃ³ thá»ƒ xem vÃ  quáº£n lÃ½ cÃ¡c capabilities Ä‘Ã£ táº¡o. Chá»n ArgoCD Ä‘á»ƒ cáº­p nháº­t cáº¥u hÃ¬nh cá»§a capability. Báº¡n cÃ³ thá»ƒ xem chi tiáº¿t capability ArgoCD. Chá»n Edit Ä‘á»ƒ thay Ä‘á»•i cÃ¡c thiáº¿t láº­p cáº¥u hÃ¬nh hoáº·c Monitor ArgoCD Ä‘á»ƒ hiá»ƒn thá»‹ tráº¡ng thÃ¡i sá»©c khá»e cá»§a capability cho cá»¥m EKS hiá»‡n táº¡i. Chá»n Go to Argo UI Ä‘á»ƒ trá»±c quan hÃ³a vÃ  theo dÃµi tráº¡ng thÃ¡i triá»ƒn khai vÃ  tÃ¬nh tráº¡ng á»©ng dá»¥ng. Äá»ƒ tÃ¬m hiá»ƒu thÃªm vá» cÃ¡ch thiáº¿t láº­p vÃ  sá»­ dá»¥ng chi tiáº¿t tá»«ng capability, hÃ£y truy cáº­p Getting started with EKS Capabilities trong Amazon EKS User Guide.\nNhá»¯ng Ä‘iá»u cáº§n biáº¿t DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c Ä‘iá»ƒm cáº§n lÆ°u Ã½ chÃ­nh vá» tÃ­nh nÄƒng nÃ y:\nQuyá»n (Permissions) â€“ EKS Capabilities lÃ  cÃ¡c tÃ i nguyÃªn quáº£n trá»‹ pháº¡m vi cá»¥m (cluster-scoped administrator resources), vÃ  quyá»n cá»§a tÃ i nguyÃªn Ä‘Æ°á»£c cáº¥u hÃ¬nh thÃ´ng qua AWS IAM. Vá»›i má»™t sá»‘ capabilities, cÃ³ thÃªm cáº¥u hÃ¬nh cho single sign-on. VÃ­ dá»¥, cáº¥u hÃ¬nh single sign-on cá»§a Argo CD Ä‘Æ°á»£c báº­t trá»±c tiáº¿p trong EKS vá»›i tÃ­ch há»£p trá»±c tiáº¿p vá»›i IAM Identity Center. NÃ¢ng cáº¥p (Upgrades) â€“ EKS tá»± Ä‘á»™ng cáº­p nháº­t cÃ¡c cluster capabilities báº¡n báº­t vÃ  cÃ¡c phá»¥ thuá»™c liÃªn quan cá»§a chÃºng. NÃ³ tá»± Ä‘á»™ng phÃ¢n tÃ­ch cÃ¡c thay Ä‘á»•i cÃ³ thá»ƒ gÃ¢y lá»—i tÆ°Æ¡ng thÃ­ch (breaking changes), vÃ¡ vÃ  cáº­p nháº­t cÃ¡c thÃ nh pháº§n khi cáº§n, vÃ  thÃ´ng bÃ¡o cho báº¡n vá» xung Ä‘á»™t hoáº·c váº¥n Ä‘á» thÃ´ng qua EKS cluster insights. Tiáº¿p nháº­n (Adoptions) â€“ ACK cung cáº¥p cÃ¡c tÃ­nh nÄƒng tiáº¿p nháº­n tÃ i nguyÃªn (resource adoption) cho phÃ©p di chuyá»ƒn cÃ¡c tÃ i nguyÃªn AWS hiá»‡n cÃ³ vÃ o pháº¡m vi quáº£n lÃ½ cá»§a ACK. ACK cÅ©ng cung cáº¥p cÃ¡c tÃ i nguyÃªn chá»‰ Ä‘á»c (read-only resources) cÃ³ thá»ƒ giÃºp há»— trá»£ má»™t lá»™ trÃ¬nh di chuyá»ƒn theo tá»«ng bÆ°á»›c tá»« cÃ¡c tÃ i nguyÃªn Ä‘Æ°á»£c cáº¥p phÃ¡t báº±ng Terraform, AWS CloudFormation sang EKS Capabilities. Hiá»‡n Ä‘Ã£ cÃ³ sáºµn Amazon EKS Capabilities hiá»‡n Ä‘Ã£ cÃ³ sáºµn táº¡i cÃ¡c AWS Regions thÆ°Æ¡ng máº¡i. Äá»ƒ biáº¿t kháº£ dá»¥ng theo vÃ¹ng vÃ  lá»™ trÃ¬nh tÆ°Æ¡ng lai, hÃ£y truy cáº­p AWS Capabilities by Region. KhÃ´ng cÃ³ cam káº¿t tráº£ trÆ°á»›c hoáº·c phÃ­ tá»‘i thiá»ƒu, vÃ  báº¡n chá»‰ tráº£ tiá»n cho EKS Capabilities vÃ  cÃ¡c tÃ i nguyÃªn mÃ  báº¡n sá»­ dá»¥ng. Äá»ƒ tÃ¬m hiá»ƒu thÃªm, hÃ£y truy cáº­p trang EKS pricing.\nHÃ£y thá»­ trong Amazon EKS console vÃ  gá»­i pháº£n há»“i Ä‘áº¿n AWS re:Post cho EKS hoáº·c thÃ´ng qua cÃ¡c kÃªnh AWS Support thÃ´ng thÆ°á»ng cá»§a báº¡n.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.10-week10/",
	"title": "Tuáº§n 10 - DevOps &amp; IaC",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 10: Triá»ƒn khai Infrastructure as Code (Terraform/CloudFormation). Thiáº¿t láº­p CI/CD pipelines (CodePipeline/GitHub Actions). CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - IaC: Viáº¿t Terraform cho cÃ¡c tÃ i nguyÃªn VPC, SG vÃ  IAM.\n- Quáº£n lÃ½ state báº±ng S3 Backend. Terraform Docs 3 - Thiáº¿t láº­p CI/CD: Cáº¥u hÃ¬nh CodeCommit/GitHub repo.\n- Thiáº¿t láº­p CodeBuild projects Ä‘á»ƒ build Docker images. AWS CodeBuild 4 - Pipeline: Táº¡o CodePipeline Ä‘á»ƒ tá»± Ä‘á»™ng hoÃ¡ Build -\u0026gt; Deploy.\n- ThÃªm cÃ¡c bÆ°á»›c Manual Approval. AWS CodePipeline 5 - Báº£o máº­t: TÃ­ch há»£p quáº£n lÃ½ secrets (SSM/Secrets Manager) vÃ o pipeline.\n- XÃ¡c minh viá»‡c deploy tá»± Ä‘á»™ng. FinOps Guide 6 - Ã”n táº­p: Test toÃ n bá»™ luá»“ng CI/CD tá»« commit Ä‘áº¿n deploy.\n- BÃ¡o cÃ¡o tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 10: Tá»± Ä‘á»™ng hoÃ¡ provisioning háº¡ táº§ng báº±ng Terraform. XÃ¢y dá»±ng CI/CD pipeline tá»± Ä‘á»™ng cho viá»‡c triá»ƒn khai container. Loáº¡i bá» hardcoded secrets trong cÃ¡c script triá»ƒn khai. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/11-cicd-jenkins-travis/",
	"title": "CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Má»¥c tiÃªu Task 13:\nThiáº¿t láº­p pipeline CI/CD tá»± Ä‘á»™ng cho toÃ n bá»™ vÃ²ng Ä‘á»i dá»± Ã¡n MLOps Retail Prediction:\nCI (Continuous Integration): build \u0026amp; test code, build Docker image, push ECR CD (Continuous Delivery): huáº¥n luyá»‡n láº¡i model, cáº­p nháº­t Model Registry, deploy phiÃªn báº£n má»›i lÃªn EKS Monitoring hook: rollback khi API hoáº·c model cÃ³ lá»—i (CloudWatch trigger) â†’ Äáº£m báº£o triá»ƒn khai liÃªn tá»¥c, giáº£m lá»—i thá»§ cÃ´ng, tiáº¿t kiá»‡m thá»i gian vÃ  chi phÃ­.\nğŸ“¥ Input tá»« cÃ¡c Task trÆ°á»›c:\nTask 6 (ECR Container Registry): Repository URIs, lifecycle policies, image scanning and push commands; credentials and ECR access for CI runners Task 8 (API Deployment on EKS): Kubernetes manifests, service \u0026amp; deployment names, healthcheck endpoints, HPA and ServiceAccount/IRSA details used by CD stage Task 10 (CloudWatch Monitoring): Log groups, alarms, dashboards and Container Insights configuration used for deployment verification and automated rollback triggers Task 2 (IAM Roles \u0026amp; Audit): IAM roles, OIDC provider configuration and least-privilege policies for CI/CD runners (GitHub Actions / Jenkins) and SageMaker execution 1. Cáº¥u trÃºc pipeline tá»•ng quÃ¡t Pipeline CI/CD cá»§a dá»± Ã¡n Retail Prediction sáº½ tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ quy trÃ¬nh tá»« commit code Ä‘áº¿n deploy lÃªn production, bao gá»“m cáº£ viá»‡c huáº¥n luyá»‡n láº¡i model khi cáº§n thiáº¿t.\nPipeline chia thÃ nh 3 mÃ´i trÆ°á»ng:\nDEV: Build, test vÃ  validate code STAGING: Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ model PROD: Deploy vÃ  monitor trÃªn production 2. IAM Roles vÃ  Permissions 2.1 Create CI/CD Service Role # Create trust policy for CI/CD role cat \u0026gt; cicd-trust-policy.json \u0026lt;\u0026lt; EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: [ \u0026#34;ec2.amazonaws.com\u0026#34;, \u0026#34;codebuild.amazonaws.com\u0026#34;, \u0026#34;codepipeline.amazonaws.com\u0026#34; ] }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::YOUR_ACCOUNT_ID:user/jenkins-user\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } EOF # Create the role aws iam create-role \\ --role-name RetailForecastCICDRole \\ --assume-role-policy-document file://cicd-trust-policy.json \\ --description \u0026#34;Role for Retail Forecast CI/CD Pipeline\u0026#34; 2.2 CI/CD IAM Policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::retail-forecast-data-bucket\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-data-bucket/*\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-artifacts-bucket\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-artifacts-bucket/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetAuthorizationToken\u0026#34;, \u0026#34;ecr:BatchCheckLayerAvailability\u0026#34;, \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34;, \u0026#34;ecr:PutImage\u0026#34;, \u0026#34;ecr:InitiateLayerUpload\u0026#34;, \u0026#34;ecr:UploadLayerPart\u0026#34;, \u0026#34;ecr:CompleteLayerUpload\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sagemaker:CreateTrainingJob\u0026#34;, \u0026#34;sagemaker:DescribeTrainingJob\u0026#34;, \u0026#34;sagemaker:StopTrainingJob\u0026#34;, \u0026#34;sagemaker:CreateModel\u0026#34;, \u0026#34;sagemaker:CreateModelPackage\u0026#34;, \u0026#34;sagemaker:CreateModelPackageGroup\u0026#34;, \u0026#34;sagemaker:DescribeModelPackage\u0026#34;, \u0026#34;sagemaker:UpdateModelPackage\u0026#34;, \u0026#34;sagemaker:ListModelPackages\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;eks:DescribeCluster\u0026#34;, \u0026#34;eks:ListClusters\u0026#34;, \u0026#34;eks:DescribeNodegroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sts:GetCallerIdentity\u0026#34;, \u0026#34;sts:AssumeRole\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:GenerateDataKey\u0026#34;, \u0026#34;kms:CreateGrant\u0026#34;], \u0026#34;Resource\u0026#34;: [\u0026#34;arn:aws:kms:us-east-1:YOUR_ACCOUNT_ID:key/*\u0026#34;] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } 2.3 Attach Policies # Create and attach the policy aws iam create-policy \\ --policy-name RetailForecastCICDPolicy \\ --policy-document file://cicd-policy.json aws iam attach-role-policy \\ --role-name RetailForecastCICDRole \\ --policy-arn arn:aws:iam::YOUR_ACCOUNT_ID:policy/RetailForecastCICDPolicy # Attach additional managed policies aws iam attach-role-policy \\ --role-name RetailForecastCICDRole \\ --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy aws iam attach-role-policy \\ --role-name RetailForecastCICDRole \\ --policy-arn arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy 3. Jenkins Pipeline Setup 3.1 Jenkins Installation on EC2 # Launch EC2 instance for Jenkins aws ec2 run-instances \\ --image-id ami-0c55b159cbfafe1d0 \\ --instance-type t3.medium \\ --key-name your-key-pair \\ --security-group-ids sg-jenkins \\ --subnet-id subnet-12345678 \\ --iam-instance-profile Name=JenkinsInstanceProfile \\ --user-data file://jenkins-install.sh \\ --tag-specifications \u0026#39;ResourceType=instance,Tags=[{Key=Name,Value=Jenkins-Server},{Key=Project,Value=RetailForecast}]\u0026#39; 3.2 Jenkins Installation Script #!/bin/bash # jenkins-install.sh # Update system yum update -y # Install Docker yum install -y docker systemctl start docker systemctl enable docker usermod -a -G docker ec2-user # Install Docker Compose curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose # Install Jenkins wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key yum upgrade -y yum install -y java-11-openjdk jenkins # Start Jenkins systemctl start jenkins systemctl enable jenkins # Install kubectl curl -o kubectl https://amazon-eks.s3.us-west-2.amazonaws.com/1.21.2/2021-07-05/bin/linux/amd64/kubectl chmod +x ./kubectl mv ./kubectl /usr/local/bin # Install AWS CLI v2 curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip ./aws/install # Install Python and dependencies yum install -y python3 python3-pip pip3 install boto3 sagemaker pandas scikit-learn # Configure Jenkins user for Docker usermod -a -G docker jenkins systemctl restart jenkins echo \u0026#34;Jenkins installation completed!\u0026#34; echo \u0026#34;Access Jenkins at: http://$(curl -s http://169.254.169.254/latest/meta-data/public-ip):8080\u0026#34; echo \u0026#34;Initial admin password: $(cat /var/lib/jenkins/secrets/initialAdminPassword)\u0026#34; 3.3 Jenkinsfile for ML Pipeline // Jenkinsfile pipeline { agent any environment { AWS_DEFAULT_REGION = \u0026#39;us-east-1\u0026#39; AWS_ACCOUNT_ID = \u0026#39;123456789012\u0026#39; ECR_REPOSITORY = \u0026#39;retail-forecast\u0026#39; EKS_CLUSTER_NAME = \u0026#39;retail-forecast-cluster\u0026#39; NAMESPACE = \u0026#39;mlops\u0026#39; S3_DATA_BUCKET = \u0026#39;retail-forecast-data-bucket\u0026#39; S3_ARTIFACTS_BUCKET = \u0026#39;retail-forecast-artifacts-bucket\u0026#39; MODEL_PACKAGE_GROUP = \u0026#39;retail-forecast-model-group\u0026#39; } parameters { choice( name: \u0026#39;DEPLOY_ENVIRONMENT\u0026#39;, choices: [\u0026#39;dev\u0026#39;, \u0026#39;staging\u0026#39;, \u0026#39;prod\u0026#39;], description: \u0026#39;Target deployment environment\u0026#39; ) booleanParam( name: \u0026#39;RETRAIN_MODEL\u0026#39;, defaultValue: false, description: \u0026#39;Force model retraining\u0026#39; ) booleanParam( name: \u0026#39;SKIP_TESTS\u0026#39;, defaultValue: false, description: \u0026#39;Skip test execution\u0026#39; ) } stages { stage(\u0026#39;Setup\u0026#39;) { steps { script { // Clean workspace cleanWs() // Checkout code checkout scm // Set build info env.BUILD_VERSION = \u0026#34;${env.BUILD_NUMBER}-${env.GIT_COMMIT.take(7)}\u0026#34; env.IMAGE_TAG = \u0026#34;v${env.BUILD_VERSION}\u0026#34; echo \u0026#34;Build Version: ${env.BUILD_VERSION}\u0026#34; echo \u0026#34;Image Tag: ${env.IMAGE_TAG}\u0026#34; } } } stage(\u0026#39;Environment Setup\u0026#39;) { steps { script { // Install Python dependencies sh \u0026#39;\u0026#39;\u0026#39; python3 -m venv venv source venv/bin/activate pip install --upgrade pip pip install -r requirements.txt pip install pytest pytest-cov flake8 \u0026#39;\u0026#39;\u0026#39; // Configure AWS credentials sh \u0026#39;\u0026#39;\u0026#39; aws sts get-caller-identity aws configure set region $AWS_DEFAULT_REGION \u0026#39;\u0026#39;\u0026#39; // Configure kubectl sh \u0026#39;\u0026#39;\u0026#39; aws eks update-kubeconfig --name $EKS_CLUSTER_NAME --region $AWS_DEFAULT_REGION kubectl cluster-info \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Code Quality \u0026amp; Testing\u0026#39;) { when { not { params.SKIP_TESTS } } parallel { stage(\u0026#39;Linting\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate flake8 src/ --max-line-length=88 --exclude=venv \u0026#39;\u0026#39;\u0026#39; } } stage(\u0026#39;Unit Tests\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html \u0026#39;\u0026#39;\u0026#39; // Publish test results publishTestResults testResultsPattern: \u0026#39;test-results.xml\u0026#39; publishCoverage adapters: [coberturaAdapter(\u0026#39;coverage.xml\u0026#39;)], sourceFileResolver: sourceFiles(\u0026#39;STORE_LAST_BUILD\u0026#39;) } } stage(\u0026#39;Integration Tests\u0026#39;) { steps { sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate pytest tests/integration/ -v \u0026#39;\u0026#39;\u0026#39; } } } } stage(\u0026#39;Data Validation\u0026#39;) { steps { script { sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate python scripts/validate_data.py \\ --bucket $S3_DATA_BUCKET \\ --key training-data/train.csv \\ --output data-validation-report.json \u0026#39;\u0026#39;\u0026#39; // Archive validation report archiveArtifacts artifacts: \u0026#39;data-validation-report.json\u0026#39;, fingerprint: true } } } stage(\u0026#39;Model Training\u0026#39;) { when { anyOf { params.RETRAIN_MODEL changeset \u0026#34;src/training/**\u0026#34; changeset \u0026#34;data/**\u0026#34; } } steps { script { sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate python scripts/trigger_training.py \\ --job-name \u0026#34;retail-forecast-training-${BUILD_VERSION}\u0026#34; \\ --data-bucket $S3_DATA_BUCKET \\ --output-bucket $S3_ARTIFACTS_BUCKET \\ --role-arn arn:aws:iam::$AWS_ACCOUNT_ID:role/SageMakerExecutionRole \u0026#39;\u0026#39;\u0026#39; // Wait for training completion sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate python scripts/wait_for_training.py \\ --job-name \u0026#34;retail-forecast-training-${BUILD_VERSION}\u0026#34; \\ --timeout 3600 \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Model Validation \u0026amp; Registration\u0026#39;) { when { anyOf { params.RETRAIN_MODEL changeset \u0026#34;src/training/**\u0026#34; } } steps { script { // Validate model performance sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate python scripts/validate_model.py \\ --job-name \u0026#34;retail-forecast-training-${BUILD_VERSION}\u0026#34; \\ --baseline-accuracy 0.85 \\ --output model-validation-report.json \u0026#39;\u0026#39;\u0026#39; // Register model if validation passes sh \u0026#39;\u0026#39;\u0026#39; source venv/bin/activate python scripts/register_model.py \\ --job-name \u0026#34;retail-forecast-training-${BUILD_VERSION}\u0026#34; \\ --model-package-group $MODEL_PACKAGE_GROUP \\ --approval-status \u0026#34;PendingManualApproval\u0026#34; \\ --model-version $BUILD_VERSION \u0026#39;\u0026#39;\u0026#39; archiveArtifacts artifacts: \u0026#39;model-validation-report.json\u0026#39;, fingerprint: true } } } stage(\u0026#39;Docker Build\u0026#39;) { steps { script { // Build Docker image sh \u0026#39;\u0026#39;\u0026#39; # Get latest approved model MODEL_URI=$(python scripts/get_latest_model.py --model-package-group $MODEL_PACKAGE_GROUP) echo \u0026#34;Using model: $MODEL_URI\u0026#34; # Build image with model URI docker build \\ --build-arg MODEL_URI=$MODEL_URI \\ --build-arg BUILD_VERSION=$BUILD_VERSION \\ -t $ECR_REPOSITORY:$IMAGE_TAG \\ -t $ECR_REPOSITORY:latest \\ . \u0026#39;\u0026#39;\u0026#39; // Security scan sh \u0026#39;\u0026#39;\u0026#39; docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ -v $(pwd):/root/.cache/ \\ aquasec/trivy:latest image \\ --exit-code 1 \\ --severity HIGH,CRITICAL \\ $ECR_REPOSITORY:$IMAGE_TAG \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Push to ECR\u0026#39;) { steps { script { sh \u0026#39;\u0026#39;\u0026#39; # Login to ECR aws ecr get-login-password --region $AWS_DEFAULT_REGION | \\ docker login --username AWS --password-stdin \\ $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com # Tag images docker tag $ECR_REPOSITORY:$IMAGE_TAG \\ $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG docker tag $ECR_REPOSITORY:latest \\ $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:latest # Push images docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:latest \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Deploy to EKS\u0026#39;) { steps { script { // Update Kubernetes deployment sh \u0026#39;\u0026#39;\u0026#39; # Update deployment image kubectl set image deployment/retail-forecast-api \\ retail-forecast-api=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$ECR_REPOSITORY:$IMAGE_TAG \\ -n $NAMESPACE # Wait for rollout kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE --timeout=600s # Verify deployment kubectl get pods -n $NAMESPACE -l app=retail-forecast-api \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Health Check\u0026#39;) { steps { script { sh \u0026#39;\u0026#39;\u0026#39; # Get service endpoint ENDPOINT=$(kubectl get service retail-forecast-service -n $NAMESPACE -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) if [ -z \u0026#34;$ENDPOINT\u0026#34; ]; then ENDPOINT=$(kubectl get service retail-forecast-service -n $NAMESPACE -o jsonpath=\u0026#39;{.spec.clusterIP}\u0026#39;) kubectl port-forward service/retail-forecast-service 8080:80 -n $NAMESPACE \u0026amp; ENDPOINT=\u0026#34;localhost:8080\u0026#34; PORT_FORWARD_PID=$! fi # Wait for service to be ready echo \u0026#34;Waiting for service to be ready...\u0026#34; for i in {1..30}; do if curl -f http://$ENDPOINT/healthz; then echo \u0026#34;Service is healthy!\u0026#34; break fi echo \u0026#34;Attempt $i/30 failed, retrying in 10 seconds...\u0026#34; sleep 10 done # Test prediction endpoint curl -X POST http://$ENDPOINT/predict \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;features\u0026#34;: {\u0026#34;store_id\u0026#34;: 1, \u0026#34;product_id\u0026#34;: 123, \u0026#34;price\u0026#34;: 29.99}}\u0026#39; # Kill port-forward if used if [ ! -z \u0026#34;$PORT_FORWARD_PID\u0026#34; ]; then kill $PORT_FORWARD_PID fi \u0026#39;\u0026#39;\u0026#39; } } } stage(\u0026#39;Performance Testing\u0026#39;) { when { environment name: \u0026#39;DEPLOY_ENVIRONMENT\u0026#39;, value: \u0026#39;prod\u0026#39; } steps { script { sh \u0026#39;\u0026#39;\u0026#39; # Run load test python scripts/load_test.py \\ --endpoint http://$ENDPOINT \\ --duration 300 \\ --concurrent-users 10 \\ --output load-test-report.json \u0026#39;\u0026#39;\u0026#39; archiveArtifacts artifacts: \u0026#39;load-test-report.json\u0026#39;, fingerprint: true } } } } post { always { // Clean up sh \u0026#39;\u0026#39;\u0026#39; docker system prune -f rm -rf venv \u0026#39;\u0026#39;\u0026#39; // Archive logs archiveArtifacts artifacts: \u0026#39;logs/**\u0026#39;, allowEmptyArchive: true } success { script { // Send success notification sh \u0026#39;\u0026#39;\u0026#39; aws sns publish \\ --topic-arn arn:aws:sns:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:deployment-notifications \\ --message \u0026#34;âœ… Deployment successful for build $BUILD_VERSION\u0026#34; \\ --subject \u0026#34;Retail Forecast Deployment Success\u0026#34; \u0026#39;\u0026#39;\u0026#39; } } failure { script { // Send failure notification sh \u0026#39;\u0026#39;\u0026#39; aws sns publish \\ --topic-arn arn:aws:sns:$AWS_DEFAULT_REGION:$AWS_ACCOUNT_ID:deployment-notifications \\ --message \u0026#34;âŒ Deployment failed for build $BUILD_VERSION. Check Jenkins logs.\u0026#34; \\ --subject \u0026#34;Retail Forecast Deployment Failed\u0026#34; \u0026#39;\u0026#39;\u0026#39; // Rollback on production failure if (params.DEPLOY_ENVIRONMENT == \u0026#39;prod\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; echo \u0026#34;Rolling back production deployment...\u0026#34; kubectl rollout undo deployment/retail-forecast-api -n $NAMESPACE kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE \u0026#39;\u0026#39;\u0026#39; } } } } } 2. GitHub Actions CI/CD Pipeline ChÃºng ta sáº½ sá»­ dá»¥ng GitHub Actions Ä‘á»ƒ xÃ¢y dá»±ng pipeline CI/CD cho dá»± Ã¡n MLOps Retail Prediction vÃ¬ kháº£ nÄƒng tÃ­ch há»£p sáºµn vá»›i GitHub repository vÃ  tÃ­nh linh hoáº¡t cao.\n2.1 Cáº¥u hÃ¬nh workflow file Táº¡o file .github/workflows/mlops-pipeline.yml:\n# .github/workflows/mlops-pipeline.yml name: MLOps Retail Prediction Pipeline on: push: branches: [main, develop] paths-ignore: - \u0026#34;**.md\u0026#34; - \u0026#34;docs/**\u0026#34; pull_request: branches: [main] schedule: # Cháº¡y má»—i tuáº§n vÃ o thá»© 2 Ä‘á»ƒ kiá»ƒm tra Ä‘á»™ chÃ­nh xÃ¡c cá»§a model - cron: \u0026#34;0 2 * * 1\u0026#34; workflow_dispatch: inputs: environment: description: \u0026#34;MÃ´i trÆ°á»ng triá»ƒn khai\u0026#34; required: true default: \u0026#34;dev\u0026#34; type: choice options: - dev - staging - prod retrain_model: description: \u0026#34;Huáº¥n luyá»‡n láº¡i model\u0026#34; required: false default: false type: boolean deploy_only: description: \u0026#34;Chá»‰ triá»ƒn khai, khÃ´ng build/huáº¥n luyá»‡n má»›i\u0026#34; required: false default: false type: boolean env: AWS_REGION: us-east-1 ECR_REPOSITORY: retail-forecast EKS_CLUSTER_NAME: retail-forecast-cluster S3_DATA_BUCKET: retail-forecast-data S3_MODEL_BUCKET: retail-forecast-models MODEL_PACKAGE_GROUP: retail-forecast-models permissions: id-token: write # Cáº§n thiáº¿t cho OIDC vá»›i AWS contents: read # Cáº§n thiáº¿t Ä‘á»ƒ checkout code jobs: setup: name: Setup Pipeline runs-on: ubuntu-latest outputs: build-id: ${{ steps.generate-id.outputs.build_id }} environment: ${{ steps.set-env.outputs.environment }} should-train: ${{ steps.set-env.outputs.should_train }} should-deploy: ${{ steps.set-env.outputs.should_deploy }} steps: - name: Generate build ID id: generate-id run: | BUILD_ID=\u0026#34;build-${GITHUB_RUN_NUMBER}-${GITHUB_SHA::7}\u0026#34; echo \u0026#34;build_id=$BUILD_ID\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;Build ID: $BUILD_ID\u0026#34; - name: Set environment variables id: set-env run: | # XÃ¡c Ä‘á»‹nh mÃ´i trÆ°á»ng if [[ \u0026#34;${{ github.event_name }}\u0026#34; == \u0026#34;workflow_dispatch\u0026#34; ]]; then ENV=\u0026#34;${{ github.event.inputs.environment }}\u0026#34; elif [[ \u0026#34;${{ github.ref }}\u0026#34; == \u0026#34;refs/heads/main\u0026#34; ]]; then ENV=\u0026#34;prod\u0026#34; elif [[ \u0026#34;${{ github.ref }}\u0026#34; == \u0026#34;refs/heads/develop\u0026#34; ]]; then ENV=\u0026#34;staging\u0026#34; else ENV=\u0026#34;dev\u0026#34; fi echo \u0026#34;environment=$ENV\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT # XÃ¡c Ä‘á»‹nh cÃ³ nÃªn huáº¥n luyá»‡n model hay khÃ´ng if [[ \u0026#34;${{ github.event.inputs.retrain_model }}\u0026#34; == \u0026#34;true\u0026#34; ]] || \\ [[ \u0026#34;${{ github.event_name }}\u0026#34; == \u0026#34;schedule\u0026#34; ]]; then SHOULD_TRAIN=\u0026#34;true\u0026#34; else SHOULD_TRAIN=\u0026#34;false\u0026#34; fi echo \u0026#34;should_train=$SHOULD_TRAIN\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT # XÃ¡c Ä‘á»‹nh cÃ³ nÃªn deploy hay khÃ´ng if [[ \u0026#34;${{ github.event.inputs.deploy_only }}\u0026#34; == \u0026#34;true\u0026#34; ]] || \\ [[ \u0026#34;$ENV\u0026#34; == \u0026#34;prod\u0026#34; \u0026amp;\u0026amp; \u0026#34;${{ github.ref }}\u0026#34; == \u0026#34;refs/heads/main\u0026#34; ]] || \\ [[ \u0026#34;$ENV\u0026#34; == \u0026#34;staging\u0026#34; \u0026amp;\u0026amp; \u0026#34;${{ github.ref }}\u0026#34; == \u0026#34;refs/heads/develop\u0026#34; ]]; then SHOULD_DEPLOY=\u0026#34;true\u0026#34; else SHOULD_DEPLOY=\u0026#34;false\u0026#34; fi echo \u0026#34;should_deploy=$SHOULD_DEPLOY\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT # In thÃ´ng tin echo \u0026#34;Environment: $ENV\u0026#34; echo \u0026#34;Should train model: $SHOULD_TRAIN\u0026#34; echo \u0026#34;Should deploy: $SHOULD_DEPLOY\u0026#34; test: name: Code Quality \u0026amp; Testing needs: setup runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 - name: Setup Python uses: actions/setup-python@v4 with: python-version: \u0026#34;3.10\u0026#34; cache: \u0026#34;pip\u0026#34; - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r core/requirements.txt pip install -r server/requirements.txt pip install pytest pytest-cov pylint black - name: Code formatting check run: | black --check core/ server/ - name: Lint code run: | pylint --disable=C0111,C0103 core/ server/ - name: Run tests run: | pytest tests/ -v --cov=core --cov=server --cov-report=xml - name: Upload coverage report uses: codecov/codecov-action@v3 with: file: ./coverage.xml fail_ci_if_error: false data_validation: name: Data Validation needs: [setup, test] runs-on: ubuntu-latest if: needs.setup.outputs.should_train == \u0026#39;true\u0026#39; steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup Python uses: actions/setup-python@v4 with: python-version: \u0026#34;3.10\u0026#34; cache: \u0026#34;pip\u0026#34; - name: Install dependencies run: | pip install pandas boto3 great_expectations - name: Validate training data run: | python aws/script/validate_data.py \\ --bucket ${{ env.S3_DATA_BUCKET }} \\ --key training/sales_data.csv \\ --output validation_report.json - name: Upload validation report uses: actions/upload-artifact@v3 with: name: data-validation-report path: validation_report.json model_training: name: Model Training needs: [setup, test, data_validation] runs-on: ubuntu-latest if: needs.setup.outputs.should_train == \u0026#39;true\u0026#39; \u0026amp;\u0026amp; success() steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup Python uses: actions/setup-python@v4 with: python-version: \u0026#34;3.10\u0026#34; cache: \u0026#34;pip\u0026#34; - name: Install dependencies run: | pip install boto3 sagemaker pandas scikit-learn - name: Create training job id: training run: | python aws/script/create_training_job.py \\ --job-name \u0026#34;retail-forecast-${{ needs.setup.outputs.build-id }}\u0026#34; \\ --data-bucket ${{ env.S3_DATA_BUCKET }} \\ --output-bucket ${{ env.S3_MODEL_BUCKET }} \\ --instance-type ml.m5.large \\ --hyperparameters \u0026#34;{\\\u0026#34;n_estimators\\\u0026#34;:\\\u0026#34;200\\\u0026#34;,\\\u0026#34;max_depth\\\u0026#34;:\\\u0026#34;10\\\u0026#34;}\u0026#34; - name: Wait for training completion run: | aws sagemaker wait training-job-completed-or-stopped \\ --training-job-name \u0026#34;retail-forecast-${{ needs.setup.outputs.build-id }}\u0026#34; STATUS=$(aws sagemaker describe-training-job \\ --training-job-name \u0026#34;retail-forecast-${{ needs.setup.outputs.build-id }}\u0026#34; \\ --query \u0026#39;TrainingJobStatus\u0026#39; --output text) if [ \u0026#34;$STATUS\u0026#34; != \u0026#34;Completed\u0026#34; ]; then echo \u0026#34;Training failed with status: $STATUS\u0026#34; exit 1 fi model_evaluation: name: Model Evaluation \u0026amp; Registration needs: [setup, model_training] runs-on: ubuntu-latest outputs: model_approved: ${{ steps.evaluate.outputs.model_approved }} model_version: ${{ steps.register.outputs.model_version }} steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup Python uses: actions/setup-python@v4 with: python-version: \u0026#34;3.10\u0026#34; cache: \u0026#34;pip\u0026#34; - name: Install dependencies run: | pip install boto3 sagemaker pandas scikit-learn matplotlib seaborn - name: Evaluate model id: evaluate run: | python aws/script/processing_evaluate.py \\ --job-name \u0026#34;retail-forecast-${{ needs.setup.outputs.build-id }}\u0026#34; \\ --evaluation-data s3://${{ env.S3_DATA_BUCKET }}/validation/sales_data.csv \\ --baseline-metrics s3://${{ env.S3_MODEL_BUCKET }}/baselines/metrics.json \\ --threshold 0.85 # Check result if [ -f \u0026#34;model_approved.txt\u0026#34; ]; then MODEL_APPROVED=$(cat model_approved.txt) echo \u0026#34;model_approved=$MODEL_APPROVED\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;Model evaluation result: $MODEL_APPROVED\u0026#34; else echo \u0026#34;model_approved=false\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;Model evaluation failed\u0026#34; exit 1 fi - name: Register model id: register if: steps.evaluate.outputs.model_approved == \u0026#39;true\u0026#39; run: | MODEL_VERSION=$(python aws/script/register_model.py \\ --job-name \u0026#34;retail-forecast-${{ needs.setup.outputs.build-id }}\u0026#34; \\ --model-package-group ${{ env.MODEL_PACKAGE_GROUP }} \\ --approval-status \u0026#34;Approved\u0026#34;) echo \u0026#34;model_version=$MODEL_VERSION\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;Model registered with version: $MODEL_VERSION\u0026#34; - name: Upload evaluation results uses: actions/upload-artifact@v3 with: name: model-evaluation-report path: | evaluation_results.json plots/*.png build_docker: name: Build \u0026amp; Push Docker Image needs: [setup, test] runs-on: ubuntu-latest outputs: image_tag: ${{ steps.build.outputs.image_tag }} steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Login to Amazon ECR id: login-ecr uses: aws-actions/amazon-ecr-login@v1 - name: Build and push Docker image id: build env: ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }} run: | # Set tag IMAGE_TAG=\u0026#34;${{ needs.setup.outputs.build-id }}\u0026#34; echo \u0026#34;image_tag=$IMAGE_TAG\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT # Get latest model URI (or use placeholder for dev environment) if [[ \u0026#34;${{ needs.setup.outputs.environment }}\u0026#34; == \u0026#34;dev\u0026#34; ]]; then MODEL_URI=\u0026#34;placeholder\u0026#34; else MODEL_URI=$(aws sagemaker list-model-packages \\ --model-package-group-name ${{ env.MODEL_PACKAGE_GROUP }} \\ --sort-by CreationTime --sort-order Descending --max-items 1 \\ --query \u0026#39;ModelPackageSummaries[0].ModelPackageArn\u0026#39; --output text) fi echo \u0026#34;Using model URI: $MODEL_URI\u0026#34; # Build image docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \\ --build-arg MODEL_URI=$MODEL_URI \\ --build-arg BUILD_ID=$IMAGE_TAG \\ --build-arg ENV=${{ needs.setup.outputs.environment }} \\ server/ # Also tag as latest-{environment} docker tag $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG \\ $ECR_REGISTRY/$ECR_REPOSITORY:latest-${{ needs.setup.outputs.environment }} # Security scan docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ aquasec/trivy:latest image --severity HIGH,CRITICAL \\ $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG # Push images docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest-${{ needs.setup.outputs.environment }} echo \u0026#34;Image built and pushed: $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG\u0026#34; deploy_eks: name: Deploy to EKS needs: [setup, test, build_docker, model_evaluation] runs-on: ubuntu-latest if: needs.setup.outputs.should_deploy == \u0026#39;true\u0026#39; environment: name: ${{ needs.setup.outputs.environment }} steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Login to Amazon ECR id: login-ecr uses: aws-actions/amazon-ecr-login@v1 - name: Update kubeconfig run: | aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }} - name: Check if need to deploy new model id: check-model run: | if [[ \u0026#34;${{ needs.model_evaluation.outputs.model_approved }}\u0026#34; == \u0026#34;true\u0026#34; ]]; then echo \u0026#34;deploy_new_model=true\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT else echo \u0026#34;deploy_new_model=false\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT fi - name: Deploy to EKS run: | # Set environment variables NAMESPACE=\u0026#34;retail-forecast-${{ needs.setup.outputs.environment }}\u0026#34; IMAGE_TAG=\u0026#34;${{ needs.build_docker.outputs.image_tag }}\u0026#34; ECR_REGISTRY=\u0026#34;${{ steps.login-ecr.outputs.registry }}\u0026#34; # Update kustomization file with new image cd aws/k8s kustomize edit set image retail-forecast-api=$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG # Apply changes kubectl apply -k overlays/${{ needs.setup.outputs.environment }}/ --namespace $NAMESPACE # Wait for deployment to complete kubectl rollout status deployment/retail-forecast-api -n $NAMESPACE --timeout=300s - name: Create SageMaker endpoint (if new model approved) if: steps.check-model.outputs.deploy_new_model == \u0026#39;true\u0026#39; run: | python aws/script/deploy_endpoint.py \\ --model-package-arn \u0026#34;arn:aws:sagemaker:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:model-package/${{ env.MODEL_PACKAGE_GROUP }}/${{ needs.model_evaluation.outputs.model_version }}\u0026#34; \\ --endpoint-name \u0026#34;retail-forecast-${{ needs.setup.outputs.environment }}\u0026#34; \\ --instance-type ml.t2.medium \\ --instance-count 1 - name: Health check run: | # Get service endpoint NAMESPACE=\u0026#34;retail-forecast-${{ needs.setup.outputs.environment }}\u0026#34; SERVICE_HOST=$(kubectl get ingress -n $NAMESPACE retail-forecast-ingress -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) # If running locally, use port-forwarding if [ -z \u0026#34;$SERVICE_HOST\u0026#34; ]; then echo \u0026#34;No external hostname found, using port-forwarding\u0026#34; kubectl port-forward svc/retail-forecast-service 8080:80 -n $NAMESPACE \u0026amp; sleep 5 SERVICE_HOST=\u0026#34;localhost:8080\u0026#34; fi # Check health endpoint echo \u0026#34;Testing health endpoint: http://$SERVICE_HOST/health\u0026#34; HEALTH_STATUS=$(curl -s -o /dev/null -w \u0026#34;%{http_code}\u0026#34; http://$SERVICE_HOST/health) if [ \u0026#34;$HEALTH_STATUS\u0026#34; -eq 200 ]; then echo \u0026#34;Health check passed: $HEALTH_STATUS\u0026#34; else echo \u0026#34;Health check failed: $HEALTH_STATUS\u0026#34; exit 1 fi # Test prediction endpoint echo \u0026#34;Testing prediction endpoint\u0026#34; PREDICTION_RESULT=$(curl -s -X POST \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;store_id\u0026#34;: 1, \u0026#34;item_id\u0026#34;: 123, \u0026#34;date\u0026#34;: \u0026#34;2023-06-15\u0026#34;}\u0026#39; \\ http://$SERVICE_HOST/predict) echo \u0026#34;Prediction result: $PREDICTION_RESULT\u0026#34; # Check if response contains prediction if [[ $PREDICTION_RESULT == *\u0026#34;prediction\u0026#34;* ]]; then echo \u0026#34;Prediction endpoint working correctly\u0026#34; else echo \u0026#34;Prediction endpoint not returning expected response\u0026#34; exit 1 fi monitoring: name: Setup Monitoring needs: [setup, deploy_eks] runs-on: ubuntu-latest if: always() \u0026amp;\u0026amp; needs.deploy_eks.result == \u0026#39;success\u0026#39; steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup CloudWatch alarms for new deployment run: | # Create/update CloudWatch alarms for API and model performance NAMESPACE=\u0026#34;retail-forecast-${{ needs.setup.outputs.environment }}\u0026#34; DEPLOYMENT_ID=\u0026#34;${{ needs.setup.outputs.build-id }}\u0026#34; aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;RetailForecast-API-Error-Rate-$NAMESPACE\u0026#34; \\ --alarm-description \u0026#34;Monitor error rate for Retail Forecast API\u0026#34; \\ --metric-name \u0026#34;ErrorRate\u0026#34; \\ --namespace \u0026#34;RetailForecast/$NAMESPACE\u0026#34; \\ --statistic \u0026#34;Average\u0026#34; \\ --period 60 \\ --threshold 5 \\ --comparison-operator \u0026#34;GreaterThanThreshold\u0026#34; \\ --evaluation-periods 3 \\ --alarm-actions \u0026#34;arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts\u0026#34; \\ --dimensions \u0026#34;Name=DeploymentId,Value=$DEPLOYMENT_ID\u0026#34; aws cloudwatch put-metric-alarm \\ --alarm-name \u0026#34;RetailForecast-API-Latency-$NAMESPACE\u0026#34; \\ --alarm-description \u0026#34;Monitor latency for Retail Forecast API\u0026#34; \\ --metric-name \u0026#34;Latency\u0026#34; \\ --namespace \u0026#34;RetailForecast/$NAMESPACE\u0026#34; \\ --statistic \u0026#34;Average\u0026#34; \\ --period 60 \\ --threshold 1000 \\ --comparison-operator \u0026#34;GreaterThanThreshold\u0026#34; \\ --evaluation-periods 3 \\ --alarm-actions \u0026#34;arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts\u0026#34; \\ --dimensions \u0026#34;Name=DeploymentId,Value=$DEPLOYMENT_ID\u0026#34; echo \u0026#34;CloudWatch alarms configured successfully\u0026#34; notify: name: Send Notifications needs: [setup, deploy_eks, monitoring] runs-on: ubuntu-latest if: always() steps: - name: Determine workflow status id: status run: | if [[ \u0026#34;${{ needs.deploy_eks.result }}\u0026#34; == \u0026#34;success\u0026#34; ]]; then echo \u0026#34;result=success\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;message=âœ… Deployment successful for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT elif [[ \u0026#34;${{ needs.deploy_eks.result }}\u0026#34; == \u0026#34;failure\u0026#34; ]]; then echo \u0026#34;result=failure\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;message=âŒ Deployment failed for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT else echo \u0026#34;result=skipped\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT echo \u0026#34;message=â„¹ï¸ Deployment skipped for ${{ needs.setup.outputs.environment }} environment (Build ${{ needs.setup.outputs.build-id }})\u0026#34; \u0026gt;\u0026gt; $GITHUB_OUTPUT fi - name: Configure AWS credentials if: steps.status.outputs.result != \u0026#39;skipped\u0026#39; uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Send SNS notification if: steps.status.outputs.result != \u0026#39;skipped\u0026#39; run: | aws sns publish \\ --topic-arn \u0026#34;arn:aws:sns:${{ env.AWS_REGION }}:${{ secrets.AWS_ACCOUNT_ID }}:retail-forecast-alerts\u0026#34; \\ --subject \u0026#34;Retail Forecast Deployment: ${{ steps.status.outputs.result }}\u0026#34; \\ --message \u0026#34;${{ steps.status.outputs.message }}\u0026#34; 3. IAM Role vÃ  Quyá»n háº¡n Äá»ƒ GitHub Actions cÃ³ thá»ƒ tÆ°Æ¡ng tÃ¡c vá»›i cÃ¡c dá»‹ch vá»¥ AWS, chÃºng ta cáº§n thiáº¿t láº­p IAM Role vá»›i quyá»n háº¡n phÃ¹ há»£p vÃ  sá»­ dá»¥ng OIDC Ä‘á»ƒ xÃ¡c thá»±c.\n3.1 Táº¡o IAM Role cho GitHub Actions # Táº¡o IAM Role cho GitHub Actions cat \u0026gt; trust-policy.json \u0026lt;\u0026lt; EOF { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:oidc-provider/token.actions.githubusercontent.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:aud\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34; }, \u0026#34;StringLike\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:sub\u0026#34;: \u0026#34;repo:\u0026lt;GITHUB_ORG\u0026gt;/\u0026lt;REPO_NAME\u0026gt;:*\u0026#34; } } } ] } EOF # Táº¡o role aws iam create-role --role-name GitHubActionsRole \\ --assume-role-policy-document file://trust-policy.json \\ --description \u0026#34;Role for GitHub Actions CI/CD pipeline\u0026#34; 3.2 IAM Policy cho CI/CD Pipeline { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::retail-forecast-data*\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-data*/*\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-models*\u0026#34;, \u0026#34;arn:aws:s3:::retail-forecast-models*/*\u0026#34; ] }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sagemaker:CreateTrainingJob\u0026#34;, \u0026#34;sagemaker:DescribeTrainingJob\u0026#34;, \u0026#34;sagemaker:StopTrainingJob\u0026#34;, \u0026#34;sagemaker:ListTrainingJobs\u0026#34;, \u0026#34;sagemaker:CreateProcessingJob\u0026#34;, \u0026#34;sagemaker:DescribeProcessingJob\u0026#34;, \u0026#34;sagemaker:StopProcessingJob\u0026#34;, \u0026#34;sagemaker:CreateModel\u0026#34;, \u0026#34;sagemaker:DeleteModel\u0026#34;, \u0026#34;sagemaker:DescribeModel\u0026#34;, \u0026#34;sagemaker:CreateEndpoint\u0026#34;, \u0026#34;sagemaker:DescribeEndpoint\u0026#34;, \u0026#34;sagemaker:DeleteEndpoint\u0026#34;, \u0026#34;sagemaker:UpdateEndpoint\u0026#34;, \u0026#34;sagemaker:CreateEndpointConfig\u0026#34;, \u0026#34;sagemaker:DeleteEndpointConfig\u0026#34;, \u0026#34;sagemaker:DescribeEndpointConfig\u0026#34;, \u0026#34;sagemaker:CreateModelPackageGroup\u0026#34;, \u0026#34;sagemaker:DescribeModelPackageGroup\u0026#34;, \u0026#34;sagemaker:ListModelPackageGroups\u0026#34;, \u0026#34;sagemaker:CreateModelPackage\u0026#34;, \u0026#34;sagemaker:DescribeModelPackage\u0026#34;, \u0026#34;sagemaker:ListModelPackages\u0026#34;, \u0026#34;sagemaker:UpdateModelPackage\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ecr:GetAuthorizationToken\u0026#34;, \u0026#34;ecr:BatchCheckLayerAvailability\u0026#34;, \u0026#34;ecr:GetDownloadUrlForLayer\u0026#34;, \u0026#34;ecr:BatchGetImage\u0026#34;, \u0026#34;ecr:InitiateLayerUpload\u0026#34;, \u0026#34;ecr:UploadLayerPart\u0026#34;, \u0026#34;ecr:CompleteLayerUpload\u0026#34;, \u0026#34;ecr:PutImage\u0026#34;, \u0026#34;ecr:ListImages\u0026#34;, \u0026#34;ecr:DescribeImages\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;eks:DescribeCluster\u0026#34;, \u0026#34;eks:ListClusters\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudwatch:PutMetricData\u0026#34;, \u0026#34;cloudwatch:GetMetricData\u0026#34;, \u0026#34;cloudwatch:PutMetricAlarm\u0026#34;, \u0026#34;cloudwatch:DescribeAlarms\u0026#34;, \u0026#34;cloudwatch:DeleteAlarms\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sns:Publish\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sns:*:*:retail-forecast-*\u0026#34; } ] } 3.3 Gáº¯n Policy cho Role # Táº¡o vÃ  gáº¯n policy aws iam create-policy \\ --policy-name GitHubActionsPolicy \\ --policy-document file://github-actions-policy.json aws iam attach-role-policy \\ --role-name GitHubActionsRole \\ --policy-arn arn:aws:iam::\u0026lt;ACCOUNT_ID\u0026gt;:policy/GitHubActionsPolicy # Gáº¯n thÃªm cÃ¡c managed policy cáº§n thiáº¿t aws iam attach-role-policy \\ --role-name GitHubActionsRole \\ --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy aws iam attach-role-policy \\ --role-name GitHubActionsRole \\ --policy-arn arn:aws:iam::aws:policy/AmazonECR-FullAccess 4. Script há»— trá»£ cho Pipeline 4.1 Script táº¡o Training Job # aws/script/create_training_job.py import boto3 import argparse import json import os from datetime import datetime def create_training_job(job_name, data_bucket, output_bucket, instance_type=\u0026#39;ml.m5.large\u0026#39;, hyperparameters=None): \u0026#34;\u0026#34;\u0026#34; Táº¡o SageMaker training job cho Retail Forecast model \u0026#34;\u0026#34;\u0026#34; sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) if hyperparameters is None: hyperparameters = { \u0026#39;n_estimators\u0026#39;: \u0026#39;100\u0026#39;, \u0026#39;max_depth\u0026#39;: \u0026#39;10\u0026#39;, \u0026#39;random_state\u0026#39;: \u0026#39;42\u0026#39; } elif isinstance(hyperparameters, str): hyperparameters = json.loads(hyperparameters) # Láº¥y SageMaker execution role tá»« mÃ´i trÆ°á»ng hoáº·c táº¡o má»›i role_arn = os.environ.get(\u0026#39;SAGEMAKER_ROLE_ARN\u0026#39;) if not role_arn: # TÃ¬m role máº·c Ä‘á»‹nh iam = boto3.client(\u0026#39;iam\u0026#39;) paginator = iam.get_paginator(\u0026#39;list_roles\u0026#39;) for page in paginator.paginate(): for role in page[\u0026#39;Roles\u0026#39;]: if \u0026#39;AmazonSageMaker-ExecutionRole\u0026#39; in role[\u0026#39;RoleName\u0026#39;]: role_arn = role[\u0026#39;Arn\u0026#39;] break if not role_arn: raise ValueError(\u0026#34;KhÃ´ng tÃ¬m tháº¥y SageMaker execution role\u0026#34;) # Cáº¥u hÃ¬nh training job training_params = { \u0026#39;TrainingJobName\u0026#39;: job_name, \u0026#39;HyperParameters\u0026#39;: hyperparameters, \u0026#39;AlgorithmSpecification\u0026#39;: { \u0026#39;TrainingImage\u0026#39;: \u0026#39;683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3\u0026#39;, \u0026#39;TrainingInputMode\u0026#39;: \u0026#39;File\u0026#39; }, \u0026#39;RoleArn\u0026#39;: role_arn, \u0026#39;InputDataConfig\u0026#39;: [ { \u0026#39;ChannelName\u0026#39;: \u0026#39;train\u0026#39;, \u0026#39;DataSource\u0026#39;: { \u0026#39;S3DataSource\u0026#39;: { \u0026#39;S3DataType\u0026#39;: \u0026#39;S3Prefix\u0026#39;, \u0026#39;S3Uri\u0026#39;: f\u0026#39;s3://{data_bucket}/training/\u0026#39;, \u0026#39;S3DataDistributionType\u0026#39;: \u0026#39;FullyReplicated\u0026#39; } }, \u0026#39;ContentType\u0026#39;: \u0026#39;text/csv\u0026#39; }, { \u0026#39;ChannelName\u0026#39;: \u0026#39;validation\u0026#39;, \u0026#39;DataSource\u0026#39;: { \u0026#39;S3DataSource\u0026#39;: { \u0026#39;S3DataType\u0026#39;: \u0026#39;S3Prefix\u0026#39;, \u0026#39;S3Uri\u0026#39;: f\u0026#39;s3://{data_bucket}/validation/\u0026#39;, \u0026#39;S3DataDistributionType\u0026#39;: \u0026#39;FullyReplicated\u0026#39; } }, \u0026#39;ContentType\u0026#39;: \u0026#39;text/csv\u0026#39; } ], \u0026#39;OutputDataConfig\u0026#39;: { \u0026#39;S3OutputPath\u0026#39;: f\u0026#39;s3://{output_bucket}/models/\u0026#39; }, \u0026#39;ResourceConfig\u0026#39;: { \u0026#39;InstanceType\u0026#39;: instance_type, \u0026#39;InstanceCount\u0026#39;: 1, \u0026#39;VolumeSizeInGB\u0026#39;: 30 }, \u0026#39;StoppingCondition\u0026#39;: { \u0026#39;MaxRuntimeInSeconds\u0026#39;: 3600 }, \u0026#39;Tags\u0026#39;: [ {\u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailForecast\u0026#39;}, {\u0026#39;Key\u0026#39;: \u0026#39;CreatedBy\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;GitHubActions\u0026#39;}, {\u0026#39;Key\u0026#39;: \u0026#39;Environment\u0026#39;, \u0026#39;Value\u0026#39;: os.environ.get(\u0026#39;ENVIRONMENT\u0026#39;, \u0026#39;dev\u0026#39;)} ] } # Táº¡o training job response = sagemaker.create_training_job(**training_params) print(f\u0026#34;Training job created: {job_name}\u0026#34;) print(f\u0026#34;ARN: {response[\u0026#39;TrainingJobArn\u0026#39;]}\u0026#34;) return response[\u0026#39;TrainingJobArn\u0026#39;] if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Create SageMaker training job\u0026#39;) parser.add_argument(\u0026#39;--job-name\u0026#39;, type=str, required=True, help=\u0026#39;Name for the training job\u0026#39;) parser.add_argument(\u0026#39;--data-bucket\u0026#39;, type=str, required=True, help=\u0026#39;S3 bucket containing training data\u0026#39;) parser.add_argument(\u0026#39;--output-bucket\u0026#39;, type=str, required=True, help=\u0026#39;S3 bucket for output artifacts\u0026#39;) parser.add_argument(\u0026#39;--instance-type\u0026#39;, type=str, default=\u0026#39;ml.m5.large\u0026#39;, help=\u0026#39;SageMaker instance type\u0026#39;) parser.add_argument(\u0026#39;--hyperparameters\u0026#39;, type=str, default=None, help=\u0026#39;JSON string of hyperparameters\u0026#39;) args = parser.parse_args() create_training_job( args.job_name, args.data_bucket, args.output_bucket, args.instance_type, args.hyperparameters ) 4.2 Script Ä‘Ã¡nh giÃ¡ Model # aws/script/processing_evaluate.py import boto3 import argparse import json import os import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score def evaluate_model(job_name, evaluation_data, baseline_metrics=None, threshold=0.85): \u0026#34;\u0026#34;\u0026#34;ÄÃ¡nh giÃ¡ model vÃ  so sÃ¡nh vá»›i baseline metrics\u0026#34;\u0026#34;\u0026#34; # Táº¡o thÆ° má»¥c cho plots os.makedirs(\u0026#39;plots\u0026#39;, exist_ok=True) sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) s3 = boto3.client(\u0026#39;s3\u0026#39;) # Láº¥y thÃ´ng tin training job response = sagemaker.describe_training_job(TrainingJobName=job_name) if response[\u0026#39;TrainingJobStatus\u0026#39;] != \u0026#39;Completed\u0026#39;: raise Exception(f\u0026#34;Training job {job_name} chÆ°a hoÃ n thÃ nh\u0026#34;) model_artifacts = response[\u0026#39;ModelArtifacts\u0026#39;][\u0026#39;S3ModelArtifacts\u0026#39;] print(f\u0026#34;Model artifacts: {model_artifacts}\u0026#34;) # Download dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ tá»« S3 if evaluation_data.startswith(\u0026#39;s3://\u0026#39;): bucket, key = evaluation_data.replace(\u0026#39;s3://\u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;/\u0026#39;, 1) local_path = \u0026#39;evaluation_data.csv\u0026#39; s3.download_file(bucket, key, local_path) else: local_path = evaluation_data # Load dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ eval_data = pd.read_csv(local_path) # Trong dá»± Ã¡n thá»±c táº¿, á»Ÿ Ä‘Ã¢y sáº½ load model tá»« S3 vÃ  Ä‘Ã¡nh giÃ¡ # VÃ­ dá»¥ Ä‘Æ¡n giáº£n nÃ y giáº£ Ä‘á»‹nh chÃºng ta Ä‘Ã£ cÃ³ káº¿t quáº£ dá»± Ä‘oÃ¡n # Giáº£ Ä‘á»‹nh káº¿t quáº£ Ä‘Ã¡nh giÃ¡ # Trong thá»±c táº¿, báº¡n sáº½ load model vÃ  thá»±c hiá»‡n dá»± Ä‘oÃ¡n y_true = eval_data[\u0026#39;target\u0026#39;].values y_pred = y_true * 0.9 + np.random.normal(0, 0.2, size=y_true.shape) # TÃ­nh toÃ¡n metrics metrics = { \u0026#39;mse\u0026#39;: mean_squared_error(y_true, y_pred), \u0026#39;rmse\u0026#39;: np.sqrt(mean_squared_error(y_true, y_pred)), \u0026#39;mae\u0026#39;: mean_absolute_error(y_true, y_pred), \u0026#39;r2_score\u0026#39;: r2_score(y_true, y_pred), \u0026#39;accuracy\u0026#39;: 0.88 # Giáº£ Ä‘á»‹nh cho forecasting task } # So sÃ¡nh vá»›i baseline metrics náº¿u cÃ³ baseline = {} if baseline_metrics: if baseline_metrics.startswith(\u0026#39;s3://\u0026#39;): bucket, key = baseline_metrics.replace(\u0026#39;s3://\u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;/\u0026#39;, 1) local_baseline = \u0026#39;baseline_metrics.json\u0026#39; try: s3.download_file(bucket, key, local_baseline) with open(local_baseline, \u0026#39;r\u0026#39;) as f: baseline = json.load(f) except: print(f\u0026#34;KhÃ´ng tÃ¬m tháº¥y baseline metrics: {baseline_metrics}\u0026#34;) else: try: with open(baseline_metrics, \u0026#39;r\u0026#39;) as f: baseline = json.load(f) except: print(f\u0026#34;KhÃ´ng tÃ¬m tháº¥y baseline metrics: {baseline_metrics}\u0026#34;) # Quyáº¿t Ä‘á»‹nh model cÃ³ Ä‘Æ°á»£c cháº¥p nháº­n hay khÃ´ng approved = True if baseline: print(\u0026#34;So sÃ¡nh vá»›i baseline:\u0026#34;) for key in metrics: if key in baseline: improvement = (metrics[key] - baseline[key]) / baseline[key] * 100 print(f\u0026#34;{key}: {metrics[key]:.4f} (baseline: {baseline[key]:.4f}, {improvement:+.2f}%)\u0026#34;) # Kiá»ƒm tra tiÃªu chÃ­ cáº£i thiá»‡n if key == \u0026#39;accuracy\u0026#39; and metrics[key] \u0026lt; threshold: approved = False elif key in [\u0026#39;mse\u0026#39;, \u0026#39;rmse\u0026#39;, \u0026#39;mae\u0026#39;] and metrics[key] \u0026gt; baseline[key] * 1.1: # Cho phÃ©p tá»‡ hÆ¡n 10% approved = False else: print(\u0026#34;KhÃ´ng cÃ³ baseline Ä‘á»ƒ so sÃ¡nh\u0026#34;) # Táº¡o visualizations plt.figure(figsize=(10, 6)) plt.scatter(y_true, y_pred, alpha=0.5) plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], \u0026#39;r--\u0026#39;) plt.xlabel(\u0026#39;Actual\u0026#39;) plt.ylabel(\u0026#39;Predicted\u0026#39;) plt.title(\u0026#39;Actual vs Predicted Values\u0026#39;) plt.savefig(\u0026#39;plots/actual_vs_predicted.png\u0026#39;) # Residual plot residuals = y_true - y_pred plt.figure(figsize=(10, 6)) plt.scatter(y_pred, residuals, alpha=0.5) plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors=\u0026#39;r\u0026#39;, linestyles=\u0026#39;--\u0026#39;) plt.xlabel(\u0026#39;Predicted\u0026#39;) plt.ylabel(\u0026#39;Residuals\u0026#39;) plt.title(\u0026#39;Residual Plot\u0026#39;) plt.savefig(\u0026#39;plots/residuals.png\u0026#39;) # LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ result = { \u0026#39;job_name\u0026#39;: job_name, \u0026#39;metrics\u0026#39;: metrics, \u0026#39;baseline\u0026#39;: baseline, \u0026#39;approved\u0026#39;: approved, \u0026#39;model_uri\u0026#39;: model_artifacts } with open(\u0026#39;evaluation_results.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(result, f, indent=2) # LÆ°u káº¿t quáº£ approved Ä‘á»ƒ GitHub Actions cÃ³ thá»ƒ Ä‘á»c with open(\u0026#39;model_approved.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(str(approved).lower()) print(f\u0026#34;Model approved: {approved}\u0026#34;) return approved if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Evaluate trained model\u0026#39;) parser.add_argument(\u0026#39;--job-name\u0026#39;, type=str, required=True, help=\u0026#39;Name of the SageMaker training job\u0026#39;) parser.add_argument(\u0026#39;--evaluation-data\u0026#39;, type=str, required=True, help=\u0026#39;S3 URI or local path to evaluation data\u0026#39;) parser.add_argument(\u0026#39;--baseline-metrics\u0026#39;, type=str, default=None, help=\u0026#39;S3 URI or local path to baseline metrics\u0026#39;) parser.add_argument(\u0026#39;--threshold\u0026#39;, type=float, default=0.85, help=\u0026#39;Accuracy threshold for model approval\u0026#39;) args = parser.parse_args() evaluate_model( args.job_name, args.evaluation_data, args.baseline_metrics, args.threshold ) 4.3 Script Ä‘Äƒng kÃ½ Model Registry # aws/script/register_model.py import boto3 import argparse import json import time def register_model(job_name, model_package_group, approval_status=\u0026#39;PendingManualApproval\u0026#39;): \u0026#34;\u0026#34;\u0026#34;ÄÄƒng kÃ½ model vÃ o Model Registry\u0026#34;\u0026#34;\u0026#34; sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) # Kiá»ƒm tra xem Model Package Group Ä‘Ã£ tá»“n táº¡i chÆ°a try: sagemaker.describe_model_package_group(ModelPackageGroupName=model_package_group) print(f\u0026#34;Model Package Group {model_package_group} Ä‘Ã£ tá»“n táº¡i\u0026#34;) except sagemaker.exceptions.ResourceNotFound: print(f\u0026#34;Táº¡o Model Package Group má»›i: {model_package_group}\u0026#34;) sagemaker.create_model_package_group( ModelPackageGroupName=model_package_group, ModelPackageGroupDescription=\u0026#34;Retail Forecast Models\u0026#34;, Tags=[ {\u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailForecast\u0026#39;} ] ) # Láº¥y thÃ´ng tin training job training_job = sagemaker.describe_training_job(TrainingJobName=job_name) model_data_url = training_job[\u0026#39;ModelArtifacts\u0026#39;][\u0026#39;S3ModelArtifacts\u0026#39;] image_uri = training_job[\u0026#39;AlgorithmSpecification\u0026#39;][\u0026#39;TrainingImage\u0026#39;] # ÄÄƒng kÃ½ model package model_package_name = f\u0026#34;{model_package_group}-{int(time.time())}\u0026#34; create_model_package_input_dict = { \u0026#39;ModelPackageGroupName\u0026#39;: model_package_group, \u0026#39;ModelPackageDescription\u0026#39;: f\u0026#34;Model trained by job {job_name}\u0026#34;, \u0026#39;InferenceSpecification\u0026#39;: { \u0026#39;Containers\u0026#39;: [ { \u0026#39;Image\u0026#39;: image_uri, \u0026#39;ModelDataUrl\u0026#39;: model_data_url } ], \u0026#39;SupportedContentTypes\u0026#39;: [\u0026#39;text/csv\u0026#39;], \u0026#39;SupportedResponseMIMETypes\u0026#39;: [\u0026#39;text/csv\u0026#39;] }, \u0026#39;ModelApprovalStatus\u0026#39;: approval_status, \u0026#39;CustomerMetadataProperties\u0026#39;: { \u0026#39;TrainingJobName\u0026#39;: job_name, \u0026#39;CreatedBy\u0026#39;: \u0026#39;GitHubActionsCI\u0026#39; }, \u0026#39;Tags\u0026#39;: [ {\u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailForecast\u0026#39;} ] } # ThÃªm model metrics náº¿u cÃ³ try: with open(\u0026#39;evaluation_results.json\u0026#39;, \u0026#39;r\u0026#39;) as f: eval_results = json.load(f) model_metrics = [] for metric_name, value in eval_results.get(\u0026#39;metrics\u0026#39;, {}).items(): model_metrics.append({ \u0026#39;Name\u0026#39;: metric_name, \u0026#39;Value\u0026#39;: float(value) }) if model_metrics: create_model_package_input_dict[\u0026#39;ModelMetrics\u0026#39;] = { \u0026#39;ModelQuality\u0026#39;: { \u0026#39;Statistics\u0026#39;: { \u0026#39;ContentType\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Values\u0026#39;: model_metrics } } } except Exception as e: print(f\u0026#34;KhÃ´ng thá»ƒ Ä‘á»c káº¿t quáº£ Ä‘Ã¡nh giÃ¡: {e}\u0026#34;) response = sagemaker.create_model_package(**create_model_package_input_dict) print(f\u0026#34;Model package Ä‘Ã£ Ä‘Æ°á»£c táº¡o: {response[\u0026#39;ModelPackageArn\u0026#39;]}\u0026#34;) # Láº¥y version tá»« ARN model_version = response[\u0026#39;ModelPackageArn\u0026#39;].split(\u0026#39;/\u0026#39;)[-1] print(f\u0026#34;Model version: {model_version}\u0026#34;) return model_version if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Register model to SageMaker Model Registry\u0026#39;) parser.add_argument(\u0026#39;--job-name\u0026#39;, type=str, required=True, help=\u0026#39;Name of the training job\u0026#39;) parser.add_argument(\u0026#39;--model-package-group\u0026#39;, type=str, required=True, help=\u0026#39;Model package group name\u0026#39;) parser.add_argument(\u0026#39;--approval-status\u0026#39;, type=str, default=\u0026#39;PendingManualApproval\u0026#39;, choices=[\u0026#39;Approved\u0026#39;, \u0026#39;Rejected\u0026#39;, \u0026#39;PendingManualApproval\u0026#39;], help=\u0026#39;Initial model approval status\u0026#39;) args = parser.parse_args() model_version = register_model( args.job_name, args.model_package_group, args.approval_status ) print(model_version) 4.4 Script triá»ƒn khai Endpoint # aws/script/deploy_endpoint.py import boto3 import argparse import time import uuid def deploy_endpoint(model_package_arn, endpoint_name, instance_type, instance_count=1): \u0026#34;\u0026#34;\u0026#34;Deploy model to SageMaker endpoint\u0026#34;\u0026#34;\u0026#34; sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) timestamp = int(time.time()) # Táº¡o model model_name = f\u0026#34;{endpoint_name}-model-{timestamp}\u0026#34; print(f\u0026#34;Creating model {model_name} from {model_package_arn}\u0026#34;) model_response = sagemaker.create_model( ModelName=model_name, PrimaryContainer={ \u0026#39;ModelPackageName\u0026#39;: model_package_arn }, ExecutionRoleArn=sagemaker.get_caller_identity()[\u0026#39;RoleArn\u0026#39;] ) # Táº¡o endpoint config config_name = f\u0026#34;{endpoint_name}-config-{timestamp}\u0026#34; print(f\u0026#34;Creating endpoint config {config_name}\u0026#34;) endpoint_config_response = sagemaker.create_endpoint_config( EndpointConfigName=config_name, ProductionVariants=[ { \u0026#39;VariantName\u0026#39;: \u0026#39;default\u0026#39;, \u0026#39;ModelName\u0026#39;: model_name, \u0026#39;InstanceType\u0026#39;: instance_type, \u0026#39;InitialInstanceCount\u0026#39;: instance_count, \u0026#39;InitialVariantWeight\u0026#39;: 1.0 } ], Tags=[ {\u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailForecast\u0026#39;} ] ) # Kiá»ƒm tra xem endpoint Ä‘Ã£ tá»“n táº¡i chÆ°a endpoint_exists = False try: response = sagemaker.describe_endpoint(EndpointName=endpoint_name) endpoint_exists = True except sagemaker.exceptions.ClientError: endpoint_exists = False # Táº¡o hoáº·c cáº­p nháº­t endpoint if endpoint_exists: print(f\u0026#34;Updating endpoint {endpoint_name}\u0026#34;) endpoint_response = sagemaker.update_endpoint( EndpointName=endpoint_name, EndpointConfigName=config_name ) else: print(f\u0026#34;Creating endpoint {endpoint_name}\u0026#34;) endpoint_response = sagemaker.create_endpoint( EndpointName=endpoint_name, EndpointConfigName=config_name, Tags=[ {\u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailForecast\u0026#39;} ] ) # Äá»£i endpoint sáºµn sÃ ng print(f\u0026#34;Waiting for endpoint {endpoint_name} to be in service...\u0026#34;) waiter = sagemaker.get_waiter(\u0026#39;endpoint_in_service\u0026#39;) waiter.wait(EndpointName=endpoint_name) # Láº¥y thÃ´ng tin endpoint endpoint_info = sagemaker.describe_endpoint(EndpointName=endpoint_name) print(f\u0026#34;Endpoint {endpoint_name} is ready (status: {endpoint_info[\u0026#39;EndpointStatus\u0026#39;]})\u0026#34;) return { \u0026#39;endpoint_name\u0026#39;: endpoint_name, \u0026#39;endpoint_arn\u0026#39;: endpoint_info[\u0026#39;EndpointArn\u0026#39;], \u0026#39;status\u0026#39;: endpoint_info[\u0026#39;EndpointStatus\u0026#39;] } if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Deploy model to SageMaker endpoint\u0026#39;) parser.add_argument(\u0026#39;--model-package-arn\u0026#39;, type=str, required=True, help=\u0026#39;ARN of model package to deploy\u0026#39;) parser.add_argument(\u0026#39;--endpoint-name\u0026#39;, type=str, required=True, help=\u0026#39;Name of the endpoint\u0026#39;) parser.add_argument(\u0026#39;--instance-type\u0026#39;, type=str, default=\u0026#39;ml.t2.medium\u0026#39;, help=\u0026#39;Instance type for inference\u0026#39;) parser.add_argument(\u0026#39;--instance-count\u0026#39;, type=int, default=1, help=\u0026#39;Number of instances\u0026#39;) args = parser.parse_args() result = deploy_endpoint( args.model_package_arn, args.endpoint_name, args.instance_type, args.instance_count ) print(f\u0026#34;Endpoint deployment complete: {result}\u0026#34;) 4.5 Script kiá»ƒm tra hiá»‡u suáº¥t Endpoint # aws/script/autoscaling_endpoint.py import boto3 import argparse import json import time import datetime def setup_autoscaling(endpoint_name, min_capacity=1, max_capacity=4, target_value=70.0, scale_in_cooldown=300, scale_out_cooldown=60): \u0026#34;\u0026#34;\u0026#34;Thiáº¿t láº­p autoscaling cho SageMaker endpoint\u0026#34;\u0026#34;\u0026#34; # Láº¥y variant name vÃ  ARN cá»§a endpoint sm = boto3.client(\u0026#39;sagemaker\u0026#39;) endpoint = sm.describe_endpoint(EndpointName=endpoint_name) endpoint_arn = endpoint[\u0026#39;EndpointArn\u0026#39;] config_name = endpoint[\u0026#39;EndpointConfigName\u0026#39;] config = sm.describe_endpoint_config(EndpointConfigName=config_name) variant_name = config[\u0026#39;ProductionVariants\u0026#39;][0][\u0026#39;VariantName\u0026#39;] # Chuáº©n bá»‹ resource ID cho autoscaling resource_id = f\u0026#34;endpoint/{endpoint_name}/variant/{variant_name}\u0026#34; # Thiáº¿t láº­p application autoscaling aas = boto3.client(\u0026#39;application-autoscaling\u0026#39;) # ÄÄƒng kÃ½ scalable target aas.register_scalable_target( ServiceNamespace=\u0026#39;sagemaker\u0026#39;, ResourceId=resource_id, ScalableDimension=\u0026#39;sagemaker:variant:DesiredInstanceCount\u0026#39;, MinCapacity=min_capacity, MaxCapacity=max_capacity ) # Táº¡o scaling policy response = aas.put_scaling_policy( PolicyName=f\u0026#34;{endpoint_name}-scaling-policy\u0026#34;, ServiceNamespace=\u0026#39;sagemaker\u0026#39;, ResourceId=resource_id, ScalableDimension=\u0026#39;sagemaker:variant:DesiredInstanceCount\u0026#39;, PolicyType=\u0026#39;TargetTrackingScaling\u0026#39;, TargetTrackingScalingPolicyConfiguration={ \u0026#39;TargetValue\u0026#39;: target_value, \u0026#39;PredefinedMetricSpecification\u0026#39;: { \u0026#39;PredefinedMetricType\u0026#39;: \u0026#39;SageMakerVariantInvocationsPerInstance\u0026#39; }, \u0026#39;ScaleInCooldown\u0026#39;: scale_in_cooldown, \u0026#39;ScaleOutCooldown\u0026#39;: scale_out_cooldown } ) print(f\u0026#34;Autoscaling configured for endpoint {endpoint_name}\u0026#34;) print(f\u0026#34;Min capacity: {min_capacity}, Max capacity: {max_capacity}\u0026#34;) print(f\u0026#34;Target value: {target_value} invocations per instance\u0026#34;) return { \u0026#39;endpoint_name\u0026#39;: endpoint_name, \u0026#39;resource_id\u0026#39;: resource_id, \u0026#39;scaling_policy_arn\u0026#39;: response[\u0026#39;PolicyARN\u0026#39;] } def load_test_endpoint(endpoint_name, test_data_path, duration=60, rate=10): \u0026#34;\u0026#34;\u0026#34;Test táº£i endpoint Ä‘á»ƒ kiá»ƒm tra hiá»‡u suáº¥t vÃ  autoscaling\u0026#34;\u0026#34;\u0026#34; sagemaker_runtime = boto3.client(\u0026#39;sagemaker-runtime\u0026#39;) # Load test data with open(test_data_path, \u0026#39;r\u0026#39;) as f: test_data = json.load(f) # Náº¿u test data lÃ  list, láº¥y pháº§n tá»­ Ä‘áº§u tiÃªn if isinstance(test_data, list): sample = test_data[0] else: sample = test_data # Chuáº©n bá»‹ test start_time = time.time() end_time = start_time + duration request_count = 0 success_count = 0 latencies = [] print(f\u0026#34;Starting load test on endpoint {endpoint_name}\u0026#34;) print(f\u0026#34;Duration: {duration} seconds, Rate: {rate} requests/second\u0026#34;) # Thá»±c hiá»‡n load test while time.time() \u0026lt; end_time: batch_start = time.time() for _ in range(rate): if time.time() \u0026gt;= end_time: break try: # Gá»­i request request_start = time.time() response = sagemaker_runtime.invoke_endpoint( EndpointName=endpoint_name, ContentType=\u0026#39;application/json\u0026#39;, Body=json.dumps(sample) ) # Äo latency latency = (time.time() - request_start) * 1000 # milliseconds latencies.append(latency) # Äá»c káº¿t quáº£ result = json.loads(response[\u0026#39;Body\u0026#39;].read().decode()) # Cáº­p nháº­t counter request_count += 1 success_count += 1 if request_count % 50 == 0: print(f\u0026#34;Processed {request_count} requests...\u0026#34;) except Exception as e: request_count += 1 print(f\u0026#34;Error invoking endpoint: {e}\u0026#34;) # Äá»£i Ä‘áº¿n Ä‘áº§u giÃ¢y tiáº¿p theo elapsed = time.time() - batch_start if elapsed \u0026lt; 1.0: time.sleep(1.0 - elapsed) # TÃ­nh toÃ¡n káº¿t quáº£ total_time = time.time() - start_time avg_rate = request_count / total_time success_rate = (success_count / request_count) * 100 if request_count \u0026gt; 0 else 0 if latencies: avg_latency = sum(latencies) / len(latencies) min_latency = min(latencies) max_latency = max(latencies) p95_latency = sorted(latencies)[int(len(latencies) * 0.95)] p99_latency = sorted(latencies)[int(len(latencies) * 0.99)] else: avg_latency = min_latency = max_latency = p95_latency = p99_latency = 0 # In káº¿t quáº£ results = { \u0026#39;endpoint\u0026#39;: endpoint_name, \u0026#39;duration_seconds\u0026#39;: total_time, \u0026#39;request_count\u0026#39;: request_count, \u0026#39;success_count\u0026#39;: success_count, \u0026#39;success_rate_percent\u0026#39;: success_rate, \u0026#39;avg_requests_per_second\u0026#39;: avg_rate, \u0026#39;latency_ms\u0026#39;: { \u0026#39;avg\u0026#39;: avg_latency, \u0026#39;min\u0026#39;: min_latency, \u0026#39;max\u0026#39;: max_latency, \u0026#39;p95\u0026#39;: p95_latency, \u0026#39;p99\u0026#39;: p99_latency } } print(\u0026#34;\\nLoad Test Results:\u0026#34;) print(json.dumps(results, indent=2)) with open(\u0026#39;load_test_results.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(results, f, indent=2) return results if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Setup autoscaling and test SageMaker endpoint\u0026#39;) subparsers = parser.add_subparsers(dest=\u0026#39;command\u0026#39;) # Subparser cho autoscaling autoscaling_parser = subparsers.add_parser(\u0026#39;autoscale\u0026#39;) autoscaling_parser.add_argument(\u0026#39;--endpoint-name\u0026#39;, type=str, required=True) autoscaling_parser.add_argument(\u0026#39;--min-capacity\u0026#39;, type=int, default=1) autoscaling_parser.add_argument(\u0026#39;--max-capacity\u0026#39;, type=int, default=4) autoscaling_parser.add_argument(\u0026#39;--target-value\u0026#39;, type=float, default=70.0) # Subparser cho load testing loadtest_parser = subparsers.add_parser(\u0026#39;loadtest\u0026#39;) loadtest_parser.add_argument(\u0026#39;--endpoint-name\u0026#39;, type=str, required=True) loadtest_parser.add_argument(\u0026#39;--test-data\u0026#39;, type=str, required=True) loadtest_parser.add_argument(\u0026#39;--duration\u0026#39;, type=int, default=60) loadtest_parser.add_argument(\u0026#39;--rate\u0026#39;, type=int, default=10) args = parser.parse_args() if args.command == \u0026#39;autoscale\u0026#39;: setup_autoscaling( args.endpoint_name, args.min_capacity, args.max_capacity, args.target_value ) elif args.command == \u0026#39;loadtest\u0026#39;: load_test_endpoint( args.endpoint_name, args.test_data, args.duration, args.rate ) else: parser.print_help() 5. GiÃ¡m sÃ¡t vÃ  ThÃ´ng bÃ¡o 5.1 Thiáº¿t láº­p SNS Topic # Táº¡o SNS topic cho thÃ´ng bÃ¡o deployment aws sns create-topic --name retail-forecast-alerts # ÄÄƒng kÃ½ email nháº­n thÃ´ng bÃ¡o aws sns subscribe \\ --topic-arn arn:aws:sns:us-east-1:\u0026lt;ACCOUNT_ID\u0026gt;:retail-forecast-alerts \\ --protocol email \\ --notification-endpoint team@example.com # ÄÄƒng kÃ½ webhook Slack aws sns subscribe \\ --topic-arn arn:aws:sns:us-east-1:\u0026lt;ACCOUNT_ID\u0026gt;:retail-forecast-alerts \\ --protocol https \\ --notification-endpoint https://hooks.slack.com/services/XXXX/YYYY/ZZZZ 5.2 CloudWatch Dashboard cho CI/CD Pipeline # Táº¡o CloudWatch dashboard cho pipeline aws cloudwatch put-dashboard \\ --dashboard-name RetailForecast-CI-CD-Pipeline \\ --dashboard-body \u0026#39;{ \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 0, \u0026#34;width\u0026#34;: 12, \u0026#34;height\u0026#34;: 6, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;AWS/SageMaker\u0026#34;, \u0026#34;TrainingJobsCompleted\u0026#34;, { \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;period\u0026#34;: 86400 } ], [ \u0026#34;AWS/SageMaker\u0026#34;, \u0026#34;TrainingJobsFailed\u0026#34;, { \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;period\u0026#34;: 86400 } ] ], \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;stacked\u0026#34;: false, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;SageMaker Training Jobs\u0026#34;, \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;x\u0026#34;: 12, \u0026#34;y\u0026#34;: 0, \u0026#34;width\u0026#34;: 12, \u0026#34;height\u0026#34;: 6, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;AWS/ApplicationELB\u0026#34;, \u0026#34;TargetResponseTime\u0026#34;, \u0026#34;LoadBalancer\u0026#34;, \u0026#34;app/retail-forecast/abcdef\u0026#34; ], [ \u0026#34;.\u0026#34;, \u0026#34;HTTPCode_Target_5XX_Count\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34; ], [ \u0026#34;.\u0026#34;, \u0026#34;HTTPCode_Target_4XX_Count\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34; ] ], \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;stacked\u0026#34;: false, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;API Performance\u0026#34;, \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;x\u0026#34;: 0, \u0026#34;y\u0026#34;: 6, \u0026#34;width\u0026#34;: 12, \u0026#34;height\u0026#34;: 6, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;AWS/SageMaker\u0026#34;, \u0026#34;Invocations\u0026#34;, \u0026#34;EndpointName\u0026#34;, \u0026#34;retail-forecast-prod\u0026#34;, { \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;period\u0026#34;: 60 } ], [ \u0026#34;.\u0026#34;, \u0026#34;InvocationsPerInstance\u0026#34;, \u0026#34;.\u0026#34;, \u0026#34;.\u0026#34; ] ], \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;stacked\u0026#34;: false, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;SageMaker Endpoint Invocations\u0026#34;, \u0026#34;period\u0026#34;: 300 } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;x\u0026#34;: 12, \u0026#34;y\u0026#34;: 6, \u0026#34;width\u0026#34;: 12, \u0026#34;height\u0026#34;: 6, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [ \u0026#34;RetailForecast/Pipeline\u0026#34;, \u0026#34;DeploymentFrequency\u0026#34; ], [ \u0026#34;.\u0026#34;, \u0026#34;FailureRate\u0026#34; ], [ \u0026#34;.\u0026#34;, \u0026#34;LeadTime\u0026#34; ] ], \u0026#34;view\u0026#34;: \u0026#34;timeSeries\u0026#34;, \u0026#34;stacked\u0026#34;: false, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;CI/CD Metrics\u0026#34;, \u0026#34;period\u0026#34;: 86400, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34; } } ] }\u0026#39; 5.3 Cáº¥u hÃ¬nh Pipeline Metrics # aws/script/pipeline_metrics.py import boto3 import json import argparse from datetime import datetime, timedelta def publish_pipeline_metrics(deploy_success=True, lead_time=None): \u0026#34;\u0026#34;\u0026#34;Publish CI/CD pipeline metrics to CloudWatch\u0026#34;\u0026#34;\u0026#34; cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) # Get date parts for daily metrics now = datetime.utcnow() today = now.strftime(\u0026#39;%Y-%m-%d\u0026#39;) # Increment deployment count cloudwatch.put_metric_data( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;DeploymentCount\u0026#39;, \u0026#39;Dimensions\u0026#39;: [ { \u0026#39;Name\u0026#39;: \u0026#39;Date\u0026#39;, \u0026#39;Value\u0026#39;: today } ], \u0026#39;Value\u0026#39;: 1.0, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39; } ] ) # Add deployment success/failure cloudwatch.put_metric_data( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;DeploymentSuccess\u0026#39;, \u0026#39;Dimensions\u0026#39;: [ { \u0026#39;Name\u0026#39;: \u0026#39;Date\u0026#39;, \u0026#39;Value\u0026#39;: today } ], \u0026#39;Value\u0026#39;: 1.0 if deploy_success else 0.0, \u0026#39;Unit\u0026#39;: \u0026#39;Count\u0026#39; } ] ) # Add lead time if provided if lead_time is not None: cloudwatch.put_metric_data( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricData=[ { \u0026#39;MetricName\u0026#39;: \u0026#39;LeadTimeMinutes\u0026#39;, \u0026#39;Dimensions\u0026#39;: [ { \u0026#39;Name\u0026#39;: \u0026#39;Date\u0026#39;, \u0026#39;Value\u0026#39;: today } ], \u0026#39;Value\u0026#39;: lead_time, \u0026#39;Unit\u0026#39;: \u0026#39;Minutes\u0026#39; } ] ) print(f\u0026#34;Pipeline metrics published successfully\u0026#34;) def calculate_pipeline_kpis(days=30): \u0026#34;\u0026#34;\u0026#34;Calculate KPIs for CI/CD pipeline\u0026#34;\u0026#34;\u0026#34; cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) end_time = datetime.utcnow() start_time = end_time - timedelta(days=days) # Get deployment frequency deploy_count_response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricName=\u0026#39;DeploymentCount\u0026#39;, StartTime=start_time, EndTime=end_time, Period=86400 * days, # entire period Statistics=[\u0026#39;Sum\u0026#39;] ) # Get deployment success/failure deploy_success_response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricName=\u0026#39;DeploymentSuccess\u0026#39;, StartTime=start_time, EndTime=end_time, Period=86400 * days, # entire period Statistics=[\u0026#39;Sum\u0026#39;] ) # Get lead time lead_time_response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;RetailForecast/Pipeline\u0026#39;, MetricName=\u0026#39;LeadTimeMinutes\u0026#39;, StartTime=start_time, EndTime=end_time, Period=86400 * days, # entire period Statistics=[\u0026#39;Average\u0026#39;] ) # Calculate KPIs deploy_count = deploy_count_response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Sum\u0026#39;] if deploy_count_response[\u0026#39;Datapoints\u0026#39;] else 0 deploy_success = deploy_success_response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Sum\u0026#39;] if deploy_success_response[\u0026#39;Datapoints\u0026#39;] else 0 # Calculate metrics deployments_per_day = deploy_count / days if days \u0026gt; 0 else 0 failure_rate = (deploy_count - deploy_success) / deploy_count * 100 if deploy_count \u0026gt; 0 else 0 avg_lead_time = lead_time_response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Average\u0026#39;] if lead_time_response[\u0026#39;Datapoints\u0026#39;] else 0 kpis = { \u0026#39;period_days\u0026#39;: days, \u0026#39;total_deployments\u0026#39;: deploy_count, \u0026#39;successful_deployments\u0026#39;: deploy_success, \u0026#39;deployments_per_day\u0026#39;: deployments_per_day, \u0026#39;failure_rate_percent\u0026#39;: failure_rate, \u0026#39;avg_lead_time_minutes\u0026#39;: avg_lead_time } print(json.dumps(kpis, indent=2)) return kpis if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser(description=\u0026#39;Manage CI/CD pipeline metrics\u0026#39;) subparsers = parser.add_subparsers(dest=\u0026#39;command\u0026#39;) # Subparser cho publish metrics publish_parser = subparsers.add_parser(\u0026#39;publish\u0026#39;) publish_parser.add_argument(\u0026#39;--success\u0026#39;, type=bool, default=True) publish_parser.add_argument(\u0026#39;--lead-time\u0026#39;, type=float, help=\u0026#39;Lead time in minutes\u0026#39;) # Subparser cho calculate KPIs kpi_parser = subparsers.add_parser(\u0026#39;kpi\u0026#39;) kpi_parser.add_argument(\u0026#39;--days\u0026#39;, type=int, default=30) args = parser.parse_args() if args.command == \u0026#39;publish\u0026#39;: publish_pipeline_metrics(args.success, args.lead_time) elif args.command == \u0026#39;kpi\u0026#39;: calculate_pipeline_kpis(args.days) else: parser.print_help() 6. Kiá»ƒm tra vÃ  XÃ¡c thá»±c 6.1 Checklist HoÃ n thÃ nh CÃ¡c thÃ nh pháº§n chÃ­nh Ä‘Ã£ triá»ƒn khai:\nPipeline GitHub Actions: Workflow Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh vá»›i cÃ¡c job Ä‘áº§y Ä‘á»§ IAM Role \u0026amp; Permissions: Role cho GitHub Actions vá»›i OIDC xÃ¡c thá»±c Automated Testing: Unit tests, code quality check, data validation Model Training: Tá»± Ä‘á»™ng trigger SageMaker training jobs Model Evaluation \u0026amp; Registry: ÄÃ¡nh giÃ¡ model vÃ  Ä‘Äƒng kÃ½ vÃ o Model Registry Docker Build: Tá»± Ä‘á»™ng build vÃ  push image lÃªn ECR EKS Deployment: Tá»± Ä‘á»™ng cáº­p nháº­t Kubernetes deployment Health Check: Kiá»ƒm tra vÃ  xÃ¡c nháº­n API hoáº¡t Ä‘á»™ng sau deployment CloudWatch Alarms: Tá»± Ä‘á»™ng cáº¥u hÃ¬nh alert cho deployment má»›i Notifications: ThÃ´ng bÃ¡o káº¿t quáº£ deployment qua SNS 6.2 Kiá»ƒm tra Pipeline Äá»ƒ xÃ¡c nháº­n pipeline hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c, thá»±c hiá»‡n cÃ¡c bÆ°á»›c sau:\nKÃ­ch hoáº¡t pipeline thÃ´ng qua commit má»›i\n# Commit code má»›i echo \u0026#34;# Test CI/CD pipeline\u0026#34; \u0026gt;\u0026gt; README.md git add README.md git commit -m \u0026#34;Test: Trigger CI/CD pipeline\u0026#34; git push origin main # Kiá»ƒm tra GitHub Actions # Pipeline sáº½ tá»± Ä‘á»™ng kÃ­ch hoáº¡t trong vÃ²ng 1 phÃºt KÃ­ch hoáº¡t pipeline thá»§ cÃ´ng vá»›i huáº¥n luyá»‡n model\n# Sá»­ dá»¥ng GitHub UI Ä‘á»ƒ kÃ­ch hoáº¡t workflow vá»›i cÃ¡c tÃ¹y chá»n: # - environment: prod # - retrain_model: true Kiá»ƒm tra ECR repository\n# Kiá»ƒm tra image má»›i trÃªn ECR aws ecr describe-images \\ --repository-name retail-forecast \\ --query \u0026#39;sort_by(imageDetails,\u0026amp; imagePushedAt)[-5:]\u0026#39; # Kiá»ƒm tra tag má»›i nháº¥t aws ecr list-images \\ --repository-name retail-forecast \\ --filter \u0026#34;tagStatus=TAGGED\u0026#34; \\ --query \u0026#39;imageIds[?contains(imageTag, `latest`)]\u0026#39; Kiá»ƒm tra SageMaker model vÃ  deployment\n# Kiá»ƒm tra training job má»›i nháº¥t aws sagemaker list-training-jobs \\ --sort-by CreationTime \\ --sort-order Descending \\ --max-items 5 # Kiá»ƒm tra model package má»›i nháº¥t aws sagemaker list-model-packages \\ --model-package-group-name retail-forecast-models \\ --sort-by CreationTime \\ --sort-order Descending \\ --max-items 5 # Kiá»ƒm tra endpoint aws sagemaker describe-endpoint \\ --endpoint-name retail-forecast-prod Kiá»ƒm tra EKS deployment\n# Kiá»ƒm tra deployment má»›i kubectl get deployments -n retail-forecast-prod # Kiá»ƒm tra pods kubectl get pods -n retail-forecast-prod # Kiá»ƒm tra image Ä‘Æ°á»£c sá»­ dá»¥ng kubectl describe deployment retail-forecast-api -n retail-forecast-prod | grep Image: # Kiá»ƒm tra history rollout kubectl rollout history deployment/retail-forecast-api -n retail-forecast-prod Test API endpoint\n# Láº¥y endpoint URL ENDPOINT=$(kubectl get ingress -n retail-forecast-prod retail-forecast-ingress -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39;) # Kiá»ƒm tra health endpoint curl -v http://$ENDPOINT/health # Test prediction endpoint curl -X POST \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;store_id\u0026#34;: 1, \u0026#34;item_id\u0026#34;: 123, \u0026#34;date\u0026#34;: \u0026#34;2023-12-01\u0026#34;}\u0026#39; \\ http://$ENDPOINT/predict 6.3 Monitoring Commands # Kiá»ƒm tra CloudWatch metrics cá»§a API aws cloudwatch get-metric-statistics \\ --namespace \u0026#34;RetailForecast/prod\u0026#34; \\ --metric-name \u0026#34;Latency\u0026#34; \\ --dimensions Name=DeploymentId,Value=\u0026lt;latest-deployment-id\u0026gt; \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%SZ) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\ --period 60 \\ --statistics Average # Kiá»ƒm tra SageMaker endpoint metrics aws cloudwatch get-metric-statistics \\ --namespace \u0026#34;AWS/SageMaker\u0026#34; \\ --metric-name \u0026#34;ModelLatency\u0026#34; \\ --dimensions Name=EndpointName,Value=retail-forecast-prod \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%SZ) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\ --period 60 \\ --statistics Average # Kiá»ƒm tra Pipeline metrics aws cloudwatch get-metric-statistics \\ --namespace \u0026#34;RetailForecast/Pipeline\u0026#34; \\ --metric-name \u0026#34;DeploymentSuccess\u0026#34; \\ --start-time $(date -u -d \u0026#39;7 days ago\u0026#39; +%Y-%m-%dT%H:%M:%SZ) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) \\ --period 86400 \\ --statistics Sum # Kiá»ƒm tra CloudWatch Logs cho pipeline aws logs get-log-events \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs\u0026#34; \\ --log-stream-name \u0026lt;latest-training-job-name\u0026gt; 7. Best Practices \u0026amp; Tá»‘i Æ°u 7.1 Pipeline Optimization Parallel Execution: Cháº¡y song song cÃ¡c job khÃ´ng phá»¥ thuá»™c nhau Caching: Sá»­ dá»¥ng caching cho pip vÃ  Docker layers Conditional Execution: Chá»‰ cháº¡y cÃ¡c job cáº§n thiáº¿t dá»±a trÃªn loáº¡i thay Ä‘á»•i Matrix Builds: Test trÃªn nhiá»u phiÃªn báº£n Python hoáº·c mÃ´i trÆ°á»ng Artifact Sharing: Sá»­ dá»¥ng artifacts Ä‘á»ƒ chia sáº» dá»¯ liá»‡u giá»¯a cÃ¡c job 7.2 Security Best Practices OIDC Integration: Sá»­ dá»¥ng federation thay vÃ¬ access keys Least Privilege IAM: Role vá»›i quyá»n háº¡n tá»‘i thiá»ƒu cáº§n thiáº¿t Secret Management: Sá»­ dá»¥ng GitHub Secrets hoáº·c AWS Secrets Manager Image Scanning: Kiá»ƒm tra lá»—i báº£o máº­t trong container images Network Security: Giá»›i háº¡n network access trong pipeline 7.3 Quality Gates Code Quality Checks: Linting, formatting, static analysis Unit \u0026amp; Integration Tests: Test tá»± Ä‘á»™ng cho má»i thay Ä‘á»•i code Data Validation: Kiá»ƒm tra tÃ­nh toÃ n váº¹n vÃ  cháº¥t lÆ°á»£ng dá»¯ liá»‡u Model Evaluation: ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t model vá»›i baseline metrics Performance Testing: Kiá»ƒm tra Ä‘á»™ trá»… vÃ  kháº£ nÄƒng xá»­ lÃ½ táº£i cá»§a API 7.4 Observability Pipeline Metrics: Theo dÃµi táº§n suáº¥t deployment, thá»i gian thá»±c hiá»‡n Model Monitoring: Theo dÃµi model drift vÃ  data drift API Metrics: Latency, error rate, throughput cá»§a endpoints Logging: Log táº­p trung vá»›i cáº¥u trÃºc thá»‘ng nháº¥t Alerting: Cáº£nh bÃ¡o sá»›m khi phÃ¡t hiá»‡n váº¥n Ä‘á» Tá»•ng káº¿t CI/CD pipeline Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p Ä‘áº§y Ä‘á»§ sá»­ dá»¥ng GitHub Actions Ä‘á»ƒ tá»± Ä‘á»™ng hÃ³a toÃ n bá»™ quy trÃ¬nh MLOps cho dá»± Ã¡n Retail Prediction:\nTÃ­ch há»£p liÃªn tá»¥c (CI): Tá»± Ä‘á»™ng test, lint, vÃ  build Docker image. Huáº¥n luyá»‡n tá»± Ä‘á»™ng: KÃ­ch hoáº¡t SageMaker training jobs khi cáº§n thiáº¿t. Model Registry: ÄÃ¡nh giÃ¡ vÃ  Ä‘Äƒng kÃ½ model vá»›i quáº£n lÃ½ version. Deployment liÃªn tá»¥c (CD): Tá»± Ä‘á»™ng triá»ƒn khai lÃªn EKS cluster. GiÃ¡m sÃ¡t: CloudWatch metrics vÃ  alerts cho toÃ n bá»™ há»‡ thá»‘ng. Pipeline nÃ y Ä‘áº£m báº£o quy trÃ¬nh MLOps Ä‘Ã¡ng tin cáº­y, tá»± Ä‘á»™ng, vÃ  cÃ³ kháº£ nÄƒng má»Ÿ rá»™ng, giÃºp team cÃ³ thá»ƒ táº­p trung vÃ o viá»‡c cáº£i thiá»‡n model vÃ  tÃ­nh nÄƒng thay vÃ¬ cÃ´ng viá»‡c váº­n hÃ nh thá»§ cÃ´ng.\nğŸ¯ Task 11 Complete - CI/CD Pipeline\nGitHub Actions workflow configured vá»›i Ä‘áº§y Ä‘á»§ CI/CD stages IAM Role \u0026amp; OIDC setup cho secure authentication Automated testing vÃ  quality gates SageMaker integration cho model training \u0026amp; registry EKS deployment automation vá»›i health checks CloudWatch monitoring vÃ  notifications 8. Clean Up CI/CD Resources 8.1 XÃ³a GitHub Actions Workflow # Disable GitHub Actions workflows # Thá»±c hiá»‡n trá»±c tiáº¿p trÃªn GitHub repository hoáº·c qua GitHub CLI # Náº¿u sá»­ dá»¥ng GitHub CLI gh workflow disable mlops-pipeline.yml # List táº¥t cáº£ workflow runs gh run list --limit 50 # Cancel running workflows gh run list --status in_progress --json databaseId --jq \u0026#39;.[].databaseId\u0026#39; | xargs -I {} gh run cancel {} # XÃ³a workflow artifacts (cÃ³ thá»ƒ tá»‘n phÃ­ storage) gh api repos/:owner/:repo/actions/artifacts --paginate | jq -r \u0026#39;.artifacts[] | select(.expired == false) | .id\u0026#39; | xargs -I {} gh api --method DELETE repos/:owner/:repo/actions/artifacts/{} 8.2 XÃ³a IAM Roles vÃ  Policies # List táº¥t cáº£ IAM roles liÃªn quan Ä‘áº¿n CI/CD aws iam list-roles \\ --query \u0026#39;Roles[?contains(RoleName, `GitHub`) || contains(RoleName, `CICD`) || contains(RoleName, `RetailForecast`)].RoleName\u0026#39; \\ --output table # Detach policies trÆ°á»›c khi xÃ³a role aws iam list-attached-role-policies \\ --role-name GitHubActionsRole \\ --query \u0026#39;AttachedPolicies[].PolicyArn\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read policy_arn; do echo \u0026#34;Detaching policy: $policy_arn\u0026#34; aws iam detach-role-policy --role-name GitHubActionsRole --policy-arn \u0026#34;$policy_arn\u0026#34; done # XÃ³a custom policies aws iam delete-policy \\ --policy-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/GitHubActionsPolicy aws iam delete-policy \\ --policy-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/RetailForecastCICDPolicy # XÃ³a roles aws iam delete-role --role-name GitHubActionsRole aws iam delete-role --role-name RetailForecastCICDRole # XÃ³a instance profiles (náº¿u cÃ³) aws iam remove-role-from-instance-profile \\ --instance-profile-name JenkinsInstanceProfile \\ --role-name JenkinsRole || true aws iam delete-instance-profile \\ --instance-profile-name JenkinsInstanceProfile || true aws iam delete-role --role-name JenkinsRole || true 8.3 XÃ³a Jenkins Infrastructure (náº¿u sá»­ dá»¥ng) # Terminate Jenkins EC2 instances aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:Name,Values=Jenkins-Server\u0026#34; \u0026#34;Name=instance-state-name,Values=running\u0026#34; \\ --query \u0026#39;Reservations[].Instances[].InstanceId\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read instance_id; do echo \u0026#34;Terminating Jenkins instance: $instance_id\u0026#34; aws ec2 terminate-instances --instance-ids \u0026#34;$instance_id\u0026#34; done # XÃ³a Jenkins security group JENKINS_SG_ID=$(aws ec2 describe-security-groups \\ --filters \u0026#34;Name=group-name,Values=sg-jenkins\u0026#34; \\ --query \u0026#39;SecurityGroups[0].GroupId\u0026#39; \\ --output text) if [ \u0026#34;$JENKINS_SG_ID\u0026#34; != \u0026#34;None\u0026#34; ]; then aws ec2 delete-security-group --group-id \u0026#34;$JENKINS_SG_ID\u0026#34; fi # XÃ³a Jenkins key pair aws ec2 delete-key-pair --key-name jenkins-key-pair || true 8.4 XÃ³a SNS Topics vÃ  Subscriptions # List táº¥t cáº£ SNS topics liÃªn quan aws sns list-topics \\ --query \u0026#39;Topics[?contains(TopicArn, `retail-forecast`) || contains(TopicArn, `mlops`)].TopicArn\u0026#39; \\ --output table # XÃ³a subscriptions trÆ°á»›c SNS_TOPIC_ARN=$(aws sns list-topics \\ --query \u0026#39;Topics[?contains(TopicArn, `retail-forecast-alerts`)].TopicArn\u0026#39; \\ --output text) if [ \u0026#34;$SNS_TOPIC_ARN\u0026#34; != \u0026#34;\u0026#34; ]; then # List vÃ  xÃ³a subscriptions aws sns list-subscriptions-by-topic \\ --topic-arn \u0026#34;$SNS_TOPIC_ARN\u0026#34; \\ --query \u0026#39;Subscriptions[].SubscriptionArn\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read sub_arn; do if [ \u0026#34;$sub_arn\u0026#34; != \u0026#34;PendingConfirmation\u0026#34; ]; then echo \u0026#34;Unsubscribing: $sub_arn\u0026#34; aws sns unsubscribe --subscription-arn \u0026#34;$sub_arn\u0026#34; fi done # XÃ³a topic aws sns delete-topic --topic-arn \u0026#34;$SNS_TOPIC_ARN\u0026#34; fi 8.5 XÃ³a CloudWatch Resources cho CI/CD # XÃ³a CloudWatch dashboard cho CI/CD aws cloudwatch delete-dashboards \\ --dashboard-names \u0026#34;RetailForecast-CI-CD-Pipeline\u0026#34; || true # XÃ³a custom metrics namespace aws cloudwatch list-metrics \\ --namespace \u0026#34;RetailForecast/Pipeline\u0026#34; \\ --query \u0026#39;Metrics[].MetricName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read metric; do echo \u0026#34;Found pipeline metric: $metric\u0026#34; # Note: CloudWatch metrics tá»± Ä‘á»™ng expire sau 15 thÃ¡ng done # XÃ³a alarms liÃªn quan Ä‘áº¿n CI/CD aws cloudwatch describe-alarms \\ --alarm-name-prefix \u0026#34;Pipeline-\u0026#34; \\ --query \u0026#39;MetricAlarms[].AlarmName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read alarm; do echo \u0026#34;Deleting pipeline alarm: $alarm\u0026#34; aws cloudwatch delete-alarms --alarm-names \u0026#34;$alarm\u0026#34; done # XÃ³a log groups cho CI/CD aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/codebuild/retail-forecast-build\u0026#34; || true aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/codepipeline/retail-forecast-pipeline\u0026#34; || true 8.6 XÃ³a ECR Images vÃ  Tags # List táº¥t cáº£ images trong ECR repository aws ecr describe-images \\ --repository-name mlops/retail-api \\ --region ap-southeast-1 \\ --query \u0026#39;imageDetails[].imageTags[]\u0026#39; \\ --output table # XÃ³a specific CI/CD tags (giá»¯ láº¡i production tags) aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --region ap-southeast-1 \\ --image-ids imageTag=dev imageTag=staging imageTag=feature-* || true # XÃ³a untagged images aws ecr describe-images \\ --repository-name mlops/retail-api \\ --region ap-southeast-1 \\ --filter tagStatus=UNTAGGED \\ --query \u0026#39;imageDetails[].imageDigest\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read digest; do echo \u0026#34;Deleting untagged image: $digest\u0026#34; aws ecr batch-delete-image \\ --repository-name mlops/retail-api \\ --region ap-southeast-1 \\ --image-ids imageDigest=\u0026#34;$digest\u0026#34; done 8.7 Clean Up SageMaker Resources # List training jobs created by CI/CD aws sagemaker list-training-jobs \\ --name-contains \u0026#34;retail-forecast\u0026#34; \\ --max-results 50 \\ --query \u0026#39;TrainingJobSummaries[].TrainingJobName\u0026#39; \\ --output table # Stop running training jobs aws sagemaker list-training-jobs \\ --status-equals InProgress \\ --name-contains \u0026#34;retail-forecast\u0026#34; \\ --query \u0026#39;TrainingJobSummaries[].TrainingJobName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read job_name; do echo \u0026#34;Stopping training job: $job_name\u0026#34; aws sagemaker stop-training-job --training-job-name \u0026#34;$job_name\u0026#34; done # XÃ³a model versions trong Model Registry (giá»¯ approved models) aws sagemaker list-model-packages \\ --model-package-group-name \u0026#34;retail-forecast-models\u0026#34; \\ --model-approval-status PendingManualApproval \\ --query \u0026#39;ModelPackageSummaryList[].ModelPackageArn\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read package_arn; do echo \u0026#34;Deleting pending model package: $package_arn\u0026#34; aws sagemaker delete-model-package --model-package-name \u0026#34;$package_arn\u0026#34; done # XÃ³a endpoints tá»« failed deployments aws sagemaker list-endpoints \\ --name-contains \u0026#34;retail-forecast-dev\u0026#34; \\ --query \u0026#39;Endpoints[?EndpointStatus==`Failed`].EndpointName\u0026#39; \\ --output text | tr \u0026#39;\\t\u0026#39; \u0026#39;\\n\u0026#39; | while read endpoint; do echo \u0026#34;Deleting failed endpoint: $endpoint\u0026#34; aws sagemaker delete-endpoint --endpoint-name \u0026#34;$endpoint\u0026#34; done 8.8 Verification # Verify IAM resources Ä‘Ã£ bá»‹ xÃ³a aws iam get-role --role-name GitHubActionsRole 2\u0026gt;/dev/null || echo \u0026#34;GitHubActionsRole deleted\u0026#34; # Verify SNS topics Ä‘Ã£ bá»‹ xÃ³a aws sns list-topics \\ --query \u0026#39;Topics[?contains(TopicArn, `retail-forecast-alerts`)]\u0026#39; || echo \u0026#34;SNS topics cleaned\u0026#34; # Verify CloudWatch resources aws cloudwatch describe-dashboards \\ --dashboard-name-prefix \u0026#34;RetailForecast-CI-CD\u0026#34; || echo \u0026#34;Dashboards cleaned\u0026#34; # Verify no running training jobs aws sagemaker list-training-jobs \\ --status-equals InProgress \\ --name-contains \u0026#34;retail-forecast\u0026#34; \\ --query \u0026#39;TrainingJobSummaries\u0026#39; || echo \u0026#34;No running training jobs\u0026#34; # Check GitHub Actions status gh run list --limit 5 --status completed 9. Báº£ng giÃ¡ CI/CD Pipeline (ap-southeast-1) 9.1. GitHub Actions Pricing Plan Included Minutes Price per minute Storage Free (Public repos) Unlimited $0 500MB Free (Private repos) 2,000 min/month $0.008 500MB Pro 3,000 min/month $0.008 1GB Team 10,000 min/month $0.008 2GB Enterprise 50,000 min/month $0.008 50GB Runner costs:\nUbuntu: Standard rate macOS: 10x Standard rate Windows: 2x Standard rate Self-hosted: Free compute, infrastructure cost only 9.2. AWS IAM vÃ  Security Costs Service Cost Description IAM Roles \u0026amp; Policies Free Unlimited roles and policies STS AssumeRole calls $0.002/1000 calls OIDC authentication AWS Config (compliance) $0.003/configuration item Policy compliance tracking Example calculation:\n100 CI/CD runs/month Ã— 5 STS calls = 500 calls = $0.001/month 9.3. SageMaker Training Costs trong CI/CD Instance Type Cost per Hour Typical Job Duration Cost per Run ml.m5.large $0.134 15 minutes $0.034 ml.m5.xlarge $0.269 10 minutes $0.045 ml.c5.xlarge $0.238 8 minutes $0.032 ml.p3.2xlarge $4.284 5 minutes $0.357 Monthly costs by frequency:\nDaily training: 30 runs Ã— $0.045 = $1.35 Weekly training: 4 runs Ã— $0.045 = $0.18 On-demand training: 2 runs Ã— $0.045 = $0.09 9.4. ECR Storage vÃ  Transfer Costs Component Cost Volume Monthly Cost Storage $0.10/GB/month 5GB images $0.50 Data Transfer IN Free Upload images $0 Data Transfer OUT $0.12/GB Download to EKS Variable Image management costs:\n# Example: 10 images Ã— 500MB each = 5GB storage # Monthly cost: 5GB Ã— $0.10 = $0.50 # Transfer to EKS: 5GB Ã— $0.12 = $0.60 (one-time per deployment) 9.5. CloudWatch Monitoring cho CI/CD Metric Type Quantity Unit Cost Monthly Cost Custom Metrics 20 metrics $0.30/metric $6.00 API Calls 100K calls $0.01/1K calls $1.00 Alarms 10 alarms $0.10/alarm $1.00 Dashboard 1 dashboard $3.00/dashboard $3.00 Total Monitoring $11.00 9.6. SNS Notification Costs Notification Type Volume Cost per Message Monthly Cost Email 200 notifications $0.75/million $0.0002 SMS 50 notifications $0.8/message $40.00 Slack Webhook 200 notifications $0.75/million $0.0002 Push Mobile 100 notifications $0.75/million $0.0001 9.7. Jenkins Infrastructure Costs (náº¿u self-hosted) Component Instance Type Monthly Hours Monthly Cost Jenkins Master t3.medium 730 hours $30.37 Build Agents t3.large (2 agents) 100 hours $13.25 EBS Storage 100GB gp3 - $8.00 Data Transfer 50GB/month $0.12/GB $6.00 Total Jenkins $57.62 9.8. CI/CD Pipeline Scenarios Scenario 1: Small Team (GitHub Actions)\nComponent Usage Monthly Cost GitHub Actions (private) 2,000 min included $0 SageMaker training 4 runs/month $0.18 ECR storage 2GB images $0.20 CloudWatch basic 5 metrics, 3 alarms $1.80 SNS notifications Email only $0.0002 Total Small Team $2.18/month Scenario 2: Medium Team (GitHub Actions Pro)\nComponent Usage Monthly Cost GitHub Actions Pro 3,000 min + 500 extra $4.00 SageMaker training 12 runs/month $0.54 ECR storage 8GB images $0.80 CloudWatch full 15 metrics, 8 alarms $5.30 SNS notifications Email + Slack $0.0004 Total Medium Team $10.64/month Scenario 3: Enterprise (Self-hosted Jenkins)\nComponent Usage Monthly Cost Jenkins infrastructure t3.medium + agents $57.62 SageMaker training 60 runs/month $2.70 ECR storage 20GB images $2.00 CloudWatch enterprise 50 metrics, 25 alarms $17.50 SNS notifications Multi-channel $40.50 Total Enterprise $120.32/month 9.9. Cost Optimization Strategies GitHub Actions Optimization:\n# Use matrix strategy Ä‘á»ƒ giáº£m runtime strategy: matrix: python-version: [3.8, 3.9, 3.10] # Cache dependencies - uses: actions/cache@v3 with: path: ~/.cache/pip key: ${{ runner.os }}-pip-${{ hashFiles(\u0026#39;**/requirements.txt\u0026#39;) }} # Conditional jobs if: github.ref == \u0026#39;refs/heads/main\u0026#39; SageMaker Training Optimization:\n# Sá»­ dá»¥ng Spot instances cho training training_params = { \u0026#39;TrainingJobName\u0026#39;: job_name, \u0026#39;ResourceConfig\u0026#39;: { \u0026#39;InstanceType\u0026#39;: \u0026#39;ml.m5.large\u0026#39;, \u0026#39;InstanceCount\u0026#39;: 1, \u0026#39;VolumeSizeInGB\u0026#39;: 30, \u0026#39;UseSpotInstances\u0026#39;: True, # 90% cost savings \u0026#39;MaxRuntimeInSeconds\u0026#39;: 3600 } } ECR Cost Optimization:\n# Lifecycle policy Ä‘á»ƒ tá»± Ä‘á»™ng xÃ³a old images aws ecr put-lifecycle-policy \\ --repository-name mlops/retail-api \\ --lifecycle-policy-text \u0026#39;{ \u0026#34;rules\u0026#34;: [ { \u0026#34;rulePriority\u0026#34;: 1, \u0026#34;selection\u0026#34;: { \u0026#34;tagStatus\u0026#34;: \u0026#34;untagged\u0026#34;, \u0026#34;countType\u0026#34;: \u0026#34;sinceImagePushed\u0026#34;, \u0026#34;countUnit\u0026#34;: \u0026#34;days\u0026#34;, \u0026#34;countNumber\u0026#34;: 7 }, \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;expire\u0026#34; } } ] }\u0026#39; 9.10. ROI Analysis cho CI/CD Investment Benefit Manual Process Automated CI/CD Time Saved Cost Benefit Code Testing 2 hours/week 5 minutes 1.9 hours $95/week Model Training 30 min setup Automatic 2 hours/month $100/month Deployment 1 hour/deploy 5 minutes 55 min/deploy $45/deploy Rollback 2 hours 5 minutes 1.9 hours $95/incident Annual ROI calculation:\nInvestment: $127.68/month Ã— 12 = $1,532 Savings: (2 hours/week Ã— 52 weeks + 2 hours/month Ã— 12) Ã— $50/hour = $6,400 ROI: 322% return on investment 9.11. Monitoring CI/CD Costs # Track GitHub Actions usage gh api /repos/:owner/:repo/actions/billing/usage # Monitor SageMaker training costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon SageMaker\u0026#34;]}}\u0026#39; # ECR storage costs aws ecr describe-registry-statistics --region ap-southeast-1 # CloudWatch costs aws ce get-cost-and-usage \\ --time-period Start=2024-01-01,End=2024-01-31 \\ --granularity MONTHLY \\ --metrics BlendedCost \\ --group-by Type=DIMENSION,Key=SERVICE \\ --filter \u0026#39;{\u0026#34;Dimensions\u0026#34;:{\u0026#34;Key\u0026#34;:\u0026#34;SERVICE\u0026#34;,\u0026#34;Values\u0026#34;:[\u0026#34;Amazon CloudWatch\u0026#34;]}}\u0026#39; ğŸ’° Cost Summary cho Task 11:\nSmall Team (GitHub Free): $2.18/month Medium Team (GitHub Pro): $10.64/month Enterprise (Self-hosted): $120.32/month ROI: 322% vá»›i automation benefits Break-even point: ~3 months cho medium team setup Next Step: Task 12: Cost Optimization \u0026amp; Teardown\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/3-blogstranslated/3.12-blog12/",
	"title": "ÄÆ¡n giáº£n hÃ³a viá»‡c táº¡o chÃ­nh sÃ¡ch IAM vá»›i IAM Policy Autopilot, má»™t MCP server mÃ£ nguá»“n má»Ÿ má»›i dÃ nh cho builders",
	"tags": [],
	"description": "",
	"content": "TÃ¡c giáº£: Micah Walter â€” 30 NOV 2025\nHÃ´m nay, chÃºng tÃ´i cÃ´ng bá»‘ IAM Policy Autopilot, má»™t mÃ¡y chá»§ Model Context Protocol (MCP) mÃ£ nguá»“n má»Ÿ má»›i phÃ¢n tÃ­ch mÃ£ á»©ng dá»¥ng cá»§a báº¡n vÃ  giÃºp cÃ¡c trá»£ lÃ½ láº­p trÃ¬nh AI cá»§a báº¡n táº¡o ra cÃ¡c chÃ­nh sÃ¡ch dá»±a trÃªn danh tÃ­nh (identity-based) cá»§a AWS Identity and Access Management (IAM). IAM Policy Autopilot tÄƒng tá»‘c phÃ¡t triá»ƒn ban Ä‘áº§u báº±ng cÃ¡ch cung cáº¥p cho builders má»™t Ä‘iá»ƒm khá»Ÿi Ä‘áº§u mÃ  há» cÃ³ thá»ƒ xem xÃ©t vÃ  tinh chá»‰nh thÃªm. NÃ³ tÃ­ch há»£p vá»›i cÃ¡c trá»£ lÃ½ láº­p trÃ¬nh AI nhÆ° Kiro, Claude Code, Cursor vÃ  Cline, vÃ  cung cáº¥p cho chÃºng kiáº¿n thá»©c vá» AWS Identity and Access Management (IAM) cÅ©ng nhÆ° sá»± hiá»ƒu biáº¿t vá» cÃ¡c dá»‹ch vá»¥ vÃ  tÃ­nh nÄƒng AWS má»›i nháº¥t. IAM Policy Autopilot cÃ³ sáºµn mÃ  khÃ´ng tÃ­nh thÃªm phÃ­, cháº¡y cá»¥c bá»™, vÃ  báº¡n cÃ³ thá»ƒ báº¯t Ä‘áº§u báº±ng cÃ¡ch truy cáº­p kho GitHub cá»§a chÃºng tÃ´i.\nCÃ¡c á»©ng dá»¥ng Amazon Web Services (AWS) yÃªu cáº§u cÃ¡c chÃ­nh sÃ¡ch IAM cho cÃ¡c vai trÃ² (roles) cá»§a chÃºng. Builders trÃªn AWS, tá»« nhÃ  phÃ¡t triá»ƒn Ä‘áº¿n lÃ£nh Ä‘áº¡o doanh nghiá»‡p, tÆ°Æ¡ng tÃ¡c vá»›i IAM nhÆ° má»™t pháº§n trong quy trÃ¬nh lÃ m viá»‡c cá»§a há». CÃ¡c nhÃ  phÃ¡t triá»ƒn thÆ°á»ng báº¯t Ä‘áº§u vá»›i cÃ¡c quyá»n rá»™ng hÆ¡n vÃ  tinh chá»‰nh chÃºng theo thá»i gian, cÃ¢n báº±ng giá»¯a phÃ¡t triá»ƒn nhanh vÃ  báº£o máº­t. Há» thÆ°á»ng sá»­ dá»¥ng cÃ¡c trá»£ lÃ½ láº­p trÃ¬nh AI vá»›i hy vá»ng tÄƒng tá»‘c phÃ¡t triá»ƒn vÃ  soáº¡n tháº£o cÃ¡c quyá»n IAM. Tuy nhiÃªn, cÃ¡c cÃ´ng cá»¥ AI nÃ y khÃ´ng hiá»ƒu Ä‘áº§y Ä‘á»§ cÃ¡c sáº¯c thÃ¡i cá»§a IAM vÃ  cÃ³ thá»ƒ bá» sÃ³t quyá»n hoáº·c gá»£i Ã½ cÃ¡c hÃ nh Ä‘á»™ng khÃ´ng há»£p lá»‡. Builders tÃ¬m kiáº¿m cÃ¡c giáº£i phÃ¡p cung cáº¥p kiáº¿n thá»©c IAM Ä‘Ã¡ng tin cáº­y, tÃ­ch há»£p vá»›i cÃ¡c trá»£ lÃ½ AI vÃ  giÃºp há» báº¯t Ä‘áº§u viá»‡c táº¡o chÃ­nh sÃ¡ch, Ä‘á»ƒ há» cÃ³ thá»ƒ táº­p trung xÃ¢y dá»±ng á»©ng dá»¥ng.\nTáº¡o chÃ­nh sÃ¡ch há»£p lá»‡ vá»›i kiáº¿n thá»©c AWS IAM Policy Autopilot giáº£i quyáº¿t cÃ¡c thÃ¡ch thá»©c nÃ y báº±ng cÃ¡ch táº¡o cÃ¡c chÃ­nh sÃ¡ch IAM dá»±a trÃªn danh tÃ­nh trá»±c tiáº¿p tá»« mÃ£ á»©ng dá»¥ng cá»§a báº¡n. Sá»­ dá»¥ng phÃ¢n tÃ­ch mÃ£ mang tÃ­nh xÃ¡c Ä‘á»‹nh (deterministic code analysis), nÃ³ táº¡o ra cÃ¡c chÃ­nh sÃ¡ch Ä‘Ã¡ng tin cáº­y vÃ  há»£p lá»‡, Ä‘á»ƒ báº¡n dÃ nh Ã­t thá»i gian hÆ¡n cho viá»‡c viáº¿t vÃ  gá»¡ lá»—i quyá»n. IAM Policy Autopilot tÃ­ch há»£p kiáº¿n thá»©c AWS, bao gá»“m triá»ƒn khai tham chiáº¿u (reference implementation) cá»§a dá»‹ch vá»¥ AWS Ä‘Ã£ Ä‘Æ°á»£c cÃ´ng bá»‘, Ä‘á»ƒ luÃ´n cáº­p nháº­t. NÃ³ sá»­ dá»¥ng thÃ´ng tin nÃ y Ä‘á»ƒ hiá»ƒu cÃ¡ch mÃ£ vÃ  cÃ¡c lá»‡nh gá»i SDK Ã¡nh xáº¡ sang cÃ¡c hÃ nh Ä‘á»™ng IAM vÃ  luÃ´n theo ká»‹p cÃ¡c dá»‹ch vá»¥ vÃ  thao tÃ¡c (operations) AWS má»›i nháº¥t.\nCÃ¡c chÃ­nh sÃ¡ch Ä‘Æ°á»£c táº¡o ra cung cáº¥p má»™t Ä‘iá»ƒm khá»Ÿi Ä‘áº§u Ä‘á»ƒ báº¡n xem xÃ©t vÃ  thu háº¹p pháº¡m vi nháº±m triá»ƒn khai quyá»n theo nguyÃªn táº¯c Ä‘áº·c quyá»n tá»‘i thiá»ƒu (least privilege). Khi báº¡n sá»­a Ä‘á»•i mÃ£ á»©ng dá»¥ng cá»§a mÃ¬nhâ€”dÃ¹ lÃ  thÃªm tÃ­ch há»£p dá»‹ch vá»¥ AWS má»›i hay cáº­p nháº­t cÃ¡c tÃ­ch há»£p hiá»‡n cÃ³â€”báº¡n chá»‰ cáº§n cháº¡y IAM Policy Autopilot láº¡i Ä‘á»ƒ nháº­n cÃ¡c quyá»n Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t.\nBáº¯t Ä‘áº§u vá»›i IAM Policy Autopilot CÃ¡c nhÃ  phÃ¡t triá»ƒn cÃ³ thá»ƒ báº¯t Ä‘áº§u vá»›i IAM Policy Autopilot chá»‰ trong vÃ i phÃºt báº±ng cÃ¡ch táº£i xuá»‘ng vÃ  tÃ­ch há»£p nÃ³ vÃ o quy trÃ¬nh lÃ m viá»‡c cá»§a há».\nLÃ  má»™t mÃ¡y chá»§ MCP, IAM Policy Autopilot váº­n hÃ nh á»Ÿ cháº¿ Ä‘á»™ ná»n khi builders trÃ² chuyá»‡n vá»›i cÃ¡c trá»£ lÃ½ láº­p trÃ¬nh AI cá»§a há». Khi á»©ng dá»¥ng cá»§a báº¡n cáº§n cÃ¡c chÃ­nh sÃ¡ch IAM, cÃ¡c trá»£ lÃ½ láº­p trÃ¬nh cÃ³ thá»ƒ gá»i IAM Policy Autopilot Ä‘á»ƒ phÃ¢n tÃ­ch cÃ¡c lá»‡nh gá»i AWS SDK trong á»©ng dá»¥ng cá»§a báº¡n vÃ  táº¡o cÃ¡c chÃ­nh sÃ¡ch IAM dá»±a trÃªn danh tÃ­nh cáº§n thiáº¿t, cung cáº¥p cho báº¡n cÃ¡c quyá»n cáº§n cÃ³ Ä‘á»ƒ báº¯t Ä‘áº§u. Sau khi táº¡o quyá»n, náº¿u báº¡n váº«n gáº·p lá»—i Access Denied trong quÃ¡ trÃ¬nh kiá»ƒm thá»­, trá»£ lÃ½ láº­p trÃ¬nh AI sáº½ gá»i IAM Policy Autopilot Ä‘á»ƒ phÃ¢n tÃ­ch sá»± tá»« chá»‘i vÃ  Ä‘á» xuáº¥t cÃ¡c báº£n sá»­a chÃ­nh sÃ¡ch IAM cÃ³ má»¥c tiÃªu. Sau khi báº¡n xem xÃ©t vÃ  phÃª duyá»‡t cÃ¡c thay Ä‘á»•i Ä‘Æ°á»£c Ä‘á» xuáº¥t, IAM Policy Autopilot sáº½ cáº­p nháº­t cÃ¡c quyá»n.\nBáº¡n cÅ©ng cÃ³ thá»ƒ dÃ¹ng IAM Policy Autopilot nhÆ° má»™t cÃ´ng cá»¥ giao diá»‡n dÃ²ng lá»‡nh (CLI) Ä‘á»™c láº­p Ä‘á»ƒ táº¡o chÃ­nh sÃ¡ch trá»±c tiáº¿p hoáº·c kháº¯c phá»¥c cÃ¡c quyá»n bá»‹ thiáº¿u. Cáº£ cÃ´ng cá»¥ CLI vÃ  mÃ¡y chá»§ MCP Ä‘á»u cung cáº¥p cÃ¹ng cÃ¡c kháº£ nÄƒng táº¡o chÃ­nh sÃ¡ch vÃ  kháº¯c phá»¥c sá»± cá»‘, vÃ¬ váº­y báº¡n cÃ³ thá»ƒ chá»n cÃ¡ch tÃ­ch há»£p phÃ¹ há»£p nháº¥t vá»›i quy trÃ¬nh lÃ m viá»‡c cá»§a mÃ¬nh.\nKhi sá»­ dá»¥ng IAM Policy Autopilot, báº¡n cÅ©ng nÃªn hiá»ƒu cÃ¡c thá»±c tiá»…n tá»‘t nháº¥t Ä‘á»ƒ tá»‘i Ä‘a hÃ³a lá»£i Ã­ch cá»§a nÃ³. IAM Policy Autopilot táº¡o cÃ¡c chÃ­nh sÃ¡ch dá»±a trÃªn danh tÃ­nh vÃ  khÃ´ng táº¡o cÃ¡c chÃ­nh sÃ¡ch dá»±a trÃªn tÃ i nguyÃªn (resource-based policies), ranh giá»›i quyá»n (permission boundaries), chÃ­nh sÃ¡ch kiá»ƒm soÃ¡t dá»‹ch vá»¥ (SCPs) hay chÃ­nh sÃ¡ch kiá»ƒm soÃ¡t tÃ i nguyÃªn (RCPs). IAM Policy Autopilot táº¡o cÃ¡c chÃ­nh sÃ¡ch Æ°u tiÃªn chá»©c nÄƒng hÆ¡n lÃ  quyá»n tá»‘i thiá»ƒu. Báº¡n nÃªn luÃ´n xem xÃ©t cÃ¡c chÃ­nh sÃ¡ch Ä‘Æ°á»£c táº¡o ra vÃ  tinh chá»‰nh náº¿u cáº§n Ä‘á»ƒ chÃºng phÃ¹ há»£p vá»›i cÃ¡c yÃªu cáº§u báº£o máº­t cá»§a báº¡n trÆ°á»›c khi triá»ƒn khai.\nHÃ£y thá»­ xem Äá»ƒ thiáº¿t láº­p IAM Policy Autopilot, trÆ°á»›c tiÃªn tÃ´i cáº§n cÃ i Ä‘áº·t nÃ³ trÃªn há»‡ thá»‘ng cá»§a mÃ¬nh. Äá»ƒ lÃ m váº­y, tÃ´i chá»‰ cáº§n cháº¡y má»™t lá»‡nh má»™t dÃ²ng:\ncurl -sSL https://github.com/awslabs/iam-policy-autopilot/raw/refs/heads/main/install.sh | sudo sh Sau Ä‘Ã³, tÃ´i cÃ³ thá»ƒ lÃ m theo hÆ°á»›ng dáº«n Ä‘á»ƒ cÃ i Ä‘áº·t báº¥t ká»³ mÃ¡y chá»§ MCP nÃ o cho IDE mÃ  tÃ´i chá»n. HÃ´m nay, tÃ´i Ä‘ang dÃ¹ng Kiro! Trong má»™t phiÃªn chat má»›i trong Kiro, tÃ´i báº¯t Ä‘áº§u vá»›i má»™t prompt Ä‘Æ¡n giáº£n, trong Ä‘Ã³ tÃ´i yÃªu cáº§u Kiro Ä‘á»c cÃ¡c tá»‡p trong thÆ° má»¥c file-to-queue cá»§a tÃ´i vÃ  táº¡o má»™t tá»‡p AWS CloudFormation má»›i Ä‘á»ƒ tÃ´i cÃ³ thá»ƒ triá»ƒn khai á»©ng dá»¥ng. ThÆ° má»¥c nÃ y chá»©a má»™t bá»™ Ä‘á»‹nh tuyáº¿n tá»‡p Amazon Simple Storage Service (Amazon S3) tá»± Ä‘á»™ng, quÃ©t má»™t bucket vÃ  gá»­i thÃ´ng bÃ¡o Ä‘áº¿n cÃ¡c hÃ ng Ä‘á»£i Amazon Simple Queue Service (Amazon SQS) hoáº·c Amazon EventBridge dá»±a trÃªn cÃ¡c quy táº¯c khá»›p tiá»n tá»‘ (prefix-matching) cÃ³ thá»ƒ cáº¥u hÃ¬nh, cho phÃ©p cÃ¡c quy trÃ¬nh lÃ m viá»‡c hÆ°á»›ng sá»± kiá»‡n (event-driven) Ä‘Æ°á»£c kÃ­ch hoáº¡t bá»Ÿi vá»‹ trÃ­ cá»§a tá»‡p.\nPháº§n cuá»‘i cÃ¹ng yÃªu cáº§u Kiro Ä‘áº£m báº£o tÃ´i Ä‘ang bao gá»“m cÃ¡c chÃ­nh sÃ¡ch IAM cáº§n thiáº¿t. Äiá»u nÃ y sáº½ Ä‘á»§ Ä‘á»ƒ Kiro sá»­ dá»¥ng mÃ¡y chá»§ MCP IAM Policy Autopilot. Tiáº¿p theo, Kiro sá»­ dá»¥ng mÃ¡y chá»§ MCP IAM Policy Autopilot Ä‘á»ƒ táº¡o má»™t tÃ i liá»‡u chÃ­nh sÃ¡ch má»›i, nhÆ° Ä‘Æ°á»£c minh há»a trong hÃ¬nh áº£nh sau. Sau khi xong, Kiro sáº½ tiáº¿p tá»¥c xÃ¢y dá»±ng máº«u CloudFormation cá»§a chÃºng ta vÃ  má»™t sá»‘ tÃ i liá»‡u bá»• sung cÅ©ng nhÆ° cÃ¡c tá»‡p mÃ£ liÃªn quan. Cuá»‘i cÃ¹ng, chÃºng ta cÃ³ thá»ƒ tháº¥y máº«u CloudFormation Ä‘Æ°á»£c táº¡o ra vá»›i má»™t tÃ i liá»‡u chÃ­nh sÃ¡ch má»›i, táº¥t cáº£ Ä‘á»u Ä‘Æ°á»£c táº¡o báº±ng mÃ¡y chá»§ MCP IAM Policy Autopilot! Quy trÃ¬nh phÃ¡t triá»ƒn Ä‘Æ°á»£c cáº£i thiá»‡n IAM Policy Autopilot tÃ­ch há»£p vá»›i cÃ¡c dá»‹ch vá»¥ AWS trÃªn nhiá»u lÄ©nh vá»±c. Vá»›i cÃ¡c dá»‹ch vá»¥ AWS cá»‘t lÃµi, IAM Policy Autopilot phÃ¢n tÃ­ch viá»‡c á»©ng dá»¥ng cá»§a báº¡n sá»­ dá»¥ng cÃ¡c dá»‹ch vá»¥ nhÆ° Amazon S3, AWS Lambda, Amazon DynamoDB, Amazon Elastic Compute Cloud (Amazon EC2) vÃ  Amazon CloudWatch Logs, sau Ä‘Ã³ táº¡o ra cÃ¡c quyá»n cáº§n thiáº¿t mÃ  mÃ£ cá»§a báº¡n cáº§n dá»±a trÃªn cÃ¡c lá»‡nh gá»i SDK mÃ  nÃ³ phÃ¡t hiá»‡n. Sau khi cÃ¡c chÃ­nh sÃ¡ch Ä‘Æ°á»£c táº¡o, báº¡n cÃ³ thá»ƒ sao chÃ©p chÃ­nh sÃ¡ch trá»±c tiáº¿p vÃ o máº«u CloudFormation, stack AWS Cloud Development Kit (AWS CDK) hoáº·c cáº¥u hÃ¬nh Terraform cá»§a báº¡n. Báº¡n cÅ©ng cÃ³ thá»ƒ prompt cÃ¡c trá»£ lÃ½ láº­p trÃ¬nh AI cá»§a mÃ¬nh Ä‘á»ƒ tÃ­ch há»£p nÃ³ cho báº¡n.\nIAM Policy Autopilot cÅ©ng bá»• trá»£ cho cÃ¡c cÃ´ng cá»¥ IAM hiá»‡n cÃ³ nhÆ° AWS IAM Access Analyzer báº±ng cÃ¡ch cung cáº¥p cÃ¡c chÃ­nh sÃ¡ch mang tÃ­nh chá»©c nÄƒng nhÆ° má»™t Ä‘iá»ƒm khá»Ÿi Ä‘áº§u, sau Ä‘Ã³ báº¡n cÃ³ thá»ƒ xÃ¡c thá»±c báº±ng tÃ­nh nÄƒng xÃ¡c thá»±c chÃ­nh sÃ¡ch (policy validation) cá»§a IAM Access Analyzer hoáº·c tinh chá»‰nh theo thá»i gian vá»›i phÃ¢n tÃ­ch truy cáº­p khÃ´ng sá»­ dá»¥ng (unused access analysis).\nHiá»‡n Ä‘Ã£ cÃ³ sáºµn IAM Policy Autopilot hiá»‡n cÃ³ sáºµn dÆ°á»›i dáº¡ng má»™t cÃ´ng cá»¥ mÃ£ nguá»“n má»Ÿ trÃªn GitHub mÃ  khÃ´ng tÃ­nh thÃªm phÃ­. CÃ´ng cá»¥ hiá»‡n há»— trá»£ cÃ¡c á»©ng dá»¥ng Python, TypeScript vÃ  Go.\nNhá»¯ng kháº£ nÄƒng nÃ y Ä‘áº¡i diá»‡n cho má»™t bÆ°á»›c tiáº¿n Ä‘Ã¡ng ká»ƒ trong viá»‡c Ä‘Æ¡n giáº£n hÃ³a tráº£i nghiá»‡m phÃ¡t triá»ƒn trÃªn AWS Ä‘á»ƒ cÃ¡c builders á»Ÿ cÃ¡c má»©c kinh nghiá»‡m khÃ¡c nhau cÃ³ thá»ƒ phÃ¡t triá»ƒn vÃ  triá»ƒn khai á»©ng dá»¥ng má»™t cÃ¡ch hiá»‡u quáº£ hÆ¡n.\n"
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.11-week11/",
	"title": "Tuáº§n 11 - Observability &amp; FinOps",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 11: Triá»ƒn khai giÃ¡m sÃ¡t/quan sÃ¡t há»‡ thá»‘ng toÃ n diá»‡n (CloudWatch, X-Ray). Tá»‘i Æ°u chi phÃ­ cloud (Budgets, Tagging, Rightsizing). CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - GiÃ¡m sÃ¡t (Monitoring): Thiáº¿t láº­p CloudWatch Dashboards vÃ  Alarms (CPU, lá»—i 5xx).\n- Báº­t X-Ray tracing. AWS CloudWatch 3 - Logs: Táº­p trung log vá» CloudWatch Logs kÃ¨m retention policies.\n- DÃ¹ng Logs Insights Ä‘á»ƒ truy váº¥n. AWS X-Ray 4 - FinOps: Triá»ƒn khai Cost Allocation Tags.\n- Thiáº¿t láº­p AWS Budgets vÃ  Cost Anomaly Detection. AWS Budgets 5 - Tá»‘i Æ°u (Optimization): RÃ  soÃ¡t Cost Explorer.\n- Thá»±c hiá»‡n rightsizing vÃ  dá»n dáº¹p tÃ i nguyÃªn khÃ´ng dÃ¹ng. AWS Cost Explorer 6 - Ã”n táº­p: XÃ¡c minh Ä‘á»™ bao phá»§ observability vÃ  cÃ¡c biá»‡n phÃ¡p tiáº¿t kiá»‡m chi phÃ­.\n- BÃ¡o cÃ¡o tuáº§n. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 11: Thiáº¿t láº­p dashboard observability thá»‘ng nháº¥t Ä‘á»ƒ theo dÃµi sá»©c khoáº» há»‡ thá»‘ng. Ãp dá»¥ng cÆ¡ cháº¿ kiá»ƒm soÃ¡t chi phÃ­ báº±ng Budgets vÃ  Tagging. Tá»‘i Æ°u má»©c sá»­ dá»¥ng tÃ i nguyÃªn dá»±a trÃªn dá»¯ liá»‡u giÃ¡m sÃ¡t. "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/5-workshop/12-cost--teardown/",
	"title": "Cost Management &amp; Teardown",
	"tags": [],
	"description": "",
	"content": "\rğŸ¯ Má»¥c tiÃªu Task 12: Quáº£n lÃ½ vÃ  tá»‘i Æ°u chi phÃ­ váº­n hÃ nh toÃ n bá»™ háº¡ táº§ng MLOps trÃªn AWS:\nGiáº£m thiá»ƒu chi phÃ­ compute (EC2, SageMaker, ALB) Tá»± Ä‘á»™ng scale-down hoáº·c xÃ³a tÃ i nguyÃªn khÃ´ng sá»­ dá»¥ng Ãp dá»¥ng lifecycle policies cho dá»¯ liá»‡u vÃ  container images Äáº£m báº£o pipeline vá»«a hoáº¡t Ä‘á»™ng hiá»‡u quáº£, vá»«a tiáº¿t kiá»‡m chi phÃ­ tá»‘i Ä‘a 1. Chi phÃ­ váº­n hÃ nh há»‡ thá»‘ng MLOps Khi triá»ƒn khai há»‡ thá»‘ng MLOps Ä‘áº§y Ä‘á»§ nhÆ° Retail Prediction API, chi phÃ­ váº­n hÃ nh cÃ³ thá»ƒ nhanh chÃ³ng tÄƒng cao náº¿u khÃ´ng Ä‘Æ°á»£c quáº£n lÃ½ há»£p lÃ½. CÃ¡c thÃ nh pháº§n chÃ­nh gÃ³p pháº§n vÃ o chi phÃ­ bao gá»“m:\n1.1. PhÃ¢n tÃ­ch chi phÃ­ theo thÃ nh pháº§n Dá»‹ch vá»¥ Chi phÃ­ khÃ´ng tá»‘i Æ°u NguyÃªn nhÃ¢n Giáº£i phÃ¡p EKS NodeGroup 0.04 USD/giá»/node On-demand instances cháº¡y 24/7 Spot instances + Auto scaling + Schedule SageMaker Training 0.3 USD/job Instances size lá»›n, cháº¡y lÃ¢u Spot training + Hyperparameter tuning tá»‘i Æ°u S3 Storage 0.023 USD/GB/thÃ¡ng LÆ°u trá»¯ táº¥t cáº£ dá»¯ liá»‡u á»Ÿ Standard tier Lifecycle policy + Intelligent-Tiering CloudWatch Logs 0.50 USD/GB LÆ°u trá»¯ logs khÃ´ng giá»›i háº¡n Log retention policy + Insights query tá»‘i Æ°u ALB 0.027 USD/giá» Cháº¡y liÃªn tá»¥c ká»ƒ cáº£ khi khÃ´ng cÃ³ traffic Schedule shutdown khi khÃ´ng sá»­ dá»¥ng ECR Storage 0.10 USD/GB/thÃ¡ng LÆ°u trá»¯ táº¥t cáº£ image versions Lifecycle policy xÃ³a image cÅ© 2. Sá»­ dá»¥ng EC2 Spot Instance EC2 Spot Instances lÃ  má»™t trong nhá»¯ng cÃ¡ch hiá»‡u quáº£ nháº¥t Ä‘á»ƒ tiáº¿t kiá»‡m chi phÃ­ compute trÃªn AWS, vá»›i má»©c giáº£m lÃªn Ä‘áº¿n 70-90% so vá»›i On-demand instances.\n2.1. Sá»­ dá»¥ng Spot cho EKS NodeGroup Cáº­p nháº­t file cáº¥u hÃ¬nh Terraform cho EKS NodeGroup:\nmodule \u0026#34;eks_managed_node_group\u0026#34; { source = \u0026#34;terraform-aws-modules/eks/aws//modules/eks-managed-node-group\u0026#34; name = \u0026#34;retail-forecast-nodes\u0026#34; cluster_name = module.eks.cluster_name cluster_version = module.eks.cluster_version # Cáº¥u hÃ¬nh Spot Instance capacity_type = \u0026#34;SPOT\u0026#34; # Thay vÃ¬ ON_DEMAND # Diversify instance types Ä‘á»ƒ tÄƒng kháº£ nÄƒng cÃ³ Spot instance_types = [\u0026#34;t3.medium\u0026#34;, \u0026#34;t3a.medium\u0026#34;, \u0026#34;t2.medium\u0026#34;] min_size = 2 max_size = 5 desired_size = 2 # ThÃªm labels vÃ  taints cho Kubernetes scheduler labels = { app = \u0026#34;retail-api\u0026#34; } } 2.2. Sá»­ dá»¥ng Spot cho SageMaker Training Update script táº¡o SageMaker training job Ä‘á»ƒ sá»­ dá»¥ng spot instances:\n# aws/script/create_training_job.py import boto3 import argparse import time def create_training_job(job_name, data_bucket, output_bucket, instance_type, use_spot=True): sagemaker = boto3.client(\u0026#39;sagemaker\u0026#39;) # TÃ­nh toÃ¡n thá»i gian stop cho Spot (giá»›i háº¡n 1 giá») current_time = int(time.time()) stop_time = current_time + 3600 # 1 hour # Cáº¥u hÃ¬nh training job training_params = { \u0026#39;TrainingJobName\u0026#39;: job_name, \u0026#39;AlgorithmSpecification\u0026#39;: { \u0026#39;TrainingImage\u0026#39;: \u0026#39;123456789012.dkr.ecr.us-east-1.amazonaws.com/retail-forecast-training:latest\u0026#39;, \u0026#39;TrainingInputMode\u0026#39;: \u0026#39;File\u0026#39; }, \u0026#39;RoleArn\u0026#39;: \u0026#39;arn:aws:iam::123456789012:role/SageMakerExecutionRole\u0026#39;, \u0026#39;InputDataConfig\u0026#39;: [ { \u0026#39;ChannelName\u0026#39;: \u0026#39;train\u0026#39;, \u0026#39;DataSource\u0026#39;: { \u0026#39;S3DataSource\u0026#39;: { \u0026#39;S3DataType\u0026#39;: \u0026#39;S3Prefix\u0026#39;, \u0026#39;S3Uri\u0026#39;: f\u0026#39;s3://{data_bucket}/train\u0026#39;, \u0026#39;S3DataDistributionType\u0026#39;: \u0026#39;FullyReplicated\u0026#39; } } } ], \u0026#39;OutputDataConfig\u0026#39;: { \u0026#39;S3OutputPath\u0026#39;: f\u0026#39;s3://{output_bucket}/output\u0026#39; }, \u0026#39;ResourceConfig\u0026#39;: { \u0026#39;InstanceType\u0026#39;: instance_type, \u0026#39;InstanceCount\u0026#39;: 1, \u0026#39;VolumeSizeInGB\u0026#39;: 10 }, \u0026#39;StoppingCondition\u0026#39;: { \u0026#39;MaxRuntimeInSeconds\u0026#39;: 3600 }, \u0026#39;Tags\u0026#39;: [ { \u0026#39;Key\u0026#39;: \u0026#39;Project\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;RetailMLOps\u0026#39; } ] } # Cáº¥u hÃ¬nh Spot training náº¿u Ä‘Æ°á»£c yÃªu cáº§u if use_spot: training_params[\u0026#39;EnableManagedSpotTraining\u0026#39;] = True training_params[\u0026#39;StoppingCondition\u0026#39;][\u0026#39;MaxWaitTimeInSeconds\u0026#39;] = 3900 # ThÃªm thá»i gian chá» tá»‘i Ä‘a response = sagemaker.create_training_job(**training_params) return response if __name__ == \u0026#34;__main__\u0026#34;: parser = argparse.ArgumentParser() parser.add_argument(\u0026#39;--job-name\u0026#39;, type=str, required=True) parser.add_argument(\u0026#39;--data-bucket\u0026#39;, type=str, required=True) parser.add_argument(\u0026#39;--output-bucket\u0026#39;, type=str, required=True) parser.add_argument(\u0026#39;--instance-type\u0026#39;, type=str, default=\u0026#39;ml.m5.large\u0026#39;) parser.add_argument(\u0026#39;--use-spot\u0026#39;, type=bool, default=True) args = parser.parse_args() response = create_training_job( args.job_name, args.data_bucket, args.output_bucket, args.instance_type, args.use_spot ) print(f\u0026#34;Training job created: {response[\u0026#39;TrainingJobArn\u0026#39;]}\u0026#34;) 2.3. Xá»­ lÃ½ giÃ¡n Ä‘oáº¡n Spot Instance Äá»ƒ Ä‘áº£m báº£o há»‡ thá»‘ng váº«n hoáº¡t Ä‘á»™ng khi cÃ¡c Spot Instances bá»‹ thu há»“i, cáº§n cáº¥u hÃ¬nh:\nPod Disruption Budget (PDB) Ä‘á»ƒ Ä‘áº£m báº£o sá»‘ lÆ°á»£ng pod tá»‘i thiá»ƒu: # aws/k8s/pdb/retail-forecast-pdb.yaml apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: retail-forecast-pdb namespace: retail-forecast spec: minAvailable: 1 # LuÃ´n Ä‘áº£m báº£o Ã­t nháº¥t 1 pod Ä‘ang cháº¡y selector: matchLabels: app: retail-forecast-api Cluster Autoscaler vá»›i Spot Instance Handling: # aws/k8s/addons/cluster-autoscaler.yaml apiVersion: apps/v1 kind: Deployment metadata: name: cluster-autoscaler namespace: kube-system labels: app: cluster-autoscaler spec: replicas: 1 selector: matchLabels: app: cluster-autoscaler template: metadata: labels: app: cluster-autoscaler spec: containers: - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.26.2 name: cluster-autoscaler command: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=aws - --skip-nodes-with-local-storage=false - --expander=least-waste - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/retail-forecast-cluster - --balance-similar-node-groups - --skip-nodes-with-system-pods=false volumeMounts: - name: ssl-certs mountPath: /etc/ssl/certs/ca-certificates.crt readOnly: true volumes: - name: ssl-certs hostPath: path: \u0026#34;/etc/ssl/certs/ca-bundle.crt\u0026#34; 3. S3 Lifecycle \u0026amp; Intelligent-Tiering 3.1. Cáº¥u hÃ¬nh Lifecycle Policy cho S3 Bucket Triá»ƒn khai lifecycle policy thÃ´ng qua Terraform:\n# infra/modules/s3/main.tf resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;retail_forecast_data\u0026#34; { bucket = \u0026#34;retail-forecast-data-${var.environment}\u0026#34; tags = { Project = \u0026#34;RetailMLOps\u0026#34; } } # Lifecycle configuration resource \u0026#34;aws_s3_bucket_lifecycle_configuration\u0026#34; \u0026#34;data_lifecycle\u0026#34; { bucket = aws_s3_bucket.retail_forecast_data.id rule { id = \u0026#34;raw-data-tier\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { prefix = \u0026#34;raw/\u0026#34; } transition { days = 30 storage_class = \u0026#34;INTELLIGENT_TIERING\u0026#34; } } rule { id = \u0026#34;silver-data-tier\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { prefix = \u0026#34;silver/\u0026#34; } transition { days = 30 storage_class = \u0026#34;INTELLIGENT_TIERING\u0026#34; } } rule { id = \u0026#34;artifacts-archive\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { prefix = \u0026#34;artifacts/\u0026#34; } transition { days = 90 storage_class = \u0026#34;GLACIER_IR\u0026#34; } transition { days = 180 storage_class = \u0026#34;DEEP_ARCHIVE\u0026#34; } } rule { id = \u0026#34;logs-archive\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { prefix = \u0026#34;logs/\u0026#34; } transition { days = 90 storage_class = \u0026#34;GLACIER_IR\u0026#34; } transition { days = 180 storage_class = \u0026#34;DEEP_ARCHIVE\u0026#34; } } rule { id = \u0026#34;temp-cleanup\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { and { prefix = \u0026#34;tmp/\u0026#34; tag { key = \u0026#34;temporary\u0026#34; value = \u0026#34;true\u0026#34; } } } expiration { days = 7 } } } 3.2. Cáº¥u hÃ¬nh Intelligent-Tiering # infra/modules/s3/intelligent_tiering.tf resource \u0026#34;aws_s3_bucket_intelligent_tiering_configuration\u0026#34; \u0026#34;retail_data_tiering\u0026#34; { bucket = aws_s3_bucket.retail_forecast_data.id name = \u0026#34;RetailDataTiering\u0026#34; tiering { access_tier = \u0026#34;ARCHIVE_ACCESS\u0026#34; days = 90 } tiering { access_tier = \u0026#34;DEEP_ARCHIVE_ACCESS\u0026#34; days = 180 } filter { prefix = \u0026#34;data/\u0026#34; } } 4. Tá»± Ä‘á»™ng dá»«ng tÃ i nguyÃªn ngoÃ i giá» 4.1. Schedule Lambda vá»›i EventBridge Táº¡o Lambda function Ä‘á»ƒ stop/start tÃ i nguyÃªn:\n# aws/scripts/resource_scheduler.py import boto3 import os def lambda_handler(event, context): action = event.get(\u0026#39;action\u0026#39;, \u0026#39;stop\u0026#39;) # \u0026#39;stop\u0026#39; or \u0026#39;start\u0026#39; # EKS NodeGroup scaling eks = boto3.client(\u0026#39;eks\u0026#39;) autoscaling = boto3.client(\u0026#39;autoscaling\u0026#39;) cluster_name = os.environ.get(\u0026#39;EKS_CLUSTER_NAME\u0026#39;, \u0026#39;retail-forecast-cluster\u0026#39;) nodegroup_name = os.environ.get(\u0026#39;NODEGROUP_NAME\u0026#39;, \u0026#39;retail-forecast-nodes\u0026#39;) if action == \u0026#39;stop\u0026#39;: # Scale down to 0 print(f\u0026#34;Scaling down nodegroup {nodegroup_name} in cluster {cluster_name}\u0026#34;) # Láº¥y auto scaling group name tá»« nodegroup response = eks.describe_nodegroup( clusterName=cluster_name, nodegroupName=nodegroup_name ) asg_name = response[\u0026#39;nodegroup\u0026#39;][\u0026#39;resources\u0026#39;][\u0026#39;autoScalingGroups\u0026#39;][0][\u0026#39;name\u0026#39;] # Scale down ASG vá» 0 autoscaling.update_auto_scaling_group( AutoScalingGroupName=asg_name, MinSize=0, DesiredCapacity=0 ) print(f\u0026#34;NodeGroup {nodegroup_name} scaled down to 0\u0026#34;) elif action == \u0026#39;start\u0026#39;: # Scale up to desired capacity print(f\u0026#34;Scaling up nodegroup {nodegroup_name} in cluster {cluster_name}\u0026#34;) response = eks.describe_nodegroup( clusterName=cluster_name, nodegroupName=nodegroup_name ) asg_name = response[\u0026#39;nodegroup\u0026#39;][\u0026#39;resources\u0026#39;][\u0026#39;autoScalingGroups\u0026#39;][0][\u0026#39;name\u0026#39;] # Scale up ASG to desired capacity autoscaling.update_auto_scaling_group( AutoScalingGroupName=asg_name, MinSize=2, DesiredCapacity=2 ) print(f\u0026#34;NodeGroup {nodegroup_name} scaled up to 2\u0026#34;) # SageMaker Endpoint if os.environ.get(\u0026#39;SAGEMAKER_ENDPOINT\u0026#39;): sm_client = boto3.client(\u0026#39;sagemaker\u0026#39;) endpoint_name = os.environ.get(\u0026#39;SAGEMAKER_ENDPOINT\u0026#39;) if action == \u0026#39;stop\u0026#39;: # Endpoint khÃ´ng thá»ƒ dá»«ng nhÆ°ng cÃ³ thá»ƒ xÃ³a vÃ  táº¡o láº¡i sau print(f\u0026#34;Deleting SageMaker endpoint {endpoint_name}\u0026#34;) try: sm_client.delete_endpoint(EndpointName=endpoint_name) print(f\u0026#34;Endpoint {endpoint_name} deleted\u0026#34;) except Exception as e: print(f\u0026#34;Error deleting endpoint: {e}\u0026#34;) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#34;Successfully executed {action} action\u0026#34; } Terraform Ä‘á»ƒ táº¡o EventBridge schedule vÃ  Lambda:\n# infra/modules/scheduler/main.tf resource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;resource_scheduler\u0026#34; { function_name = \u0026#34;retail-forecast-resource-scheduler\u0026#34; handler = \u0026#34;resource_scheduler.lambda_handler\u0026#34; runtime = \u0026#34;python3.9\u0026#34; role = aws_iam_role.lambda_role.arn filename = \u0026#34;resource_scheduler.zip\u0026#34; timeout = 300 environment { variables = { EKS_CLUSTER_NAME = var.eks_cluster_name NODEGROUP_NAME = var.nodegroup_name SAGEMAKER_ENDPOINT = var.sagemaker_endpoint } } } # Táº¡o EventBridge rule Ä‘á»ƒ dá»«ng tÃ i nguyÃªn lÃºc 19:00 UTC resource \u0026#34;aws_cloudwatch_event_rule\u0026#34; \u0026#34;stop_resources\u0026#34; { name = \u0026#34;retail-forecast-stop-resources\u0026#34; description = \u0026#34;Stop resources at 19:00 UTC\u0026#34; schedule_expression = \u0026#34;cron(0 19 * * ? *)\u0026#34; } # Gáº¯n Lambda vá»›i rule stop resource \u0026#34;aws_cloudwatch_event_target\u0026#34; \u0026#34;stop_resources_target\u0026#34; { rule = aws_cloudwatch_event_rule.stop_resources.name target_id = \u0026#34;RetailForecastStopResources\u0026#34; arn = aws_lambda_function.resource_scheduler.arn input = jsonencode({ action = \u0026#34;stop\u0026#34; }) } # Táº¡o EventBridge rule Ä‘á»ƒ khá»Ÿi Ä‘á»™ng tÃ i nguyÃªn lÃºc 7:00 UTC resource \u0026#34;aws_cloudwatch_event_rule\u0026#34; \u0026#34;start_resources\u0026#34; { name = \u0026#34;retail-forecast-start-resources\u0026#34; description = \u0026#34;Start resources at 7:00 UTC\u0026#34; schedule_expression = \u0026#34;cron(0 7 * * ? *)\u0026#34; } # Gáº¯n Lambda vá»›i rule start resource \u0026#34;aws_cloudwatch_event_target\u0026#34; \u0026#34;start_resources_target\u0026#34; { rule = aws_cloudwatch_event_rule.start_resources.name target_id = \u0026#34;RetailForecastStartResources\u0026#34; arn = aws_lambda_function.resource_scheduler.arn input = jsonencode({ action = \u0026#34;start\u0026#34; }) } # Cáº¥p quyá»n cho EventBridge Ä‘á»ƒ gá»i Lambda resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_eventbridge_stop\u0026#34; { statement_id = \u0026#34;AllowExecutionFromEventBridgeStop\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = aws_lambda_function.resource_scheduler.function_name principal = \u0026#34;events.amazonaws.com\u0026#34; source_arn = aws_cloudwatch_event_rule.stop_resources.arn } resource \u0026#34;aws_lambda_permission\u0026#34; \u0026#34;allow_eventbridge_start\u0026#34; { statement_id = \u0026#34;AllowExecutionFromEventBridgeStart\u0026#34; action = \u0026#34;lambda:InvokeFunction\u0026#34; function_name = aws_lambda_function.resource_scheduler.function_name principal = \u0026#34;events.amazonaws.com\u0026#34; source_arn = aws_cloudwatch_event_rule.start_resources.arn } 4.2. Terraform Destroy cho CI/CD Pipeline ThÃªm job vÃ o GitHub Actions workflow Ä‘á»ƒ xÃ³a toÃ n bá»™ tÃ i nguyÃªn sau khi hoÃ n thÃ nh:\n# .github/workflows/mlops-pipeline.yml jobs: # CÃ¡c job hiá»‡n táº¡i... terraform_destroy: name: Destroy Infrastructure runs-on: ubuntu-latest needs: [deploy_eks, monitoring] if: github.event.inputs.destroy_after_demo == \u0026#39;true\u0026#39; environment: name: ${{ needs.setup.outputs.environment }} steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup Terraform uses: hashicorp/setup-terraform@v2 - name: Terraform Init run: | cd aws/infra terraform init - name: Terraform Destroy run: | cd aws/infra terraform destroy -auto-approve 5. Cost Visibility \u0026amp; Alerts 5.1. Tag Strategy cho AWS Resources # ThÃªm vÃ o táº¥t cáº£ resource modules locals { common_tags = { Project = \u0026#34;RetailMLOps\u0026#34; } } # Ãp dá»¥ng tag vÃ o táº¥t cáº£ resource 5.2. AWS Budgets Alert # infra/modules/budget/main.tf resource \u0026#34;aws_budgets_budget\u0026#34; \u0026#34;cost\u0026#34; { name = \u0026#34;retail-forecast-${var.environment}-monthly-budget\u0026#34; budget_type = \u0026#34;COST\u0026#34; limit_amount = var.monthly_limit limit_unit = \u0026#34;USD\u0026#34; time_unit = \u0026#34;MONTHLY\u0026#34; time_period_start = \u0026#34;2023-01-01_00:00\u0026#34; cost_filter { name = \u0026#34;TagKeyValue\u0026#34; values = [ \u0026#34;user:Project$RetailForecastMLOps\u0026#34; ] } notification { comparison_operator = \u0026#34;GREATER_THAN\u0026#34; threshold = 80 threshold_type = \u0026#34;PERCENTAGE\u0026#34; notification_type = \u0026#34;ACTUAL\u0026#34; subscriber_email_addresses = var.notification_emails } notification { comparison_operator = \u0026#34;GREATER_THAN\u0026#34; threshold = 100 threshold_type = \u0026#34;PERCENTAGE\u0026#34; notification_type = \u0026#34;ACTUAL\u0026#34; subscriber_email_addresses = var.notification_emails subscriber_sns_topic_arns = [var.sns_topic_arn] } } Script Ä‘á»ƒ táº¡o AWS Budget qua CLI:\n# aws/scripts/create_budget.sh #!/bin/bash ACCOUNT_ID=$(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) EMAIL=\u0026#34;your-email@example.com\u0026#34; BUDGET_NAME=\u0026#34;MLOpsBudget\u0026#34; BUDGET_LIMIT=5 # USD # Táº¡o AWS Budget aws budgets create-budget \\ --account-id $ACCOUNT_ID \\ --budget \u0026#39;{ \u0026#34;BudgetName\u0026#34;: \u0026#34;\u0026#39;$BUDGET_NAME\u0026#39;\u0026#34;, \u0026#34;BudgetLimit\u0026#34;: { \u0026#34;Amount\u0026#34;: \u0026#34;\u0026#39;$BUDGET_LIMIT\u0026#39;\u0026#34;, \u0026#34;Unit\u0026#34;: \u0026#34;USD\u0026#34; }, \u0026#34;CostFilters\u0026#34;: { \u0026#34;TagKeyValue\u0026#34;: [ \u0026#34;user:Project$RetailForecastMLOps\u0026#34; ] }, \u0026#34;BudgetType\u0026#34;: \u0026#34;COST\u0026#34;, \u0026#34;TimeUnit\u0026#34;: \u0026#34;MONTHLY\u0026#34; }\u0026#39; \\ --notifications-with-subscribers \u0026#39;[ { \u0026#34;Notification\u0026#34;: { \u0026#34;NotificationType\u0026#34;: \u0026#34;ACTUAL\u0026#34;, \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GREATER_THAN\u0026#34;, \u0026#34;Threshold\u0026#34;: 80, \u0026#34;ThresholdType\u0026#34;: \u0026#34;PERCENTAGE\u0026#34; }, \u0026#34;Subscribers\u0026#34;: [ { \u0026#34;SubscriptionType\u0026#34;: \u0026#34;EMAIL\u0026#34;, \u0026#34;Address\u0026#34;: \u0026#34;\u0026#39;$EMAIL\u0026#39;\u0026#34; } ] }, { \u0026#34;Notification\u0026#34;: { \u0026#34;NotificationType\u0026#34;: \u0026#34;ACTUAL\u0026#34;, \u0026#34;ComparisonOperator\u0026#34;: \u0026#34;GREATER_THAN\u0026#34;, \u0026#34;Threshold\u0026#34;: 100, \u0026#34;ThresholdType\u0026#34;: \u0026#34;PERCENTAGE\u0026#34; }, \u0026#34;Subscribers\u0026#34;: [ { \u0026#34;SubscriptionType\u0026#34;: \u0026#34;EMAIL\u0026#34;, \u0026#34;Address\u0026#34;: \u0026#34;\u0026#39;$EMAIL\u0026#39;\u0026#34; } ] } ]\u0026#39; 6. ECR \u0026amp; CloudWatch Optimization 6.1. ECR Lifecycle Policy # infra/modules/ecr/main.tf resource \u0026#34;aws_ecr_repository\u0026#34; \u0026#34;retail_forecast\u0026#34; { name = \u0026#34;retail-forecast\u0026#34; image_tag_mutability = \u0026#34;MUTABLE\u0026#34; image_scanning_configuration { scan_on_push = true } } resource \u0026#34;aws_ecr_lifecycle_policy\u0026#34; \u0026#34;retail_forecast_policy\u0026#34; { repository = aws_ecr_repository.retail_forecast.name policy = jsonencode({ rules = [ { rulePriority = 1, description = \u0026#34;Keep only 3 latest untagged images\u0026#34;, selection = { tagStatus = \u0026#34;untagged\u0026#34;, countType = \u0026#34;imageCountMoreThan\u0026#34;, countNumber = 3 }, action = { type = \u0026#34;expire\u0026#34; } }, { rulePriority = 2, description = \u0026#34;Keep only 3 latest images per tag prefix\u0026#34;, selection = { tagStatus = \u0026#34;tagged\u0026#34;, tagPrefixList = [\u0026#34;prod\u0026#34;, \u0026#34;stage\u0026#34;, \u0026#34;dev\u0026#34;], countType = \u0026#34;imageCountMoreThan\u0026#34;, countNumber = 3 }, action = { type = \u0026#34;expire\u0026#34; } }, { rulePriority = 3, description = \u0026#34;Keep only the 10 most recent images\u0026#34;, selection = { tagStatus = \u0026#34;any\u0026#34;, countType = \u0026#34;imageCountMoreThan\u0026#34;, countNumber = 10 }, action = { type = \u0026#34;expire\u0026#34; } } ] }) } Hoáº·c sá»­ dá»¥ng AWS CLI:\naws ecr put-lifecycle-policy \\ --repository-name retail-forecast \\ --lifecycle-policy-text \u0026#39;{ \u0026#34;rules\u0026#34;: [ { \u0026#34;rulePriority\u0026#34;: 1, \u0026#34;description\u0026#34;: \u0026#34;Keep only 3 latest untagged images\u0026#34;, \u0026#34;selection\u0026#34;: { \u0026#34;tagStatus\u0026#34;: \u0026#34;untagged\u0026#34;, \u0026#34;countType\u0026#34;: \u0026#34;imageCountMoreThan\u0026#34;, \u0026#34;countNumber\u0026#34;: 3 }, \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;expire\u0026#34; } }, { \u0026#34;rulePriority\u0026#34;: 2, \u0026#34;description\u0026#34;: \u0026#34;Keep only 3 latest images per tag prefix\u0026#34;, \u0026#34;selection\u0026#34;: { \u0026#34;tagStatus\u0026#34;: \u0026#34;tagged\u0026#34;, \u0026#34;tagPrefixList\u0026#34;: [\u0026#34;prod\u0026#34;, \u0026#34;stage\u0026#34;, \u0026#34;dev\u0026#34;], \u0026#34;countType\u0026#34;: \u0026#34;imageCountMoreThan\u0026#34;, \u0026#34;countNumber\u0026#34;: 3 }, \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;expire\u0026#34; } }, { \u0026#34;rulePriority\u0026#34;: 3, \u0026#34;description\u0026#34;: \u0026#34;Keep only the 10 most recent images\u0026#34;, \u0026#34;selection\u0026#34;: { \u0026#34;tagStatus\u0026#34;: \u0026#34;any\u0026#34;, \u0026#34;countType\u0026#34;: \u0026#34;imageCountMoreThan\u0026#34;, \u0026#34;countNumber\u0026#34;: 10 }, \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;expire\u0026#34; } } ] }\u0026#39; 6.2. CloudWatch Log Retention Policy # infra/modules/logs/main.tf resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;eks_logs\u0026#34; { name = \u0026#34;/aws/eks/mlops-retail-cluster/cluster\u0026#34; retention_in_days = 7 } resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;app_logs\u0026#34; { name = \u0026#34;/aws/retail/api\u0026#34; retention_in_days = 7 } Hoáº·c sá»­ dá»¥ng AWS CLI:\n# Äáº·t retention policy cho CloudWatch Logs aws logs put-retention-policy \\ --log-group-name \u0026#34;/aws/eks/retail-forecast-cluster/cluster\u0026#34; \\ --retention-in-days 30 aws logs put-retention-policy \\ --log-group-name \u0026#34;/aws/retail-forecast/api\u0026#34; \\ --retention-in-days 30 aws logs put-retention-policy \\ --log-group-name \u0026#34;/aws/sagemaker/TrainingJobs\u0026#34; \\ --retention-in-days 30 7. Tá»± Ä‘á»™ng Teardown toÃ n bá»™ háº¡ táº§ng 7.1. Script Terraform Destroy # aws/scripts/teardown.sh #!/bin/bash echo \u0026#34;Starting teardown...\u0026#34; # XÃ³a Kubernetes resources kubectl delete namespace mlops --ignore-not-found=true # XÃ³a ECR images aws ecr batch-delete-image --repository-name mlops/retail-api --image-ids imageTag=latest imageTag=v2 imageTag=v3 # Terraform destroy cd ../infra terraform destroy -auto-approve echo \u0026#34;Teardown completed!\u0026#34; 7.2. ThÃªm vÃ o GitHub Actions Workflow # .github/workflows/teardown.yml name: MLOps Infrastructure Teardown on: workflow_dispatch: inputs: confirmation: description: \u0026#39;Type \u0026#34;destroy\u0026#34; to confirm deletion of all resources\u0026#39; required: true env: AWS_REGION: us-east-1 TF_VAR_environment: dev jobs: teardown: name: Teardown Infrastructure runs-on: ubuntu-latest if: github.event.inputs.confirmation == \u0026#39;destroy\u0026#39; steps: - name: Checkout repository uses: actions/checkout@v4 - name: Configure AWS credentials uses: aws-actions/configure-aws-credentials@v2 with: role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/GitHubActionsRole aws-region: ${{ env.AWS_REGION }} - name: Setup kubectl uses: azure/setup-kubectl@v3 - name: Update kubeconfig run: | aws eks update-kubeconfig --name retail-forecast-cluster --region ${{ env.AWS_REGION }} - name: Delete Kubernetes resources run: | kubectl delete namespace retail-forecast --ignore-not-found=true - name: Delete SageMaker endpoints run: | ENDPOINTS=$(aws sagemaker list-endpoints --name-contains retail-forecast --query \u0026#34;Endpoints[].EndpointName\u0026#34; --output text) if [ ! -z \u0026#34;$ENDPOINTS\u0026#34; ]; then for ENDPOINT in $ENDPOINTS; do echo \u0026#34;Deleting endpoint: $ENDPOINT\u0026#34; aws sagemaker delete-endpoint --endpoint-name $ENDPOINT done fi - name: Setup Terraform uses: hashicorp/setup-terraform@v2 - name: Terraform Init run: | cd aws/infra terraform init - name: Terraform Destroy run: | cd aws/infra terraform destroy -auto-approve - name: Send notification if: always() run: | if [ \u0026#34;${{ job.status }}\u0026#34; == \u0026#34;success\u0026#34; ]; then MESSAGE=\u0026#34;âœ… Infrastructure teardown completed successfully\u0026#34; else MESSAGE=\u0026#34;âŒ Infrastructure teardown failed\u0026#34; fi aws sns publish \\ --topic-arn ${{ secrets.SNS_TOPIC_ARN }} \\ --subject \u0026#34;MLOps Infrastructure Teardown\u0026#34; \\ --message \u0026#34;$MESSAGE\u0026#34; 8. Chi phÃ­ Æ°á»›c tÃ­nh sau khi tá»‘i Æ°u DÆ°á»›i Ä‘Ã¢y lÃ  chi phÃ­ dá»± kiáº¿n sau khi Ã¡p dá»¥ng cÃ¡c biá»‡n phÃ¡p tá»‘i Æ°u:\nThÃ nh pháº§n TrÆ°á»›c tá»‘i Æ°u Sau tá»‘i Æ°u Tiáº¿t kiá»‡m (%) EKS NodeGroup 28.80 USD 2.40 USD 92% S3 Storage 1.15 USD 0.63 USD 45% CloudWatch Logs 2.50 USD 0.75 USD 70% LoadBalancer 19.44 USD 5.40 USD 72% ECR Storage 0.50 USD 0.20 USD 60% Tá»•ng chi phÃ­ 52.39 USD 9.38 USD 82% 9. Káº¿t quáº£ ká»³ vá»ng âœ… Checklist HoÃ n thÃ nh EC2 Spot Instance: Cáº¥u hÃ¬nh EKS NodeGroup vÃ  SageMaker sá»­ dá»¥ng Spot S3 Lifecycle: Triá»ƒn khai lifecycle policies cho data vÃ  artifacts Auto Schedule: Lambda + EventBridge Ä‘á»ƒ tá»± Ä‘á»™ng dá»«ng/khá»Ÿi Ä‘á»™ng tÃ i nguyÃªn Budget Alert: Thiáº¿t láº­p giÃ¡m sÃ¡t chi phÃ­ vÃ  cáº£nh bÃ¡o ECR Lifecycle: Chá»‰ giá»¯ láº¡i 3 phiÃªn báº£n image má»›i nháº¥t Log Retention: CloudWatch logs retention policy 30 ngÃ y Complete Teardown: Script Ä‘á»ƒ xÃ³a hoÃ n toÃ n tÃ i nguyÃªn sau demo ğŸ“Š Verification Steps # 1. Kiá»ƒm tra EKS NodeGroup Ä‘ang sá»­ dá»¥ng Spot aws eks describe-nodegroup \\ --cluster-name retail-forecast-cluster \\ --nodegroup-name retail-forecast-nodes \\ --query \u0026#39;nodegroup.capacityType\u0026#39; # 2. Kiá»ƒm tra S3 lifecycle policy aws s3api get-bucket-lifecycle-configuration \\ --bucket retail-forecast-data-dev # 3. Kiá»ƒm tra CloudWatch logs retention aws logs describe-log-groups \\ --log-group-name-prefix /aws/retail-forecast \\ --query \u0026#39;logGroups[*].[logGroupName,retentionInDays]\u0026#39; # 4. Kiá»ƒm tra ECR lifecycle policy aws ecr get-lifecycle-policy \\ --repository-name retail-forecast # 5. Kiá»ƒm tra AWS Budget Ä‘Ã£ Ä‘Æ°á»£c táº¡o aws budgets describe-budgets \\ --account-id $(aws sts get-caller-identity --query \u0026#34;Account\u0026#34; --output text) # 6. Thá»±c hiá»‡n teardown vÃ  kiá»ƒm tra xÃ³a sáº¡ch cd aws/scripts ./teardown.sh # Kiá»ƒm tra khÃ´ng cÃ²n resources aws eks list-clusters --query \u0026#39;clusters[*]\u0026#39; | grep retail aws s3 ls | grep retail-forecast aws ecr describe-repositories --query \u0026#39;repositories[*].repositoryName\u0026#39; | grep retail aws sagemaker list-endpoints --query \u0026#39;Endpoints[*].EndpointName\u0026#39; | grep retail Tá»•ng káº¿t Quáº£n lÃ½ chi phÃ­ hiá»‡u quáº£ lÃ  má»™t trong nhá»¯ng khÃ­a cáº¡nh quan trá»ng nháº¥t cá»§a MLOps trÃªn AWS. Báº±ng viá»‡c Ã¡p dá»¥ng cÃ¡c chiáº¿n lÆ°á»£c tá»‘i Æ°u nhÆ° Spot Instances, S3 lifecycle policies, auto scheduling, vÃ  resource cleanup, chÃºng ta cÃ³ thá»ƒ giáº£m chi phÃ­ váº­n hÃ nh Ä‘áº¿n 80% mÃ  váº«n duy trÃ¬ Ä‘Æ°á»£c kháº£ nÄƒng má»Ÿ rá»™ng vÃ  hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng.\nCÃ¡c biá»‡n phÃ¡p tá»‘i Æ°u chi phÃ­ nÃ y khÃ´ng chá»‰ giÃºp tiáº¿t kiá»‡m ngÃ¢n sÃ¡ch mÃ  cÃ²n giÃºp há»‡ thá»‘ng MLOps hoáº¡t Ä‘á»™ng hiá»‡u quáº£ hÆ¡n thÃ´ng qua viá»‡c tá»± Ä‘á»™ng hÃ³a quáº£n lÃ½ tÃ i nguyÃªn, giÃ¡m sÃ¡t chi phÃ­, vÃ  thá»±c hiá»‡n cÃ¡c best practices trong quáº£n lÃ½ vÃ²ng Ä‘á»i cá»§a dá»¯ liá»‡u vÃ  container images.\nKáº¿t quáº£ chÃ­nh:\nTiáº¿t kiá»‡m 82% chi phÃ­ váº­n hÃ nh (~9.38 USD/thÃ¡ng so vá»›i ~52.39 USD/thÃ¡ng) Tá»± Ä‘á»™ng schedule start/stop resources S3 lifecycle policies tiáº¿t kiá»‡m storage CloudWatch logs retention 7 ngÃ y Complete teardown script "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/1-worklog/1.12-week12/",
	"title": "Tuáº§n 12 - Capstone: Dá»± Ã¡n cÃ¡ nhÃ¢n",
	"tags": [],
	"description": "",
	"content": "Má»¥c tiÃªu Tuáº§n 12: Tá»•ng há»£p toÃ n bá»™ ká»¹ nÄƒng Ä‘Ã£ há»c Ä‘á»ƒ hoÃ n thiá»‡n dá»± Ã¡n cuá»‘i ká»³ â€œRetail Prediction MLOpsâ€. HoÃ n táº¥t tÃ i liá»‡u vÃ  dá»n dáº¹p (teardown) tÃ i nguyÃªn. CÃ¡c nhiá»‡m vá»¥ thá»±c hiá»‡n trong tuáº§n nÃ y: NgÃ y Nhiá»‡m vá»¥ TÃ i liá»‡u tham kháº£o 2 - TÃ­ch há»£p (Integration): Triá»ƒn khai full stack (VPC, EKS/ECS, RDS/DynamoDB).\n- Káº¿t ná»‘i Frontend (CDN) vÃ  Backend (API). Project Docs 3 - Kiá»ƒm thá»­ (Testing): Thá»±c hiá»‡n Load Testing vÃ  Failure Injection (Chaos Engineering).\n- XÃ¡c minh Auto Scaling vÃ  Rollbacks. Testing 4 - MLOps: Thiáº¿t láº­p pipeline huáº¥n luyá»‡n/triá»ƒn khai (train â†’ registry â†’ deploy).\n- Theo dÃµi metrics/logs vÃ  cáº£nh bÃ¡o cho mÃ´ hÃ¬nh \u0026amp; dá»‹ch vá»¥. MLOps Notes 5 - FinOps \u0026amp; Báº£o máº­t: RÃ  soÃ¡t tagging, budgets, vÃ  quyá»n IAM tá»‘i thiá»ƒu.\n- Kiá»ƒm tra mÃ£ hoÃ¡ dá»¯ liá»‡u (KMS) vÃ  secrets. AWS Best Practices 6 - Tá»•ng káº¿t: HoÃ n thiá»‡n tÃ i liá»‡u kiáº¿n trÃºc + hÆ°á»›ng dáº«n triá»ƒn khai.\n- Teardown tÃ i nguyÃªn, xÃ¡c minh khÃ´ng phÃ¡t sinh chi phÃ­.\n- BÃ¡o cÃ¡o tuáº§n/cuá»‘i ká»³. - Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c Tuáº§n 12: HoÃ n thiá»‡n há»‡ thá»‘ng â€œRetail Prediction MLOpsâ€ "
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenhk64.github.io/aws-mlops-retail-prediction/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]